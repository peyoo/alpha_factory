import numpy as np
import polars as pl
import fastcluster
from scipy.cluster.hierarchy import fcluster
from scipy.spatial.distance import squareform
from loguru import logger
from typing import List, Dict, Optional, Tuple
from deap import tools


class DependenceManager:
    """
    GP å› å­ç‹¬ç«‹æ€§ç®¡ç†å™¨ (DependenceManager)

    é‡æ„é‡ç‚¹:
    1. é‡‡é›†ä¸è¯„ä¼°è§£è€¦: register_fingerprints (è®¡ç®—å±‚) vs calculate_contextual_independence (è¯„ä¼°å±‚)ã€‚
    2. åŠ¨æ€æ‰“åˆ†æœºåˆ¶: æ¯ä¸€ä»£å…¨é‡é‡ç®—ç‹¬ç«‹æ€§ï¼Œå½»åº•è§£å†³ç”±äºç¼“å­˜å‘½ä¸­å¯¼è‡´çš„â€œå…‹éš†ä½“éœ¸æ¦œâ€é—®é¢˜ã€‚
    3. èšç±»å¼•æ“å‰¥ç¦»: _run_fast_clustering è´Ÿè´£çº¯ç²¹çš„æ•°å­¦è®¡ç®—ã€‚
    """

    def __init__(self,
                 opt_names: Tuple[str, ...],
                 opt_weights: Tuple[float, ...],
                 cluster_threshold: float = 0.9,
                 penalty_factor: float = -0.1):
        # æŒ‰ç…§ç”¨æˆ·è®°å¿†ï¼Œthreshold é»˜è®¤ä¸º 0.8
        self.threshold = cluster_threshold
        self.penalty_factor = penalty_factor
        self.opt_names = opt_names
        self.opt_weights = opt_weights

        # ç­›é€‰ç»©æ•ˆæŒ‡æ ‡ç´¢å¼•ï¼Œç”¨äºè®¡ç®—ç»¼åˆâ€œæ­¦åŠ›å€¼â€ (å¦‚ IC, Returns)
        self.perf_indices = [i for i, name in enumerate(opt_names)
                             if name not in ["complexity", "independence"]]
        self.perf_weights = [opt_weights[i] for i in self.perf_indices]

        # é‡‡æ ·é”šç‚¹æ•°æ®
        self.anchor_df: Optional[pl.DataFrame] = None

        # æŒ‡çº¹ç¼“å­˜: è¡¨è¾¾å¼å­—ç¬¦ä¸² -> é‡‡æ ·åçš„ numpy æ•°ç»„
        self.fingerprints_dict: Dict[str, np.ndarray] = {}

        # ç²¾è‹±åº“ï¼šè®°å½•ä¸Šä¸€ä»£ HoF çš„æˆå‘˜æ ‡è¯†åŠå…¶å†å²ç»¼åˆå¾—åˆ†
        self.elite_keys: List[str] = []
        self.elite_power_scores: Dict[str, float] = {}

        logger.info(f"ğŸš€ DependenceManager åˆå§‹åŒ– | é˜ˆå€¼: {self.threshold} | æƒ©ç½š: {self.penalty_factor}")

    def _init_anchor_if_needed(self, df_output: pl.DataFrame):
        """åˆå§‹åŒ–é‡‡æ ·é”šç‚¹ (å›ºå®š 50,000 ç‚¹)"""
        if self.anchor_df is not None:
            return
        full_coords = df_output.select(["DATE", "ASSET"]).unique()
        sample_n = min(50000, full_coords.height)
        self.anchor_df = full_coords.sample(n=sample_n, seed=42).sort(["DATE", "ASSET"])
        logger.success(f"âš“ ç‹¬ç«‹æ€§é‡‡æ ·é”šç‚¹å·²å›ºå®š: {sample_n} è¡Œ")

    def _get_power_score(self, metrics: Dict[str, float]) -> float:
        """æ ¹æ®é…ç½®æƒé‡è®¡ç®—ç»¼åˆç»©æ•ˆå¾—åˆ†"""
        score = 0.0
        for idx, weight in zip(self.perf_indices, self.perf_weights):
            name = self.opt_names[idx]
            score += metrics.get(name, 0.0) * weight
        return score

    # --- é˜¶æ®µ 1: æŒ‡çº¹é‡‡é›† (åœ¨ batched_exprs è®¡ç®—å±‚è°ƒç”¨) ---

    def register_fingerprints(self, df_output: pl.DataFrame, expr_batch_info: List[Tuple]):
        """
        Args:
            df_output: åŒ…å«è®¡ç®—ç»“æœçš„æ•°æ®æ¡† (åˆ—åä¸ºä¸´æ—¶çš„å› å­å)
            expr_batch_info: ä¼ å…¥ [(å› å­å, è¡¨è¾¾å¼å¯¹è±¡, ...), ...]
        """
        self._init_anchor_if_needed(df_output)

        # 1. é‡‡æ ·å¯¹é½
        df_sampled = self.anchor_df.join(df_output, on=["DATE", "ASSET"], how="inner")

        # 2. å»ºç«‹æ˜ å°„å¹¶å­˜å‚¨
        for col_name, expr_obj, _ in expr_batch_info:
            # ã€å…³é”®ã€‘ç”Ÿæˆå…¨å±€å”¯ä¸€çš„ Key
            expr_str = str(expr_obj)

            # å¦‚æœè¿™ä¸ªè¡¨è¾¾å¼å·²ç»åœ¨æŒ‡çº¹åº“äº†ï¼ˆæ¯”å¦‚ä¸åŒä¸ªä½“å˜å¼‚å‡ºäº†ç›¸åŒè¡¨è¾¾å¼ï¼‰ï¼Œå°±ä¸ç”¨å†å­˜ä¸€é
            if expr_str not in self.fingerprints_dict:
                try:
                    # 1. æå– Series
                    series = df_sampled.select(pl.col(col_name)).to_series()

                    # 2. å¤„ç†æç«¯å€¼ (å…³é”®æ­¥éª¤)
                    # ä½¿ç”¨ np.nan_to_num å°† inf è½¬ä¸º float32 çš„æœ€å¤§å€¼ï¼Œnan è½¬ä¸º 0
                    arr = series.to_numpy()
                    arr = np.nan_to_num(arr, nan=0.0, posinf=3e38, neginf=-3e38)

                    # 3. ã€å…³é”®ã€‘å¢åŠ ä¸€æ­¥å‰ªåˆ‡ï¼Œé™åˆ¶åœ¨ float32 çš„å®‰å…¨èŒƒå›´å†…
                    # ä½¿ç”¨ np.clip ç¡®ä¿æ•°å€¼ä¸ä¼šåœ¨ cast æ—¶æº¢å‡º
                    f32_max = np.finfo(np.float32).max
                    f32_min = np.finfo(np.float32).min
                    arr = np.clip(arr, f32_min, f32_max)

                    # 4. è½¬æ¢ä¸º float32 å¹¶å­˜å‚¨
                    self.fingerprints_dict[expr_str] = arr.astype(np.float32)

                except Exception as e:
                    logger.error(f"æå–æŒ‡çº¹å¤±è´¥: {col_name} | {e}")

    # --- é˜¶æ®µ 2: åŠ¨æ€è¯„ä»· (åœ¨ fill_fitness è¯„ä¼°å±‚è°ƒç”¨) ---
    def calculate_contextual_independence(self, exprs_list: List[str], current_results: Dict) -> List[float]:
        """
        [ç»ˆæè¿›æ”»ç‰ˆ]
        1. ç‰©ç†å±‚ï¼šå…¨å‘˜é»˜è®¤ 0.1ï¼Œå½»åº•å°æ€å…‹éš†ä½“ã€‚
        2. å‚èµ›æƒï¼šä»…é™â€˜æœ‰æŒ‡çº¹â€™çš„ä¸ªä½“ï¼ˆå½“å‰æ–°å› å­ + ä¸Šæ¦œè€ç²¾è‹±ï¼‰ã€‚
        3. è§„åˆ™ï¼šç°‡å†…æ­¦åŠ›å€¼å† å†›æ‹¿ 1.0ï¼Œä¸çœ‹èµ„å†ï¼Œåªçœ‹å¼ºå¼±ã€‚
        4. æ¡£æ¡ˆï¼šç¼“å­˜ä¸­æ— æŒ‡çº¹çš„å› å­ç›´æ¥ç»´æŒ 0.1ï¼Œä¸ç»™ç¿»èº«æœºä¼šã€‚
        """
        # 1. åˆå§‹åŒ–å…¨å‘˜æƒ©ç½š
        scores_list = [self.penalty_factor] * len(exprs_list)

        # 2. å»ºç«‹ä½ç½®æ˜ å°„ï¼Œé”å®šæ¯ä¸ªé€»è¾‘çš„â€œé¦–å‘ä»£ç†äººâ€
        expr_to_indices = {}
        for idx, expr in enumerate(exprs_list):
            if expr not in expr_to_indices:
                expr_to_indices[expr] = []
            expr_to_indices[expr].append(idx)

        all_to_cluster = []  # çœŸæ­£è¿›å…¥èšç±»ç«æŠ€åœºçš„åå•
        batch_power = {}

        # 3. ç­›é€‰å‚èµ›è€…
        for expr_str, indices in expr_to_indices.items():
            first_idx = indices[0]  # åªå–ç¬¬ä¸€ä¸ªä½ç½®ä»£è¡¨è¯¥é€»è¾‘

            # ã€å…³é”®ã€‘åªæœ‰å…·å¤‡æŒ‡çº¹çš„å› å­æ‰æœ‰èµ„æ ¼äº‰å¤º 1.0
            # è¿™è‡ªåŠ¨åŒ…å«äº†ï¼š
            #   - æœ¬ä»£æ–°è®¡ç®—å‡ºæ¥çš„å› å­ (åœ¨ register_fingerprints ä¸­å½•å…¥)
            #   - ä¸Šä¸€ä»£ç•™å­˜çš„ç²¾è‹± (åœ¨ update_and_prune ä¸­ä¿ç•™)
            if expr_str in self.fingerprints_dict:
                all_to_cluster.append((expr_str, first_idx))
                # è®°å½•è¯¥å› å­çš„çº¯æ­¦åŠ›å€¼
                batch_power[expr_str] = self._get_power_score(current_results.get(expr_str, {}))
            else:
                # å‡¡æ˜¯æ²¡æŒ‡çº¹çš„ï¼ˆå³ï¼šæ—¢æ²¡è¿›æ¦œï¼Œæœ¬ä»£ä¹Ÿæ²¡è¢«å˜å¼‚å‡ºæ¥çš„è€å› å­ï¼‰
                # å“ªæ€•ä½ æ˜¯é¦–å‘ï¼Œä¹Ÿç»´æŒ scores_list[first_idx] = 0.1
                pass

                # 4. ç°‡å†…å¤§ä¹±æ–—ï¼šæ–°è€åŒå°ï¼Œå¼ºè€…èƒœå‡º
        if all_to_cluster:
            try:
                # æå–å‚ä¸ç«äº‰çš„æ‰€æœ‰å› å­æŒ‡çº¹
                keys_to_calc = [x[0] for x in all_to_cluster]
                matrix = np.column_stack([self.fingerprints_dict[k] for k in keys_to_calc])

                # èšç±»ï¼šæŠŠé€»è¾‘ç›¸ä¼¼ï¼ˆ>0.8ï¼‰çš„åˆ’åˆ†ä¸ºä¸€ç°‡
                labels = self._run_fast_clustering(matrix, self.threshold)
                key_to_label = dict(zip(keys_to_calc, labels))

                # ç°‡å†…æ’åºï¼šæŒ‰æ­¦åŠ›å€¼ç»å¯¹é«˜ä½æ’åˆ—
                # å¦‚æœæ­¦åŠ›å€¼å®Œå…¨ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä¿ç•™ä¸€ä¸ªç¨³å®šçš„æ¬¡åºï¼ˆå¦‚ expr_str å­—å…¸åºï¼‰
                sorted_candidates = sorted(
                    all_to_cluster,
                    key=lambda x: (batch_power.get(x[0], 0), x[0]),
                    reverse=True
                )

                cluster_occupied = set()
                for expr_str, first_idx in sorted_candidates:
                    label = key_to_label[expr_str]

                    # åªæœ‰æ¯ä¸ªé€»è¾‘é¢†åœ°çš„â€œæœ€å¼ºè€…â€èƒ½æ‹¿åˆ° 1.0 ç‹¬ç«‹æ€§
                    if label not in cluster_occupied:
                        scores_list[first_idx] = 1.0
                        cluster_occupied.add(label)
                    else:
                        # ä½ è™½ç„¶æ˜¯æŸä¸ªè¡¨è¾¾å¼çš„é¦–å‘ï¼Œä½†ä½ è¿™ä¸€ä»£é‡åˆ°äº†æ›´å¼ºçš„åŒæ—ç«äº‰è€…
                        scores_list[first_idx] = self.penalty_factor

            except Exception as e:
                logger.error(f"èšç±»å¤±è´¥: {e}")
                # å®¹é”™å¤„ç†ï¼šè‡³å°‘è®©é¦–å‘ä»£è¡¨ä»¬æ´»ä¸‹å»
                for _, f_idx in all_to_cluster:
                    scores_list[f_idx] = 1.0

        return scores_list

    def _run_fast_clustering(self, matrix: np.ndarray, threshold: float) -> np.ndarray:
        """
        [æ ¸å¿ƒè®¡ç®—é€»è¾‘] åŸºäº Spearman ç›¸å…³æ€§çš„å¿«é€Ÿèšç±»å®ç°
        """
        # 1. ç§©å˜æ¢ (Spearman ç›¸å…³æ€§åŸºç¡€)
        matrix_rank = np.apply_along_axis(lambda x: x.argsort().argsort(), 0, matrix).astype(np.float32)

        # 2. ç›¸å…³æ€§çŸ©é˜µ
        corr_matrix = np.nan_to_num(np.corrcoef(matrix_rank, rowvar=False), nan=0.0)

        # 3. è·ç¦»çŸ©é˜µ: d = sqrt(2 * (1 - |rho|))
        dist_matrix = np.sqrt(np.clip(2 * (1 - np.abs(corr_matrix)), 0, None))
        dist_matrix = (dist_matrix + dist_matrix.T) / 2
        np.fill_diagonal(dist_matrix, 0)

        # 4. å¿«é€Ÿèšç±»
        Z = fastcluster.linkage(squareform(dist_matrix), method='complete')

        # 5. åˆ‡å‰²
        t_val = np.sqrt(2 * (1 - threshold))
        return fcluster(Z, t=t_val, criterion='distance')

    # --- é˜¶æ®µ 3: å†…å­˜ä¸ç²¾è‹±çŠ¶æ€åŒæ­¥ ---

    def update_and_prune(self, halloffame: tools.HallOfFame):
        """ä»£æœ«æ¸…ç†ï¼ŒåŒæ­¥ç²¾è‹±æŒ‡çº¹å¹¶é‡Šæ”¾æ— æ•ˆå†…å­˜"""
        # hof_size æŒ‰ç…§è®°å¿†åº”ä¸º 100
        before_count = len(self.fingerprints_dict)
        new_fingerprints = {}
        self.elite_keys = []
        self.elite_power_scores = {}

        for ind in halloffame:
            expr_str = getattr(ind, 'expr_str',None)
            if expr_str is None:
                 logger.error(f"æ— æ³•ä»ä¸ªä½“è·å– expr_str å±æ€§: {ind}")
                 continue

            if expr_str in self.fingerprints_dict:
                new_fingerprints[expr_str] = self.fingerprints_dict[expr_str]
                if expr_str not in self.elite_keys:
                    self.elite_keys.append(expr_str)
                    metrics = {name: val for name, val in zip(self.opt_names, ind.fitness.values)}
                    self.elite_power_scores[expr_str] = self._get_power_score(metrics)

        self.fingerprints_dict = new_fingerprints
        logger.info(f"ğŸ§¹ ç‹¬ç«‹æ€§ç®¡ç†å™¨å‰ªæ: {before_count} -> {len(self.elite_keys)} (ä¿ç•™ç²¾è‹±)")
