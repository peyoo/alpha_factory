import numpy as np
import polars as pl
import fastcluster
from scipy.cluster.hierarchy import fcluster
from scipy.spatial.distance import squareform
from loguru import logger
from typing import List, Dict, Optional, Tuple
from deap import tools


class DependenceManager:
    """
    GP å› å­ç‹¬ç«‹æ€§ç®¡ç†å™¨ (DependenceManager)

    æ ¸å¿ƒæœºåˆ¶ - å±æ€§æŒ‚è½½åè®® (Attribute Tagging Protocol):
    --------------------------------------------------
    æœ¬ç±»é«˜åº¦ä¾èµ–äº DEAP ä¸ªä½“ (Individual) å¯¹è±¡ä¸ŠæŒ‚è½½çš„ `expr_str` å±æ€§ã€‚
    1. åè®®è¦æ±‚ï¼šåœ¨è¿›åŒ–æµç¨‹çš„è¯„ä¼°é˜¶æ®µï¼ˆé€šå¸¸åœ¨ fill_fitness å‡½æ•°ä¸­ï¼‰ï¼Œå¿…é¡»æ‰§è¡Œ:
       `ind.expr_str = str(simplified_expression)`
    2. ä½œç”¨ï¼šè¯¥å±æ€§ä½œä¸ºâ€œå”¯ä¸€æ ‡è¯†ç¬¦â€ï¼Œå°† DEAP æ ‘å¯¹è±¡ä¸æœ¬ç®¡ç†å™¨ç¼“å­˜çš„â€œå› å­æŒ‡çº¹ (fingerprints_dict)â€
       ä»¥åŠâ€œç®€åŒ–è¡¨è¾¾å¼å­—ç¬¦ä¸²â€å¼ºè¡Œç»‘å®šã€‚
    3. ä¼˜åŠ¿ï¼šé¿å…äº†ä»£æœ«å‰ªææ—¶é‡å¤è¿›è¡Œ Sympy è¡¨è¾¾å¼ç®€åŒ–è®¡ç®—ï¼Œæå¤§æå‡äº†å¤šä»£è¿›åŒ–ä¸‹çš„è¿è¡Œæ•ˆç‡ã€‚

    ä¸»è¦ä»»åŠ¡:
    1. é‡‡æ ·å¯„å­˜ï¼šå°†æ‰¹æ¬¡è®¡ç®—å‡ºçš„å› å­å€¼é‡‡æ ·å¹¶å­˜å‚¨åœ¨ fingerprints_dictã€‚
    2. ç‹¬ç«‹æ‰“åˆ†ï¼šåŸºäºç°‡å†…æ­¦åŠ›å€¼å¯¹æ¯”ï¼Œå¯¹é€»è¾‘é‡å¤çš„å› å­è¿›è¡Œé™åˆ†å¤„ç†ã€‚
    3. ä»£æœ«å‰ªæï¼šä¾æ® HoF æˆå‘˜çš„ `expr_str` å±æ€§æ¸…ç†æ— æ•ˆæŒ‡çº¹ï¼Œç»´æŒå†…å­˜å¥åº·ã€‚
    """

    def __init__(self,
                 opt_names: Tuple[str, ...],
                 opt_weights: Tuple[float, ...],
                 cluster_threshold: float = 0.5,
                 penalty_factor: float = 0.5):
        """
        åˆå§‹åŒ–é…ç½®ã€‚

        Args:
            opt_names: ä¼˜åŒ–ç›®æ ‡åç§°åˆ—è¡¨ (éœ€åŒ…å« 'independence' æ‰èƒ½æ¿€æ´»æ‰“åˆ†é€»è¾‘)ã€‚
            opt_weights: ä¼˜åŒ–ç›®æ ‡æƒé‡ï¼Œæ­£æ•°ä»£è¡¨æœ€å¤§åŒ–ï¼Œè´Ÿæ•°ä»£è¡¨æœ€å°åŒ–ã€‚
            cluster_threshold: èšç±»é˜ˆå€¼ (0.95 ä»£è¡¨ç›¸å…³æ€§ > 0.95 çš„å› å­ä¼šè¢«åˆ’åˆ†ä¸ºåŒä¸€é€»è¾‘ç°‡)ã€‚
            penalty_factor: æƒ©ç½šå› å­ï¼Œç°‡å†…è¡¨ç°ä¸å¦‚å† å†›çš„å› å­å°†è·å¾—çš„ç‹¬ç«‹æ€§è¯„ä»·å¾—åˆ†ã€‚
        """
        self.threshold = cluster_threshold
        self.penalty_factor = penalty_factor
        self.opt_names = opt_names
        self.opt_weights = opt_weights

        # é¢„å…ˆç­›é€‰ç»©æ•ˆæŒ‡æ ‡ç´¢å¼•ï¼ˆæ’é™¤å¤æ‚åº¦ã€ç‹¬ç«‹æ€§ç­‰è¾…åŠ©æŒ‡æ ‡ï¼‰ï¼Œç”¨äºè®¡ç®—å› å­çš„ç»¼åˆâ€œæ­¦åŠ›å€¼â€
        self.perf_indices = [i for i, name in enumerate(opt_names)
                             if name not in ["complexity", "independence"]]
        self.perf_weights = [opt_weights[i] for i in self.perf_indices]

        # é”šç‚¹æ•°æ®æ¡†ï¼šç”¨äºç¡®ä¿æ‰€æœ‰å› å­çš„é‡‡æ ·åæ ‡ï¼ˆDATE, ASSETï¼‰å®Œå…¨ä¸€è‡´ï¼Œä¿è¯ç›¸å…³æ€§è®¡ç®—çš„æœ‰æ•ˆæ€§
        self.anchor_df: Optional[pl.DataFrame] = None

        # æ ¸å¿ƒç¼“å­˜å­—å…¸: ç®€åŒ–è¡¨è¾¾å¼å­—ç¬¦ä¸² -> é‡‡æ ·åçš„å› å­å€¼ Series (æŒ‡çº¹)
        self.fingerprints_dict: Dict[str, pl.Series] = {}

        # ç²¾è‹±åº“ç»´æŠ¤ï¼šè®°å½•åäººå ‚æˆå‘˜çš„æ ‡è¯†ç¬¦åŠå…¶å†å²ç»¼åˆå¾—åˆ†
        self.elite_keys: List[str] = []
        self.elite_power_scores: Dict[str, float] = {}

        logger.info(f"ğŸš€ DependenceManager åˆå§‹åŒ– | é˜ˆå€¼: {self.threshold} | æƒ©ç½šåˆ†: {self.penalty_factor}")

    def _init_anchor_if_needed(self, df_output: pl.DataFrame):
        """
        åˆå§‹åŒ–é‡‡æ ·é”šç‚¹ã€‚å›ºå®š 50,000 ä¸ªéšæœºæ ·æœ¬ç‚¹ï¼Œåœ¨ä¿è¯ç»Ÿè®¡æœ‰æ•ˆæ€§çš„åŒæ—¶æœ€å¤§åŒ–è®¡ç®—é€Ÿåº¦ã€‚
        """
        if self.anchor_df is not None:
            return

        full_coords = df_output.select(["DATE", "ASSET"]).unique()
        sample_n = min(50000, full_coords.height)

        # å›ºå®š seed=42 ä¿è¯è¿›åŒ–è¿‡ç¨‹ä¸­ä¸åŒæ‰¹æ¬¡çš„æŒ‡çº¹æå–å…·æœ‰ä¸¥æ ¼å¯æ¯”æ€§
        self.anchor_df = full_coords.sample(n=sample_n, seed=42).sort(["DATE", "ASSET"])
        logger.success(f"âš“ ç‹¬ç«‹æ€§é‡‡æ ·é”šç‚¹å·²å›ºå®šï¼Œæ ·æœ¬è§„æ¨¡: {sample_n} è¡Œ")

    def _get_power_score(self, metrics: Dict[str, float]) -> float:
        """æ ¹æ®é…ç½®æƒé‡è®¡ç®—å› å­çš„ç»¼åˆç»©æ•ˆå¾—åˆ†ï¼ˆæ­¦åŠ›å€¼ï¼‰"""
        score = 0.0
        for idx, weight in zip(self.perf_indices, self.perf_weights):
            name = self.opt_names[idx]
            score += metrics.get(name, 0.0) * weight
        return score

    def evaluate_independence(self, df_output: pl.DataFrame, exprs_list: List[Tuple], new_results: Dict) -> Dict:
        """
        [è°ƒç”¨ç‚¹: å› å­è¯„ä¼°æ‰¹å¤„ç†é˜¶æ®µ]
        å¯¹å½“å‰æ‰¹æ¬¡çš„å› å­è¿›è¡Œç‹¬ç«‹æ€§æ‰“åˆ†ï¼Œå¹¶å°†å…¶æŒ‡çº¹å¯„å­˜ã€‚

        æ¥å£è¯´æ˜:
            - df_output: åŒ…å«å› å­è®¡ç®—ç»“æœçš„ Polars DataFrameã€‚
            - exprs_list: åŒ…å« (å› å­å, è¡¨è¾¾å¼å¯¹è±¡, _) çš„å…ƒç»„åˆ—è¡¨ã€‚
            - new_results: ç»©æ•ˆè®¡ç®—ç»“æœå­—å…¸ {è¡¨è¾¾å¼å­—ç¬¦ä¸²: {æŒ‡æ ‡å: æŒ‡æ ‡å€¼}}ã€‚

        æ³¨æ„:
            è°ƒç”¨æ­¤å‡½æ•°åï¼Œå¤–éƒ¨å¿…é¡»ç¡®ä¿ Individual å¯¹è±¡æ‰§è¡Œäº†å±æ€§æŒ‚è½½:
            `ind.expr_str = str(simplified_expression)`ã€‚
        """
        if not exprs_list:
            return {}

        self._init_anchor_if_needed(df_output)

        # 1. é‡‡æ ·å¹¶å¯„å­˜æŒ‡çº¹
        df_sampled = self.anchor_df.join(df_output, on=["DATE", "ASSET"], how="inner")

        current_batch_keys = []
        batch_power = {}
        for col_name, expr_obj, _ in exprs_list:
            expr_str = str(expr_obj)
            self.fingerprints_dict[expr_str] = df_sampled[col_name]
            current_batch_keys.append(expr_str)
            batch_power[expr_str] = self._get_power_score(new_results.get(expr_str, {}))

        try:
            # 2. æ„é€ è®¡ç®—çŸ©é˜µ (å†å²ç²¾è‹± + å½“å‰æ‰¹æ¬¡)
            compare_keys = self.elite_keys + current_batch_keys
            matrix_df = pl.DataFrame({k: self.fingerprints_dict[k] for k in compare_keys})
            full_power_map = {**self.elite_power_scores, **batch_power}

            # 3. æ‰§è¡Œå¿«é€Ÿå±‚æ¬¡èšç±»
            cluster_labels = self._run_fast_clustering(matrix_df, self.threshold)
            n_clusters = len(set(cluster_labels.values()))
            logger.debug(f"ğŸ” èšç±»å®Œæˆ | å› å­æ€»æ•°: {len(compare_keys)} | é€»è¾‘ç°‡æ•°: {n_clusters}")

            # 4. ç°‡å†…æ¯”æ­¦ï¼šé”å®šæ¯ä¸ªé€»è¾‘ç±»åˆ«çš„æœ€é«˜å¾—åˆ†
            cluster_max_power = {}
            for col, label in cluster_labels.items():
                p = full_power_map.get(col, -np.inf)
                if label not in cluster_max_power or p > cluster_max_power[label]:
                    cluster_max_power[label] = p

            # 5. è®¡ç®—æœ€ç»ˆç‹¬ç«‹æ€§åˆ†æ•°
            scores = {}
            for expr_str in current_batch_keys:
                label = cluster_labels[expr_str]
                # åªæœ‰ç°‡å†…è¡¨ç°æœ€å¥½çš„å› å­èƒ½è·å¾— 1.0 (æ»¡åˆ†ç‹¬ç«‹æ€§)
                if batch_power[expr_str] >= cluster_max_power[label] - 1e-9:
                    scores[expr_str] = 1.0
                else:
                    scores[expr_str] = self.penalty_factor

            logger.debug(f"ğŸ“Š ç‹¬ç«‹æ€§è¯„ä¼°å®Œæˆ | æ‰¹æ¬¡å› å­: {len(current_batch_keys)} | ç²¾è‹±åº“è§„æ¨¡: {len(self.elite_keys)}")
            return scores

        except Exception as e:
            logger.error(f"âŒ èšç±»è¿‡ç¨‹å¼‚å¸¸: {e}")
            return {str(item[1]): 1.0 for item in exprs_list}

    def _run_fast_clustering(self, df: pl.DataFrame, threshold: float):
        """æ ¸å¿ƒèšç±»é€»è¾‘ï¼šå¸¦æ•°å€¼ç¨³å®šæ€§ä¿®å¤"""
        # ä½¿ç”¨ç§©å˜æ¢è®¡ç®—ç›¸å…³æ€§
        rank_df = df.fill_null(0).select([pl.col(c).rank() for c in df.columns])
        corr_array = np.nan_to_num(rank_df.corr().to_numpy(), nan=0.0)

        # 1. è·ç¦»è½¬æ¢: d = sqrt(2 * (1 - |rho|))
        dist_matrix = np.sqrt(np.clip(2 * (1 - np.abs(corr_array)), 0, None))

        # --- æ ¸å¿ƒä¿®å¤ï¼šå¤„ç†æµ®ç‚¹æ•°éå¯¹ç§°é—®é¢˜ ---
        dist_matrix = (dist_matrix + dist_matrix.T) / 2
        np.fill_diagonal(dist_matrix, 0)
        # ------------------------------------

        # ä½¿ç”¨ fastcluster æå‡ linkage æ•ˆç‡
        Z = fastcluster.linkage(squareform(dist_matrix), method='complete')

        # åˆ‡å‰²èšç±»æ ‘
        t_val = np.sqrt(2 * (1 - threshold))
        labels = fcluster(Z, t=t_val, criterion='distance')

        return dict(zip(df.columns, labels))

    def update_and_prune(self, halloffame: tools.HallOfFame):
        """
        [è°ƒç”¨ç‚¹: è¿›åŒ–å¾ªç¯ä»£æœ«]
        ä»£æœ«å‰ªæã€‚æ ¹æ® HoF æˆå‘˜æºå¸¦çš„ `expr_str` å±æ€§è¿›è¡ŒæŒ‡çº¹æ¸…ç†ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸ã€‚

        ä¾èµ–è¯´æ˜:
            - halloffame å†…éƒ¨çš„ä¸ªä½“å¯¹è±¡å¿…é¡»å·²ç»è¿‡ fill_fitness é˜¶æ®µçš„å±æ€§æŒ‚è½½ã€‚
        """
        before_count = len(self.fingerprints_dict)
        new_fingerprints = {}
        self.elite_keys = []
        self.elite_power_scores = {}

        for ind in halloffame:
            # åè®®è¯»å–ï¼šå°è¯•è·å–æŒ‚è½½çš„æ ‡è¯†ç¬¦
            expr_str = getattr(ind, 'expr_str', str(ind))

            if expr_str in self.fingerprints_dict:
                # è¿ç§»ç²¾è‹±æŒ‡çº¹ï¼Œæœªåœ¨ HoF ä¸­çš„æŒ‡çº¹å°†è¢« GC è‡ªåŠ¨å›æ”¶
                new_fingerprints[expr_str] = self.fingerprints_dict[expr_str]
                self.elite_keys.append(expr_str)

                # åŒæ­¥ç²¾è‹±é€‚åº”åº¦ï¼Œä¾›ä¸‹ä¸€ä»£ evaluate æ—¶è¿›è¡Œç°‡å†…å¯¹æ¯”
                metrics = {name: val for name, val in zip(self.opt_names, ind.fitness.values)}
                self.elite_power_scores[expr_str] = self._get_power_score(metrics)

        self.fingerprints_dict = new_fingerprints
        logger.info(f"ğŸ§¹ æŒ‡çº¹å­—å…¸å‰ªæå®Œæˆ: {before_count} -> {len(self.elite_keys)} (ä¿ç•™ç²¾è‹±)")
