"""
GP å› å­ç”Ÿæˆå™¨ä¸»ç±»

ä½¿ç”¨ DEAP åº“è¿›è¡Œé—ä¼ ç¼–ç¨‹ï¼Œè‡ªåŠ¨åŒ–å› å­æŒ–æ˜ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. åŸºäºé—ä¼ ç¼–ç¨‹è‡ªåŠ¨ç”Ÿæˆå› å­è¡¨è¾¾å¼
2. æ‰¹é‡è®¡ç®—å’Œè¯„ä¼°å› å­é€‚åº”åº¦
3. æ”¯æŒè¿›åŒ–è¿‡ç¨‹çš„æ–­ç‚¹æ¢å¤
4. è‡ªåŠ¨ç¼“å­˜ä¸­é—´ç»“æœ

å…¸å‹ç”¨æ³•ï¼š
    config = {
        "split_date": datetime(2021, 1, 1),
        "batch_size": 50,
        "mu": 100,
        "lambda": 100,
        "hof_size": 100
    }
    generator = GPDeapGenerator(config)
    pop, logbook, hof = generator.run(input_data, n_gen=10, n_pop=500)
"""

import operator
import pickle
import time
from datetime import datetime
from itertools import count
from pathlib import Path
from typing import Any, Dict, List, Tuple


import numpy as np
import polars as pl
from deap import base, creator, gp, tools
from deap.gp import PrimitiveTree
from expr_codegen.tool import ExprTool
from loguru import logger
import more_itertools

from alpha.data_provider import DataProvider
# å¯¼å…¥æ‰“è¿‡è¡¥ä¸çš„ç»„ä»¶å’ŒåŸºç¡€å·¥å…·
from alpha.gp.base import population_to_exprs, filter_exprs, print_population
from alpha.gp.base import RET_TYPE, Expr
from alpha.gp.dependence import DependenceManager
from alpha.gp.ea import eaMuPlusLambda_NSGA2
from alpha.gp.label import label_OO_for_IC, label_OO_for_tradable
from alpha.patch.expr_codegen_patch import apply_expr_codegen_patches
from alpha.polars.utils import CUSTOM_OPERATORS
from alpha.utils.config import settings

from typing import TypeVar
from polars import DataFrame as _pl_DataFrame
from polars import LazyFrame as _pl_LazyFrame

from alpha.utils.schema import F

DataFrame = TypeVar("DataFrame", _pl_LazyFrame, _pl_DataFrame)


# åœ¨è„šæœ¬æœ€ä¸Šæ–¹æˆ– __init__ ä¸­è°ƒç”¨ä¸€æ¬¡å³å¯
apply_expr_codegen_patches()

class GPDeapGenerator(object):
    """
    é—ä¼ ç¼–ç¨‹å› å­ç”Ÿæˆå™¨

    ä½¿ç”¨ DEAP æ¡†æ¶å®ç°çš„è‡ªåŠ¨åŒ–å› å­æŒ–æ˜å¼•æ“ã€‚

    Attributes:
        config (Dict): é…ç½®å‚æ•°å­—å…¸
        split_date (datetime): è®­ç»ƒ/æµ‹è¯•é›†åˆ†å‰²æ—¥æœŸ
        batch_size (int): æ‰¹é‡è®¡ç®—å¤§å°
        save_dir (Path): ç»“æœä¿å­˜ç›®å½•
        mu (int): ç§ç¾¤ä¿ç•™è§„æ¨¡
        lambda_ (int): æ¯ä»£ç”Ÿæˆåä»£è§„æ¨¡
        hof_size (int): åäººå ‚å¤§å°
    """
    def __init__(self, config: Dict[str, Any] = {}) -> None:
        """
        åˆå§‹åŒ– GP å› å­ç”Ÿæˆå™¨

        Args:
            config: é…ç½®å­—å…¸ï¼Œæ”¯æŒçš„é”®ï¼š
                - split_date (datetime): åˆ†å‰²æ—¥æœŸï¼Œè®­ç»ƒé›†äºéªŒè¯é›†çš„åˆ†å‰²æ—¥æœŸï¼Œé»˜è®¤None
                - batch_size (int): æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ 50
                - mu (int): è¿›åŒ–ç®—æ³•çš„ mu å‚æ•°ï¼Œé»˜è®¤ 100
                - lambda (int): è¿›åŒ–ç®—æ³•çš„ lambda å‚æ•°ï¼Œé»˜è®¤ 100
                - hof_size (int): åäººå ‚å¤§å°ï¼Œé»˜è®¤ 100

        Raises:
            ValueError: å¦‚æœé…ç½®å‚æ•°æ— æ•ˆ
        """
        # --- 1. åŸºç¡€ä¿¡æ¯é…ç½® ---
        self.config = config
        self.name = config.get("name", self.__class__.__name__)

        # --- 2. æ•°æ®ä¸æ—¥æœŸé…ç½® ---
        self.start_date = config.get("start_date", "20190101")
        self.end_date = config.get("end_date", "20241231")
        # åˆ†å‰²æ—¥æœŸï¼Œè®­ç»ƒé›†ä¸éªŒè¯é›†çš„åˆ†å‰²æ—¥æœŸï¼Œé»˜è®¤None
        self.split_date = config.get("split_date", None)
        # å¤šç›®æ ‡ä¼˜åŒ–åç§°åŠæƒé‡
        # è¿™é‡Œçš„åç§°å’Œæƒé‡è¦å’Œfitness_population_func è¾“å‡ºæŒ‡æ ‡ä¿æŒä¸€è‡´
        # complexityï¼Œè¡¨ç¤ºå› å­å¤æ‚åº¦ï¼Œå¯ä»¥ä¸åŒ…å«åœ¨fitness_population_funcè¾“å‡ºæŒ‡æ ‡ä¸­
        self.opt_names = config.get("opt_names",("ic_mean_abs", "ic_ir_abs",'complexity'))  #
        self.opt_weights = config.get("opt_weights",(1.0, 1.0,-0.01))  # å¤šç›®æ ‡ä¼˜åŒ–æƒé‡
        # æ•´ä½“ç§ç¾¤fitnesså‡½æ•°,
        # è¾“å…¥å‚æ•°ä¸º:df,factorsï¼ˆæ‰€æœ‰çš„å› å­åˆ—åï¼‰,split_date(å¯ä»¥æ²¡æœ‰ï¼Œè®­ç»ƒé›†ä¸éªŒè¯é›†çš„åˆ†å‰²æ—¥æœŸ),å…¶å®ƒå‚æ•°é‡‡ç”¨é»˜è®¤å€¼
        # è¾“å‡ºæ•°æ®æ ¼å¼ä¸º: pl.DataFrameï¼Œå¿…é¡»åŒ…å«åˆ—factor,ä»¥åŠopt_namesæ‰€åŒ…å«çš„åˆ—
        self.fitness_population_func = config.get("fitness_population_func", None)

        self.pool_func = config.get("pool_func", None)  # è‚¡ç¥¨æ± å‡½æ•°
        # æ ‡ç­¾è®¡ç®—å‡½æ•°ï¼Œæä¾›fitness_population_funcè®¡ç®—æ‰€éœ€çš„æ ‡ç­¾åˆ—ï¼Œ
        # ç”Ÿæˆçš„æ ‡ç­¾åˆ—åå¿…é¡»å’Œå‡½æ•°æ‰€éœ€åˆ—åä¸€è‡´ï¼Œä¸€èˆ¬ä¸º F.LABEL_FOR_IC å’Œ F.LABEL_FOR_RET
        self.label_funcs = config.get("label_funcs", [label_OO_for_IC,label_OO_for_tradable])
        self.extra_terminal_func = config.get("extra_terminal_func", [])  # é¢å¤–ç»ˆç«¯å› å­è®¡ç®—å‡½æ•°

        self.terminals = config.get('terminals', [])  # ç»ˆç«¯å› å­åˆ—è¡¨
        self.random_window_func = config.get("random_window_func", None)  # éšæœºçª—å£å‡½æ•°

        # --- 4. è¿›åŒ–ç®—æ³•è¶…å‚æ•° ---
        self.mu = config.get("mu", 400) # ç§ç¾¤ä¿ç•™è§„æ¨¡
        self.lambda_ = config.get("lambda", 400)  # æ¯ä»£ç”Ÿæˆåä»£è§„æ¨¡
        self.cxpb = config.get("cxpb", 0.4)  # äº¤å‰æ¦‚ç‡
        self.mutpb = config.get("mutpb", 0.3)  # å˜å¼‚æ¦‚ç‡
        self.hof_size = config.get("hof_size", 1000) # åäººå ‚å¤§å°
        self.batch_size = config.get("batch_size", 200) # æ‰¹å¤„ç†å¤§å°
        self.max_height = config.get("max_height", 3) # æœ€å¤§æ ‘é«˜é™åˆ¶
        # è·¯å¾„è®¾ç½®
        self.save_dir = Path(settings.GP_DEAP_DIR)/ self.name
        self.save_dir.mkdir(parents=True, exist_ok=True)

        self.dep_manager = None  # å› å­ç‹¬ç«‹æ€§ç®¡ç†å™¨ï¼Œç¨ååˆå§‹åŒ–

        logger.info(f"âœ“ GP ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ | æ‰¹å¤§å°: {self.batch_size}")


    def _build_pset(self) -> gp.PrimitiveSetTyped:
        """
        ç²¾ç®€ç‰ˆç®—å­é›†ï¼šä¸“ä¸º LightGBM/ElasticNet ç‰¹å¾å·¥ç¨‹è®¾è®¡
        æ‰€æœ‰ç®—å­å‡è¿”å›æ•°å€¼ç±»å‹ï¼Œç§»é™¤äº†å¯¼è‡´ SchemaError çš„é€»è¾‘ç®—å­
        """
        # ç›´æ¥ä½¿ç”¨ Expr ä½œä¸ºæ ‡è¯†ï¼Œé»˜è®¤å³ä¸ºæµ®ç‚¹æ•°åºåˆ—
        pset = gp.PrimitiveSetTyped("MAIN", [], Expr)
        return pset

    def build_toolbox(self, input_data: pl.LazyFrame) -> base.Toolbox:
        """
        æ„å»ºè¿›åŒ–å·¥å…·ç®±

        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œç”¨äºé€‚åº”åº¦è¯„ä¼°

        Returns:
            base.Toolbox: DEAP å·¥å…·ç®±å®ä¾‹
        """
        creator.create("FitnessMulti", base.Fitness, weights=self.opt_weights)
        creator.create("Individual", PrimitiveTree, fitness=creator.FitnessMulti)

        toolbox = base.Toolbox()

        # æ ‘ç”Ÿæˆç®—æ³•: åŠæ•°åŠèŒæ³• (Half and Half)
        toolbox.register("expr", gp.genHalfAndHalf, pset=self.pset, min_=2, max_=5)
        toolbox.register("individual", tools.initIterate, creator.Individual, toolbox.expr)
        toolbox.register("population", tools.initRepeat, list, toolbox.individual)

        # é—ä¼ ç®—å­: é”¦æ ‡èµ›é€‰æ‹©ã€äº¤å‰ã€å˜å¼‚
        # toolbox.register("select", tools.selTournament, tournsize=3) # å•ç›®æ ‡ä¼˜åŒ–é€‰æ‹©
        toolbox.register("select", tools.selNSGA2)  # å¤šç›®æ ‡ä¼˜åŒ–é€‰æ‹©

        toolbox.register("mate", gp.cxOnePoint)
        toolbox.register("expr_mut", gp.genFull, min_=0, max_=2)
        toolbox.register("mutate", gp.mutUniform, expr=toolbox.expr_mut, pset=self.pset)

        # é™åˆ¶æ ‘é«˜ï¼Œé˜²æ­¢è†¨èƒ€ (Bloat)
        toolbox.decorate("mate", gp.staticLimit(key=operator.attrgetter("height"), max_value=self.max_height))
        toolbox.decorate("mutate", gp.staticLimit(key=operator.attrgetter("height"), max_value=self.max_height))

        # æ ¸å¿ƒï¼šæ‰¹é‡è¯„ä¼°æ˜ å°„
        toolbox.register("evaluate", lambda x: (np.nan, np.nan))  # å®é™…è¯„åˆ†åœ¨ map ä¸­å®Œæˆ
        toolbox.register(
            "map",
            self.map_exprs,
            gen=count(),
            split_date=self.split_date,
            input_data=input_data
        )

        logger.debug("âœ“ Toolbox æ„å»ºå®Œæˆ")
        return toolbox


    def build_statistics(self) -> tools.Statistics:
        """
        å®šä¹‰è¿›åŒ–è¿‡ç¨‹ä¸­çš„ç»Ÿè®¡ç›‘æ§æŒ‡æ ‡

        Returns:
            tools.Statistics: DEAP ç»Ÿè®¡å¯¹è±¡
        """
        stats = tools.Statistics(lambda ind: ind.fitness.values)
        stats.register("avg", np.nanmean, axis=0)
        stats.register("max", np.nanmax, axis=0)
        stats.register("min", np.nanmin, axis=0)
        stats.register("std", np.nanstd, axis=0)
        return stats

    def run(
        self,
        n_gen: int = 10,
        n_pop: int = 1000
    ) -> Tuple[List, Any, tools.HallOfFame]:
        """
        å¯åŠ¨è¿›åŒ–æµç¨‹

        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œå¿…é¡»åŒ…å«æ ‡ç­¾åˆ—
            n_gen: è¿›åŒ–ä»£æ•°ï¼Œé»˜è®¤ 10
            n_pop: åˆå§‹ç§ç¾¤å¤§å°ï¼Œé»˜è®¤ 1000

        Returns:
            Tuple: (æœ€ç»ˆç§ç¾¤, è¿›åŒ–æ—¥å¿—, åäººå ‚)

        Raises:
            ValueError: å¦‚æœè¾“å…¥æ•°æ®æ— æ•ˆ
        """
        # åˆå§‹åŒ–ç®¡ç†å™¨, ç”¨äºå› å­ç‹¬ç«‹æ€§è¯„ä¼°
        self.dep_manager = DependenceManager(
            opt_names=self.opt_names,
            opt_weights=self.opt_weights,
            cluster_threshold=0.7
        )

        # 2. è½½å…¥åŸå§‹æ•°æ®
        # æŒ–æ˜å› å­é€šå¸¸éœ€è¦ OHLCVï¼Œè®¡ç®— OO æ”¶ç›Šç‡éœ€è¦ OPEN
        input_data = DataProvider().load_data(
            start_date=self.start_date,
            end_date=self.end_date,
            funcs=[self.pool_func, *self.label_funcs, self.extra_terminal_func],
            select_cols=[F.POOL_MASK, F.LABEL_FOR_IC,F.LABEL_FOR_RET, *self.terminals],
            cache_path=self.save_dir / f"{self.pool_func.__name__}.parquet"
        )
        logger.info("ğŸ’¾ æ ‡ç­¾æ•°æ®å·²å°±ç»ª")

        logger.info(f"ğŸš€ å¯åŠ¨ GP è¿›åŒ– | ä»£æ•°: {n_gen} | ç§ç¾¤: {n_pop}")
        self.pset = self._build_pset()
        toolbox = self.build_toolbox(input_data)
        stats = self.build_statistics()
        hof = tools.HallOfFame(self.hof_size)
        # å¤šç›®æ ‡ selNSGA2
        # hof = tools.ParetoFront(similar=operator.eq)

        # åˆå§‹åŒ–ç§ç¾¤
        pop = toolbox.population(n=n_pop)
        logger.info(f"âœ“ åˆå§‹ç§ç¾¤å·²ç”Ÿæˆ | å¤§å°: {len(pop)}")

        # æ‰§è¡Œè¿›åŒ–
        logger.info("â–¶ï¸ å¼€å§‹é—ä¼ ç¼–ç¨‹è¿›åŒ–...")
        pop, logbook = eaMuPlusLambda_NSGA2(
            pop, toolbox,
            mu=self.mu,
            lambda_=self.lambda_,
            cxpb= self.cxpb,  # äº¤å‰æ¦‚ç‡
            mutpb= self.mutpb,  # å˜å¼‚æ¦‚ç‡
            ngen=n_gen,
            stats=stats,
            halloffame=hof,
            verbose=True,
            generator = self
        )

        # ä¿å­˜åäººå ‚
        hof_path = self.save_dir / 'best_hof.pkl'
        try:
            with open(hof_path, 'wb') as f:
                pickle.dump(hof, f)
            logger.info(f"ğŸ’¾ åäººå ‚å·²ä¿å­˜è‡³: {hof_path}")
        except Exception as e:
            logger.error(f"âŒ åäººå ‚ä¿å­˜å¤±è´¥: {e}")

        logger.info(f"âœ¨ GP è¿›åŒ–å®Œæˆ | æœ€ç»ˆç§ç¾¤: {len(pop)} | åäººå ‚: {len(hof)}")

        print('=' * 60)
        print(logbook)

        print('=' * 60)
        print_population(hof, globals().copy())
        self.export_hof_to_csv(hof, globals().copy())
        return pop, logbook, hof


    def map_exprs(
        self,
        evaluate_func: Any,
        individuals: List,
        gen,
        split_date: datetime,
        input_data: pl.DataFrame
    ) -> List[Tuple]:
        """
        æ‰¹é‡è®¡ç®—ç§ç¾¤é€‚åº”åº¦çš„æ ¸å¿ƒæ–¹æ³•

        å¤„ç†æµç¨‹ï¼š
        1. å¤‡ä»½å½“å‰ä»£çš„è¡¨è¾¾å¼
        2. åŠ è½½å†å²é€‚åº”åº¦ç¼“å­˜
        3. æå–å¹¶è¿‡æ»¤è¡¨è¾¾å¼
        4. æ‰¹é‡è®¡ç®—æ–°è¡¨è¾¾å¼çš„é€‚åº”åº¦
        5. æ›´æ–°ç¼“å­˜å¹¶è¿”å›ç»“æœ

        Args:
            evaluate_func: è¯„ä¼°å‡½æ•°ï¼ˆæœªä½¿ç”¨ï¼Œç”± map è°ƒç”¨è¦æ±‚ï¼‰
            individuals: å½“å‰ä»£çš„ä¸ªä½“åˆ—è¡¨
            gen: ä»£æ•°è¿­ä»£å™¨
            split_date: è®­ç»ƒ/æµ‹è¯•åˆ†å‰²æ—¥æœŸ
            input_data: è¾“å…¥æ•°æ®

        Returns:
            List[Tuple]: æ¯ä¸ªä¸ªä½“çš„é€‚åº”åº¦å…ƒç»„åˆ—è¡¨
        """
        g = next(gen)
        logger.info(f">>> ç¬¬ {g} ä»£ | ç§ç¾¤å¤§å°: {len(individuals)}")

        # 2. ç¼“å­˜ç®¡ç†
        cache_path = self.save_dir / 'fitness_cache.pkl'
        fitness_results: Dict = {} # è¡¨è¾¾å¼å­—ç¬¦ä¸² -> é€‚åº”åº¦å…ƒç»„
        if cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    fitness_results = pickle.load(f)
                logger.debug(f"âœ“ åŠ è½½å†å²ç¼“å­˜ | å·²æœ‰ç»“æœ: {len(fitness_results)}")
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")

        # 3. è¡¨è¾¾å¼æ¸…æ´—ä¸è¿‡æ»¤
        logger.debug("ğŸ”„ è½¬æ¢ DEAP æ ‘ -> Sympy è¡¨è¾¾å¼...")
        exprs_list = population_to_exprs(individuals, globals().copy())
        exprs_to_calc = filter_exprs(exprs_list, self.pset, RET_TYPE, fitness_results)

        logger.info(f"ğŸ“Š éœ€è®¡ç®—: {len(exprs_to_calc)} / {len(exprs_list)} ä¸ªè¡¨è¾¾å¼")

        # 4. æ‰¹é‡è®¡ç®—
        if len(exprs_to_calc) > 0:
            for batch_id, batch in enumerate(more_itertools.batched(exprs_to_calc, self.batch_size)):
                logger.debug(f"  æ‰¹æ¬¡ {batch_id + 1} | å¤§å°: {len(list(batch))}")
                new_scores = self.batched_exprs(batch_id, list(batch), g, split_date, input_data)
                fitness_results.update(new_scores)

            # æ›´æ–°å…¨å±€ç¼“å­˜
            try:
                with open(cache_path, 'wb') as f:
                    pickle.dump(fitness_results, f)
                logger.debug(f"âœ“ ç¼“å­˜å·²æ›´æ–° | æ€»ç»“æœæ•°: {len(fitness_results)}")
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

        # 5. å›å¡«é€‚åº”åº¦ï¼ˆå¯åŠ å…¥æƒ©ç½šï¼‰
        fitness_values = self.fill_fitness(individuals,exprs_list, fitness_results)
        logger.info(f"âœ“ ç¬¬ {g} ä»£è¯„ä¼°å®Œæˆ")
        return fitness_values

    def batched_exprs(self, batch_id, exprs_list, gen, split_date, df_input):
        """æ¯ä»£ç§ç¾¤åˆ†æ‰¹è®¡ç®—ï¼ŒåŒ…å«è¯¦ç»†æ€§èƒ½æ—¥å¿—åŠå¹³å‡ç”¨æ—¶"""
        if len(exprs_list) == 0:
            return {}

        lf = df_input.lazy() if isinstance(df_input, pl.DataFrame) else df_input

        tool = ExprTool()
        codes, G = tool.all(exprs_list, style='polars', template_file='template.py.j2',
                            replace=False, regroup=True, format=True,
                            date='DATE', asset='ASSET', over_null=None,
                            skip_simplify=True)

        cnt = len(exprs_list)
        globals_ = {**CUSTOM_OPERATORS}
        exec(codes, globals_)

        # --- é˜¶æ®µ A: å› å­å€¼è®¡ç®— ---
        logger.info("ç¬¬{}ä»£-ç¬¬{}æ‰¹ï¼šå¼€å§‹è®¡ç®—å› å­å€¼ (å…± {} æ¡)", gen, batch_id, cnt)
        tic_calc = time.perf_counter()

        df_output = globals_['main'](lf, ge_date_idx=0).collect()

        toc_calc = time.perf_counter()
        calc_duration = toc_calc - tic_calc

        # æ—¥å¿—è¾“å‡ºï¼šæ·»åŠ é€Ÿåº¦å’Œå¹³å‡è€—æ—¶
        logger.info(
            "ç¬¬{}ä»£-ç¬¬{}æ‰¹ï¼šè®¡ç®—å®Œæˆã€‚æ€»è€—æ—¶: {:.3f}s | é€Ÿåº¦: {:.2f} æ¡/s | å¹³å‡: {:.4f}s/æ¡",
            gen, batch_id, calc_duration, cnt / calc_duration, calc_duration / cnt
        )



        # --- é˜¶æ®µ B: é€‚åº”åº¦è®¡ç®— ---
        logger.info("ç¬¬{}ä»£-ç¬¬{}æ‰¹ï¼šå¼€å§‹èšåˆè®¡ç®— IC/RET é€‚åº”åº¦æŒ‡æ ‡", gen, batch_id)
        tic_fit = time.perf_counter()

        factor_columns = [k for k, v, c in exprs_list]
        import inspect
        if split_date and 'split_date' in inspect.signature(self.fitness_population_func).parameters:
            fitness_df = self.fitness_population_func(df_output, factors=factor_columns, split_date=split_date)
        else:
            fitness_df = self.fitness_population_func(df_output, factors=factor_columns)

        toc_fit = time.perf_counter()
        fit_duration = toc_fit - tic_fit

        logger.info(
            "ç¬¬{}ä»£-ç¬¬{}æ‰¹ï¼šèšåˆå®Œæˆã€‚è€—æ—¶: {:.3f}s | å¹³å‡: {:.4f}s/æ¡",
            gen, batch_id, fit_duration, fit_duration / cnt
        )

        # 3. ç»“æœè½¬æ¢
        key_to_expr = {k: str(v) for k, v, c in exprs_list}
        new_results = {}
        for row in fitness_df.to_dicts():
            f_name = row.pop("factor")
            # è·å–å¯¹åº”çš„è¡¨è¾¾å¼å­—ç¬¦ä¸²ä½œä¸º Key
            expr_str = key_to_expr[f_name]
            new_results[expr_str] = row

        if "independence" in self.opt_names:
            # è¿™é‡Œçš„ exprs_list åŒ…å«äº† (å› å­å, è¡¨è¾¾å¼å¯¹è±¡, å¤æ‚åº¦)
            indep_map = self.dep_manager.evaluate_independence(df_output, exprs_list,new_results)
            for expr_str, indep_score in indep_map.items():
                if expr_str in new_results:
                    new_results[expr_str]['independence'] = indep_score
        # 4. æ±‡æ€»
        total_dur = calc_duration + fit_duration
        logger.info(
            "ç¬¬{}ä»£-ç¬¬{}æ‰¹ï¼šæµç¨‹ç»“æŸã€‚æ€»è®¡: {:.3f}s | æ€»å¹³å‡: {:.4f}s/æ¡ (ç®—å€¼:{:.1%}, æŒ‡æ ‡:{:.1%})",
            gen, batch_id, total_dur, total_dur / cnt, calc_duration / total_dur, fit_duration / total_dur
        )

        return new_results

    def fill_fitness(self, individuals, exprs_old, fitness_results):
        """
        æ ¹æ®æƒ¯ä¾‹å¤„ç†å¹¶è¿”å› Fitness å…ƒç»„åˆ—è¡¨ã€‚
        åŒæ—¶åŸåœ°æ›´æ–°ä¸ªä½“çš„ stats å±æ€§ã€‚

        Args:
            individuals: DEAP ä¸ªä½“åˆ—è¡¨ [ind1, ind2, ...]
            exprs_old: è¾…åŠ©ä¿¡æ¯ [(k, v, c), ...]ï¼Œå…¶ä¸­ v æ˜¯è¡¨è¾¾å¼å¯¹è±¡æˆ–å­—ç¬¦ä¸²
            fitness_results: è®¡ç®—ç»“æœå­—å…¸ {str(v): {metrics_dict}}

        Returns:
            List[Tuple]: å¯¹åº”æ¯ä¸ªä¸ªä½“çš„é€‚åº”åº¦å…ƒç»„åˆ—è¡¨ï¼Œä¾‹å¦‚ [(ic, ir, comp), ...]
        """
        # 0. é•¿åº¦å®‰å…¨æ€§æ£€æŸ¥
        if len(individuals) != len(exprs_old):
            raise ValueError(f"æ•°æ®å¯¹é½å¤±è´¥: individuals({len(individuals)}) != exprs_old({len(exprs_old)})")

        # 1. é¢„è®¡ç®—æƒ©ç½šå‘é‡ (æ ¹æ® opt_weights ç¬¦å·ç¡®å®šæƒ©ç½šæ–¹å‘)
        # è‹¥æƒé‡ä¸ºæ­£(æ±‚æœ€å¤§)ï¼Œæƒ©ç½šå€¼ä¸º 0.0ï¼›è‹¥æƒé‡ä¸ºè´Ÿ(æ±‚æœ€å°)ï¼Œæƒ©ç½šå€¼ä¸º 999.0
        penalty_values = tuple(0.0 if w > 0 else 999.0 for w in self.opt_weights)

        fit_tuples_list = []

        # 2. éå†ä¸ªä½“ä¸å¯¹åº”çš„è¡¨è¾¾å¼æè¿°
        # v è¡¨ç¤ºå› å­è¡¨è¾¾å¼å­—ç¬¦ä¸²
        for ind, (_, v, _) in zip(individuals, exprs_old):
            # ç»Ÿä¸€ä½¿ç”¨å­—ç¬¦ä¸²é”®åŒ¹é…ç»“æœå­—å…¸
            search_key = str(v)
            ind.expr_str = search_key  # åŸåœ°æŒ‚è½½è¡¨è¾¾å¼å­—ç¬¦ä¸²ï¼Œä¾¿äºåç»­æŸ¥è¯¢
            score_dict = fitness_results.get(search_key)

            # æƒ…å†µ A: åŒ¹é…å¤±è´¥ (è¯¥å› å­å› éæ³•ã€é‡å¤è¢«è¿‡æ»¤ï¼Œæˆ–è®¡ç®—æ¨¡å—æŠ¥é”™)
            if score_dict is None:
                fit_tuples_list.append(penalty_values)
                ind.stats = None  # æ¸…ç©ºæˆ–åˆå§‹åŒ– stats
                continue

            if self.is_penalty(score_dict):
                fit_tuples_list.append(penalty_values)
                ind.stats = None  # æ¸…ç©ºæˆ–åˆå§‹åŒ– stats
                continue

            # æƒ…å†µ B: åŒ¹é…æˆåŠŸï¼Œæ ¹æ® self.opt_names æå–æŒ‡æ ‡
            try:
                current_fit = []
                for i, name in enumerate(self.opt_names):
                    if name == "complexity":
                        # ç›´æ¥è·å– DEAP æ ‘çš„èŠ‚ç‚¹æ•°ä½œä¸ºå¤æ‚åº¦
                        val = float(len(ind))
                    else:
                        # ä»ç»“æœå­—å…¸æå–æŒ‡æ ‡ï¼Œè‹¥ Key ä¸å­˜åœ¨åˆ™ç›´æ¥è§¦å‘ KeyError (é…ç½®é”™è¯¯)
                        try:
                            raw_val = score_dict[name]

                            # æ ¸å¿ƒé˜²å¾¡ï¼šå¤„ç†è®¡ç®—ç»“æœä¸­çš„ NaN æˆ– Infï¼Œé˜²æ­¢ Logbook å´©æºƒ
                            if raw_val is None or not np.isfinite(raw_val):
                                val = penalty_values[i]
                            else:
                                val = float(raw_val)
                        except KeyError:
                            logger.error(f"âŒ æŒ‡æ ‡é…ç½®é”™è¯¯: '{name}' ä¸åœ¨è®¡ç®—ç»“æœä¸­!")
                            logger.error(f"å½“å‰å¯ç”¨æŒ‡æ ‡: {list(score_dict.keys())}")
                            raise KeyError(f"Metric '{name}' missing in fitness_results.")

                    current_fit.append(val)

                # è½¬æ¢ä¸ºå…ƒç»„å¹¶å­˜å…¥ç»“æœåˆ—è¡¨
                fit_tuple = tuple(current_fit)
                fit_tuples_list.append(fit_tuple)

                # åŸåœ°æŒ‚è½½å…¨é‡æŒ‡æ ‡å­—å…¸ï¼Œæ–¹ä¾¿åéªŒåˆ†æ
                ind.stats = score_dict

            except Exception as e:
                logger.error(f"å¤„ç†ä¸ªä½“é€‚åº”åº¦å¼‚å¸¸: {e} | è¡¨è¾¾å¼: {search_key}")
                fit_tuples_list.append(penalty_values)
                ind.stats = None

        return fit_tuples_list

    def is_penalty(self,score_dict):
        """åˆ¤æ–­æŸä¸ªè®¡ç®—ç»“æœæ˜¯å¦ä¸ºæƒ©ç½šå€¼"""
        if 'ic_ir' in score_dict:
            val = score_dict['ic_ir']
            if np.isnan(val) or val < 0.0001:
                return True
        return False

    def export_hof_to_csv(self, hof, globals_, filename="best_factors.csv"):
        """
        å°†åäººå ‚å†…å®¹å¯¼å‡ºåˆ° CSV

        Args:
            hof: åäººå ‚å¯¹è±¡
            globals_: å…¨å±€å‘½åç©ºé—´ globals()
            filename: è¾“å‡ºæ–‡ä»¶å
        """
        import pandas as pd
        # exprs_list å¾—åˆ°çš„æ˜¯ (ç®€åŒ–å k, è¡¨è¾¾å¼æ–‡æœ¬ v, å¤æ‚åº¦ c)
        exprs_list = population_to_exprs(hof, globals_)
        data = []
        for (k, v, c), ind in zip(exprs_list, hof):
            kvs = {
                "factor_name": k,  # å› å­ç®€åŒ–å
                "expression": v,  # ç®€åŒ–åçš„è¡¨è¾¾å¼æ–‡æœ¬ (v)
                "complexity": c,  # å¤æ‚åº¦ (c)
                "raw_tree": str(ind),  # åŸå§‹ DEAP æ ‘ç»“æ„
            }
            # æå–åäººå ‚ä¸ªä½“çš„é€‚åº”åº¦å€¼å¹¶å­˜å‚¨åˆ°å­—å…¸ä¸­
            for name, value in zip(self.opt_names, ind.fitness.values):
                kvs[name] = value
            data.append(kvs)

        # 2. è½¬æ¢ä¸º DataFrame å¹¶ä¿å­˜
        df = pd.DataFrame(data)
        output_path = self.save_dir / filename
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        logger.info(f"âœ… åäººå ‚å› å­å·²å¯¼å‡ºè‡³ CSV: {output_path}")
        return df
