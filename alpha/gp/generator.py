"""
GP å› å­ç”Ÿæˆå™¨ä¸»ç±»

ä½¿ç”¨ DEAP åº“è¿›è¡Œé—ä¼ ç¼–ç¨‹ï¼Œè‡ªåŠ¨åŒ–å› å­æŒ–æ˜ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. åŸºäºé—ä¼ ç¼–ç¨‹è‡ªåŠ¨ç”Ÿæˆå› å­è¡¨è¾¾å¼
2. æ‰¹é‡è®¡ç®—å’Œè¯„ä¼°å› å­é€‚åº”åº¦
3. æ”¯æŒè¿›åŒ–è¿‡ç¨‹çš„æ–­ç‚¹æ¢å¤
4. è‡ªåŠ¨ç¼“å­˜ä¸­é—´ç»“æœ

å…¸å‹ç”¨æ³•ï¼š
    config = {
        "label_y": "RETURN_OO_1",
        "split_date": datetime(2021, 1, 1),
        "batch_size": 50,
        "mu": 100,
        "lambda": 100,
        "hof_size": 100
    }
    generator = GPDeapGenerator(config)
    pop, logbook, hof = generator.run(input_data, n_gen=10, n_pop=500)
"""

import operator
import pickle
import random
from datetime import datetime
from itertools import count
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np
import polars as pl
from deap import base, creator, gp, tools
from deap.gp import PrimitiveSetTyped, PrimitiveTree
from loguru import logger
import more_itertools
import polars.selectors as cs

from alpha.data_provider import DataProvider
# å¯¼å…¥æ‰“è¿‡è¡¥ä¸çš„ç»„ä»¶å’ŒåŸºç¡€å·¥å…·
from alpha.gp.deap_patch import eaMuPlusLambda  # æ ¸å¿ƒè¿›åŒ–ç®—æ³•
from alpha.gp.base import population_to_exprs, filter_exprs, print_population, dummy
# from alpha.gp.cs.helper import batched_exprs, fill_fitness
from alpha.gp.base import RET_TYPE, Expr
from alpha.utils.config import settings



class GPDeapGenerator(object):
    """
    é—ä¼ ç¼–ç¨‹å› å­ç”Ÿæˆå™¨

    ä½¿ç”¨ DEAP æ¡†æ¶å®ç°çš„è‡ªåŠ¨åŒ–å› å­æŒ–æ˜å¼•æ“ã€‚

    Attributes:
        config (Dict): é…ç½®å‚æ•°å­—å…¸
        label_y (str): ç›®æ ‡æ ‡ç­¾åˆ—å
        split_date (datetime): è®­ç»ƒ/æµ‹è¯•é›†åˆ†å‰²æ—¥æœŸ
        batch_size (int): æ‰¹é‡è®¡ç®—å¤§å°
        save_dir (Path): ç»“æœä¿å­˜ç›®å½•
        mu (int): ç§ç¾¤ä¿ç•™è§„æ¨¡
        lambda_ (int): æ¯ä»£ç”Ÿæˆåä»£è§„æ¨¡
        hof_size (int): åäººå ‚å¤§å°
    """

    def __init__(self, config: Dict[str, Any] = {}) -> None:
        """
        åˆå§‹åŒ– GP å› å­ç”Ÿæˆå™¨

        Args:
            config: é…ç½®å­—å…¸ï¼Œæ”¯æŒçš„é”®ï¼š
                - label_y (str): ç›®æ ‡åˆ—åï¼Œé»˜è®¤ "RETURN_OO_1"
                - split_date (datetime): åˆ†å‰²æ—¥æœŸï¼Œé»˜è®¤ 2021-01-01
                - batch_size (int): æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ 50
                - mu (int): è¿›åŒ–ç®—æ³•çš„ mu å‚æ•°ï¼Œé»˜è®¤ 100
                - lambda (int): è¿›åŒ–ç®—æ³•çš„ lambda å‚æ•°ï¼Œé»˜è®¤ 100
                - hof_size (int): åäººå ‚å¤§å°ï¼Œé»˜è®¤ 100

        Raises:
            ValueError: å¦‚æœé…ç½®å‚æ•°æ— æ•ˆ
        """
        """
                åˆå§‹åŒ–é…ç½®ï¼Œå°†æ‰€æœ‰ config å–å€¼é›†ä¸­åœ¨æ­¤
                """
        # --- 1. åŸºç¡€ä¿¡æ¯é…ç½® ---
        self.config = config
        self.name = config.get("name", self.__class__.__name__)

        # --- 2. æ•°æ®ä¸æ—¥æœŸé…ç½® ---
        self.start_date = config.get("start_date", "20190101")
        self.end_date = config.get("end_date", "20241231")
        self.split_date = config.get("split_date", datetime(2022, 1, 1))
        self.overwrite = config.get("overwrite_data", False)

        # --- 3. æ ‡ç­¾è®¡ç®—é…ç½® ---
        self.label_window = config.get("label_window", 1) # è®¡ç®—æ ‡ç­¾çš„æœªæ¥çª—å£å¤§å°
        self.label_y = config.get("label_y", f"RETURN_OO_{self.label_window}")  # ç›®æ ‡æ ‡ç­¾åˆ—å,å½“å‰ä»…æ”¯æŒ OPEN-OPEN æ”¶ç›Šç‡

        # --- 4. è¿›åŒ–ç®—æ³•è¶…å‚æ•° ---
        self.mu = config.get("mu", 150) # ç§ç¾¤ä¿ç•™è§„æ¨¡
        self.lambda_ = config.get("lambda", 100)  # æ¯ä»£ç”Ÿæˆåä»£è§„æ¨¡
        self.cxpb = config.get("cxpb", 0.5)  # äº¤å‰æ¦‚ç‡
        self.mutpb = config.get("mutpb", 0.1)  # å˜å¼‚æ¦‚ç‡
        self.hof_size = config.get("hof_size", 1000) # åäººå ‚å¤§å°
        self.batch_size = config.get("batch_size", 100) # æ‰¹å¤„ç†å¤§å°
        self.max_height = config.get("max_height", 17) # æœ€å¤§æ ‘é«˜é™åˆ¶


        # éªŒè¯é…ç½®
        if self.batch_size <= 0:
            raise ValueError(f"batch_size å¿…é¡»å¤§äº 0ï¼Œå½“å‰å€¼: {self.batch_size}")
        if not isinstance(self.label_y, str) or not self.label_y:
            raise ValueError(f"label_y å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²ï¼Œå½“å‰å€¼: {self.label_y}")


        # è·¯å¾„è®¾ç½®
        self.save_dir = Path(settings.GP_DEAP_DIR)/ self.name
        self.save_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"âœ“ GP ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ | æ ‡ç­¾: {self.label_y} | æ‰¹å¤§å°: {self.batch_size}")

        self.pset = self._build_pset()
        self._setup_deap_creator()

    def _prepare_labeled_data(self) -> pl.DataFrame:
        """
        ä» DataProvider è·å–æ•°æ®å¹¶è®¡ç®—æŒ–æ˜æ ‡ç­¾
        è®¡ç®—é€»è¾‘ï¼šæœªæ¥ N æ—¥çš„ Open-to-Open æ”¶ç›Šç‡
        """

        cache_file = self.save_dir / f"labeled_{self.label_y}.parquet"

        data_provider = DataProvider()

        if not self.overwrite and cache_file.exists():
            logger.info(f"ğŸ“‚ å‘ç°æ ‡ç­¾æ•°æ®ç¼“å­˜ï¼Œç›´æ¥åŠ è½½: {cache_file}")
            return pl.read_parquet(cache_file)

        logger.info(f"ğŸ“¡ æ­£åœ¨è®¡ç®—æ ‡ç­¾ '{self.label_y}'...")


        # 2. è½½å…¥åŸå§‹æ•°æ®
        # æŒ–æ˜å› å­é€šå¸¸éœ€è¦ OHLCVï¼Œè®¡ç®— OO æ”¶ç›Šç‡éœ€è¦ OPEN
        lf = data_provider.load_data(
            start_date= self.start_date,
            end_date=self.end_date,
            columns=["DATE", "ASSET", "OPEN", "CLOSE", "HIGH", "LOW", "VOLUME",'AMOUNT']
        )

        # 3. è®¡ç®—æœªæ¥æ”¶ç›Šç‡ (Label Generation)
        # è®¡ç®—å…¬å¼: (æœªæ¥ç¬¬ N+1 æ—¥å¼€ç›˜ä»· / æœªæ¥ç¬¬ 1 æ—¥å¼€ç›˜ä»·) - 1
        # æ³¨æ„ï¼šOO_1 å®é™…ä¸Šéœ€è¦ shift(-2) å’Œ shift(-1)
        lf = lf.sort(["ASSET", "DATE"]).with_columns([
            (
                    (pl.col("OPEN").shift(-(self.label_window  + 1)).over("ASSET") /
                     pl.col("OPEN").shift(-1).over("ASSET")) - 1
            ).alias(self.label_y)
        ])

        # 4. æˆªé¢ä¸­æ€§åŒ–ä¸å»æå€¼ (Z-Score)
        # è¿™ä¸€æ­¥å°†æ”¶ç›Šç‡è½¬åŒ–ä¸ºâ€œè¯¥è‚¡ç¥¨åœ¨å½“æ—¥å…¨å¸‚åœºä¸­çš„ç›¸å¯¹å¼ºåº¦â€
        date_col = "DATE"
        lf = lf.filter(pl.col(self.label_y).is_not_null()).with_columns([
            (
                    (pl.col(self.label_y) - pl.col(self.label_y).mean().over(date_col))
                    / (pl.col(self.label_y).std().over(date_col) + 1e-6)
            )
            .clip(-3, 3)  # é™åˆ¶åœ¨æ­£è´Ÿ 3 ä¸ªæ ‡å‡†å·®å†…ï¼Œæ¶ˆé™¤å¦–è‚¡å™ªå£°
            .fill_nan(0.0)
            .alias(self.label_y)
        ])
        lf = lf.sort(['ASSET', 'DATE']).with_columns([
            pl.col("ASSET").set_sorted(),
            # å¼ºåˆ¶å°†æ‰€æœ‰æ•°å€¼åˆ—è½¬ä¸º Float64ï¼Œé¿å… GP è¿è¡Œæ—¶ SchemaError
            cs.numeric().cast(pl.Float64)
        ])
        # 5. æ‰§è¡Œè®¡ç®—å¹¶æŒä¹…åŒ–
        df = lf.collect()

        if df.height == 0:
            raise ValueError("è®¡ç®—åæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ DataProvider è¿”å›ç»“æœæˆ–æ—¥æœŸèŒƒå›´")

        df.write_parquet(cache_file, compression="snappy")
        logger.info(f"ğŸ’¾ æ ‡ç­¾æ•°æ®å·²å°±ç»ª | è¡Œæ•°: {df.height:,} | å‡å€¼: {df[self.label_y].mean():.4f}")

        return df

    def run_workflow(self, n_gen: int = 10) -> Tuple[List, Any, Any]:
        """
        å…¨æµç¨‹ä¸€é”®å¯åŠ¨ï¼šæ•°æ®å‡†å¤‡ -> è¿›åŒ–æŒ–æ˜

        Args:
            n_gen: è¿›åŒ–ä»£æ•°ï¼Œé»˜è®¤ 10
        Returns:
            Tuple[List, logbook, HallOfFame]: (æœ€ç»ˆç§ç¾¤, è¿›åŒ–æ—¥å¿—, åäººå ‚)
        """
        logger.info("=" * 60)
        logger.info("GP å› å­æŒ–æ˜å…¨æµç¨‹å¯åŠ¨")
        logger.info("=" * 60)

        # æ­¥éª¤ 1: æ•°æ®å‡†å¤‡
        logger.info(">>> æ­¥éª¤ 1/2: æ•°æ®å‡†å¤‡")
        input_df = self._prepare_labeled_data()

        # æ­¥éª¤ 2: å¯åŠ¨è¿›åŒ–
        logger.info(">>> æ­¥éª¤ 2/2: å¯åŠ¨é—ä¼ ç¼–ç¨‹è¿›åŒ–")
        result = self.run(input_df, n_gen=n_gen)

        logger.info("=" * 60)
        logger.info("âœ… å…¨æµç¨‹æ‰§è¡Œå®Œæˆ")
        logger.info("=" * 60)

        return result

    def _setup_deap_creator(self) -> None:
        """
        åˆå§‹åŒ– DEAP creator ç±»å‹ç³»ç»Ÿ

        åˆ›å»ºè‡ªå®šä¹‰çš„ Fitness å’Œ Individual ç±»å‹ï¼Œç”¨äºé—ä¼ ç¼–ç¨‹ã€‚
        ä½¿ç”¨å¤šç›®æ ‡ä¼˜åŒ–ï¼š(IC_train, IC_test) æˆ– (IC, IR)

        Note:
            creator.create ä¼šä¿®æ”¹å…¨å±€çŠ¶æ€ï¼Œé‡å¤è°ƒç”¨ä¼šæŠ¥é”™ã€‚
            è¿™é‡Œä½¿ç”¨ hasattr æ£€æŸ¥é¿å…é‡å¤åˆ›å»ºã€‚
        """
        # weights=(1.0, 1.0) ä»£è¡¨åŒæ—¶æœ€å¤§åŒ–æ ·æœ¬å†…å’Œæ ·æœ¬å¤– IC
        if not hasattr(creator, "FitnessMulti"):
            creator.create("FitnessMulti", base.Fitness, weights=(1.0, 1.0))
            logger.debug("âœ“ åˆ›å»º FitnessMulti ç±»å‹")

        if not hasattr(creator, "Individual"):
            creator.create("Individual", PrimitiveTree, fitness=creator.FitnessMulti)
            logger.debug("âœ“ åˆ›å»º Individual ç±»å‹")

    def _build_pset(self) -> gp.PrimitiveSetTyped:
        """
        ç²¾ç®€ç‰ˆç®—å­é›†ï¼šä¸“ä¸º LightGBM/ElasticNet ç‰¹å¾å·¥ç¨‹è®¾è®¡
        æ‰€æœ‰ç®—å­å‡è¿”å›æ•°å€¼ç±»å‹ï¼Œç§»é™¤äº†å¯¼è‡´ SchemaError çš„é€»è¾‘ç®—å­
        """
        # ç›´æ¥ä½¿ç”¨ Expr ä½œä¸ºæ ‡è¯†ï¼Œé»˜è®¤å³ä¸ºæµ®ç‚¹æ•°åºåˆ—
        pset = gp.PrimitiveSetTyped("MAIN", [], Expr)
        return pset

    def build_toolbox(self, input_data: pl.DataFrame) -> base.Toolbox:
        """
        æ„å»ºè¿›åŒ–å·¥å…·ç®±

        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œç”¨äºé€‚åº”åº¦è¯„ä¼°

        Returns:
            base.Toolbox: DEAP å·¥å…·ç®±å®ä¾‹
        """
        toolbox = base.Toolbox()

        # æ ‘ç”Ÿæˆç®—æ³•: åŠæ•°åŠèŒæ³• (Half and Half)
        toolbox.register("expr", gp.genHalfAndHalf, pset=self.pset, min_=2, max_=5)
        toolbox.register("individual", tools.initIterate, creator.Individual, toolbox.expr)
        toolbox.register("population", tools.initRepeat, list, toolbox.individual)

        # é—ä¼ ç®—å­: é”¦æ ‡èµ›é€‰æ‹©ã€äº¤å‰ã€å˜å¼‚
        toolbox.register("select", tools.selTournament, tournsize=3)
        toolbox.register("mate", gp.cxOnePoint)
        toolbox.register("expr_mut", gp.genFull, min_=0, max_=2)
        toolbox.register("mutate", gp.mutUniform, expr=toolbox.expr_mut, pset=self.pset)

        # é™åˆ¶æ ‘é«˜ï¼Œé˜²æ­¢è†¨èƒ€ (Bloat)
        toolbox.decorate("mate", gp.staticLimit(key=operator.attrgetter("height"), max_value=self.max_height))
        toolbox.decorate("mutate", gp.staticLimit(key=operator.attrgetter("height"), max_value=self.max_height))

        # æ ¸å¿ƒï¼šæ‰¹é‡è¯„ä¼°æ˜ å°„
        toolbox.register("evaluate", lambda x: (np.nan, np.nan))  # å®é™…è¯„åˆ†åœ¨ map ä¸­å®Œæˆ
        toolbox.register(
            "map",
            self.map_exprs,
            gen=count(),
            label=self.label_y,
            split_date=self.split_date,
            input_data=input_data
        )

        logger.debug("âœ“ Toolbox æ„å»ºå®Œæˆ")
        return toolbox

    def map_exprs(
        self,
        evaluate_func: Any,
        individuals: List,
        gen,
        label: str,
        split_date: datetime,
        input_data: pl.DataFrame
    ) -> List[Tuple[float, float]]:
        """
        æ‰¹é‡è®¡ç®—ç§ç¾¤é€‚åº”åº¦çš„æ ¸å¿ƒæ–¹æ³•

        å¤„ç†æµç¨‹ï¼š
        1. å¤‡ä»½å½“å‰ä»£çš„è¡¨è¾¾å¼
        2. åŠ è½½å†å²é€‚åº”åº¦ç¼“å­˜
        3. æå–å¹¶è¿‡æ»¤è¡¨è¾¾å¼
        4. æ‰¹é‡è®¡ç®—æ–°è¡¨è¾¾å¼çš„é€‚åº”åº¦
        5. æ›´æ–°ç¼“å­˜å¹¶è¿”å›ç»“æœ

        Args:
            evaluate_func: è¯„ä¼°å‡½æ•°ï¼ˆæœªä½¿ç”¨ï¼Œç”± map è°ƒç”¨è¦æ±‚ï¼‰
            individuals: å½“å‰ä»£çš„ä¸ªä½“åˆ—è¡¨
            gen_iter: ä»£æ•°è¿­ä»£å™¨
            label: æ ‡ç­¾åˆ—å
            split_date: è®­ç»ƒ/æµ‹è¯•åˆ†å‰²æ—¥æœŸ
            input_data: è¾“å…¥æ•°æ®

        Returns:
            List[Tuple[float, float]]: æ¯ä¸ªä¸ªä½“çš„é€‚åº”åº¦å…ƒç»„åˆ—è¡¨
        """
        g = next(gen)
        logger.info(f">>> ç¬¬ {g} ä»£ | ç§ç¾¤å¤§å°: {len(individuals)}")

        # # 1. å¤‡ä»½åŸå§‹ç§ç¾¤ï¼ˆå› å­è¡¨è¾¾å¼ï¼‰ï¼ˆæ–­ç‚¹æ¢å¤ç”¨ï¼‰
        # expr_backup_path = self.save_dir / f'exprs_{g:03d}.pkl'
        # with open(expr_backup_path, 'wb') as f:
        #     pickle.dump(individuals, f)
        # logger.debug(f"âœ“ ç§ç¾¤å·²å¤‡ä»½è‡³: {expr_backup_path}")

        # 2. ç¼“å­˜ç®¡ç†
        cache_path = self.save_dir / 'fitness_cache.pkl'
        fitness_results: Dict = {} # è¡¨è¾¾å¼å­—ç¬¦ä¸² -> é€‚åº”åº¦å…ƒç»„
        if cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    fitness_results = pickle.load(f)
                logger.debug(f"âœ“ åŠ è½½å†å²ç¼“å­˜ | å·²æœ‰ç»“æœ: {len(fitness_results)}")
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")

        # 3. è¡¨è¾¾å¼æ¸…æ´—ä¸è¿‡æ»¤
        logger.debug("ğŸ”„ è½¬æ¢ DEAP æ ‘ -> Sympy è¡¨è¾¾å¼...")
        exprs_list = population_to_exprs(individuals, globals().copy())
        exprs_to_calc = filter_exprs(exprs_list, self.pset, RET_TYPE, fitness_results)

        logger.info(f"ğŸ“Š éœ€è®¡ç®—: {len(exprs_to_calc)} / {len(exprs_list)} ä¸ªè¡¨è¾¾å¼")

        # 4. æ‰¹é‡è®¡ç®—
        if len(exprs_to_calc) > 0:
            for batch_id, batch in enumerate(more_itertools.batched(exprs_to_calc, self.batch_size)):
                logger.debug(f"  æ‰¹æ¬¡ {batch_id + 1} | å¤§å°: {len(list(batch))}")
                new_scores = self.batched_exprs(batch_id, list(batch), g, label, split_date, input_data)
                fitness_results.update(new_scores)

            # æ›´æ–°å…¨å±€ç¼“å­˜
            try:
                with open(cache_path, 'wb') as f:
                    pickle.dump(fitness_results, f)
                logger.debug(f"âœ“ ç¼“å­˜å·²æ›´æ–° | æ€»ç»“æœæ•°: {len(fitness_results)}")
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

        # 5. å›å¡«é€‚åº”åº¦
        fitness_values = self.fill_fitness(exprs_list, fitness_results)
        logger.info(f"âœ“ ç¬¬ {g} ä»£è¯„ä¼°å®Œæˆ")
        return fitness_values

    def build_statistics(self) -> tools.Statistics:
        """
        å®šä¹‰è¿›åŒ–è¿‡ç¨‹ä¸­çš„ç»Ÿè®¡ç›‘æ§æŒ‡æ ‡

        Returns:
            tools.Statistics: DEAP ç»Ÿè®¡å¯¹è±¡
        """
        stats = tools.Statistics(lambda ind: ind.fitness.values)
        stats.register("avg", np.nanmean, axis=0)
        stats.register("max", np.nanmax, axis=0)
        stats.register("min", np.nanmin, axis=0)
        stats.register("std", np.nanstd, axis=0)
        return stats

    def run(
        self,
        input_data: pl.DataFrame,
        n_gen: int = 10,
        n_pop: int = 1000
    ) -> Tuple[List, Any, tools.HallOfFame]:
        """
        å¯åŠ¨è¿›åŒ–æµç¨‹

        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œå¿…é¡»åŒ…å«æ ‡ç­¾åˆ—
            n_gen: è¿›åŒ–ä»£æ•°ï¼Œé»˜è®¤ 10
            n_pop: åˆå§‹ç§ç¾¤å¤§å°ï¼Œé»˜è®¤ 1000

        Returns:
            Tuple: (æœ€ç»ˆç§ç¾¤, è¿›åŒ–æ—¥å¿—, åäººå ‚)

        Raises:
            ValueError: å¦‚æœè¾“å…¥æ•°æ®æ— æ•ˆ
        """
        # éªŒè¯è¾“å…¥æ•°æ®
        if input_data.height == 0:
            raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
        if self.label_y not in input_data.columns:
            raise ValueError(f"è¾“å…¥æ•°æ®ç¼ºå°‘æ ‡ç­¾åˆ—: {self.label_y}")

        logger.info(f"ğŸš€ å¯åŠ¨ GP è¿›åŒ– | ä»£æ•°: {n_gen} | ç§ç¾¤: {n_pop}")

        toolbox = self.build_toolbox(input_data)
        stats = self.build_statistics()
        hof = tools.HallOfFame(self.hof_size)

        # åˆå§‹åŒ–ç§ç¾¤
        pop = toolbox.population(n=n_pop)
        logger.info(f"âœ“ åˆå§‹ç§ç¾¤å·²ç”Ÿæˆ | å¤§å°: {len(pop)}")

        # æ‰§è¡Œè¿›åŒ–
        logger.info("â–¶ï¸ å¼€å§‹é—ä¼ ç¼–ç¨‹è¿›åŒ–...")
        pop, logbook = eaMuPlusLambda(
            pop, toolbox,
            mu=self.mu,
            lambda_=self.lambda_,
            cxpb= self.cxpb,  # äº¤å‰æ¦‚ç‡
            mutpb= self.mutpb,  # å˜å¼‚æ¦‚ç‡
            ngen=n_gen,
            stats=stats,
            halloffame=hof,
            verbose=True
        )

        # ä¿å­˜åäººå ‚
        hof_path = self.save_dir / 'best_hof.pkl'
        try:
            with open(hof_path, 'wb') as f:
                pickle.dump(hof, f)
            logger.info(f"ğŸ’¾ åäººå ‚å·²ä¿å­˜è‡³: {hof_path}")
        except Exception as e:
            logger.error(f"âŒ åäººå ‚ä¿å­˜å¤±è´¥: {e}")

        logger.info(f"âœ¨ GP è¿›åŒ–å®Œæˆ | æœ€ç»ˆç§ç¾¤: {len(pop)} | åäººå ‚: {len(hof)}")

        print('=' * 60)
        print(logbook)

        print('=' * 60)
        print_population(hof, globals().copy())
        self.export_hof_to_csv(hof, globals().copy())
        return pop, logbook, hof

    def export_hof_to_csv(self, hof, globals_, filename="best_factors.csv"):
        """
        å°†åäººå ‚å†…å®¹å¯¼å‡ºåˆ° CSV

        Args:
            hof: åäººå ‚å¯¹è±¡
            globals_: å…¨å±€å‘½åç©ºé—´ globals()
            filename: è¾“å‡ºæ–‡ä»¶å
        """
        import pandas as pd

        # 1. æ¨¡ä»¿ä½ çš„é€»è¾‘è§£æè¡¨è¾¾å¼
        # exprs_list å¾—åˆ°çš„æ˜¯ (ç®€åŒ–å k, è¡¨è¾¾å¼æ–‡æœ¬ v, å¤æ‚åº¦ c)
        exprs_list = population_to_exprs(hof, globals_)

        data = []
        for (k, v, c), ind in zip(exprs_list, hof):
            # æå–é€‚åº”åº¦ï¼ˆå¤„ç†å¤šç›®æ ‡æƒ…å†µï¼‰
            fitness_values = ind.fitness.values
            train_ic = fitness_values[0] if len(fitness_values) > 0 else None
            test_ic = fitness_values[1] if len(fitness_values) > 1 else None

            data.append({
                "factor_name": k,  # å› å­ç®€åŒ–å
                "fitness_train": train_ic,  # è®­ç»ƒé›†é€‚åº”åº¦/IC
                "fitness_test": test_ic,  # æµ‹è¯•é›†é€‚åº”åº¦/IC
                "expression": v,  # ç®€åŒ–åçš„è¡¨è¾¾å¼æ–‡æœ¬ (v)
                "complexity": c,  # å¤æ‚åº¦ (c)
                "raw_tree": str(ind)  # åŸå§‹ DEAP æ ‘ç»“æ„
            })

        # 2. è½¬æ¢ä¸º DataFrame å¹¶ä¿å­˜
        df = pd.DataFrame(data)
        output_path = self.save_dir / filename
        df.to_csv(output_path, index=False, encoding='utf-8-sig')

        logger.info(f"âœ… åäººå ‚å› å­å·²å¯¼å‡ºè‡³ CSV: {output_path}")
        return df

    def fitness_individual(self,a: str, b: str) -> pl.Expr:
        """ä¸ªä½“fitnesså‡½æ•°"""
        return pl.corr(a, b, method='spearman', ddof=0, propagate_nans=False)

    def batched_exprs(self,batch_id, exprs_list, gen, label, split_date, df_input):
        return {}

    def fill_fitness(self,exprs_old, fitness_results):
        return []