"""
GP å› å­ç”Ÿæˆå™¨ä¸»ç±»

ä½¿ç”¨ DEAP åº“è¿›è¡Œé—ä¼ ç¼–ç¨‹ï¼Œè‡ªåŠ¨åŒ–å› å­æŒ–æŽ˜ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. åŸºäºŽé—ä¼ ç¼–ç¨‹è‡ªåŠ¨ç”Ÿæˆå› å­è¡¨è¾¾å¼
2. æ‰¹é‡è®¡ç®—å’Œè¯„ä¼°å› å­é€‚åº”åº¦
3. æ”¯æŒè¿›åŒ–è¿‡ç¨‹çš„æ–­ç‚¹æ¢å¤
4. è‡ªåŠ¨ç¼“å­˜ä¸­é—´ç»“æžœ

å…¸åž‹ç”¨æ³•ï¼š
    config = {
        "label_y": "RETURN_OO_1",
        "split_date": datetime(2021, 1, 1),
        "batch_size": 50,
        "mu": 100,
        "lambda": 100,
        "hof_size": 100
    }
    generator = GPDeapGenerator(config)
    pop, logbook, hof = generator.run(input_data, n_gen=10, n_pop=500)
"""

import operator
import pickle
from datetime import datetime
from itertools import count
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np
import polars as pl
from deap import base, creator, gp, tools
from deap.gp import PrimitiveTree
from loguru import logger
import more_itertools
import polars.selectors as cs

from alpha.data_provider import DataProvider
# å¯¼å…¥æ‰“è¿‡è¡¥ä¸çš„ç»„ä»¶å’ŒåŸºç¡€å·¥å…·
from alpha.gp.deap_patch import eaMuPlusLambda  # æ ¸å¿ƒè¿›åŒ–ç®—æ³•
from alpha.gp.base import population_to_exprs, filter_exprs, print_population
# from alpha.gp.cs.helper import batched_exprs, fill_fitness
from alpha.gp.base import RET_TYPE, Expr
from alpha.utils.config import settings

from typing import TypeVar
from polars import DataFrame as _pl_DataFrame
from polars import LazyFrame as _pl_LazyFrame

from alpha.utils.schema import F

DataFrame = TypeVar("DataFrame", _pl_LazyFrame, _pl_DataFrame)



class GPDeapGenerator(object):
    """
    é—ä¼ ç¼–ç¨‹å› å­ç”Ÿæˆå™¨

    ä½¿ç”¨ DEAP æ¡†æž¶å®žçŽ°çš„è‡ªåŠ¨åŒ–å› å­æŒ–æŽ˜å¼•æ“Žã€‚

    Attributes:
        config (Dict): é…ç½®å‚æ•°å­—å…¸
        label_y (str): ç›®æ ‡æ ‡ç­¾åˆ—å
        split_date (datetime): è®­ç»ƒ/æµ‹è¯•é›†åˆ†å‰²æ—¥æœŸ
        batch_size (int): æ‰¹é‡è®¡ç®—å¤§å°
        save_dir (Path): ç»“æžœä¿å­˜ç›®å½•
        mu (int): ç§ç¾¤ä¿ç•™è§„æ¨¡
        lambda_ (int): æ¯ä»£ç”ŸæˆåŽä»£è§„æ¨¡
        hof_size (int): åäººå ‚å¤§å°
    """

    def __init__(self, config: Dict[str, Any] = {}) -> None:
        """
        åˆå§‹åŒ– GP å› å­ç”Ÿæˆå™¨

        Args:
            config: é…ç½®å­—å…¸ï¼Œæ”¯æŒçš„é”®ï¼š
                - label_y (str): ç›®æ ‡åˆ—åï¼Œé»˜è®¤ "RETURN_OO_1"
                - split_date (datetime): åˆ†å‰²æ—¥æœŸï¼Œé»˜è®¤ 2021-01-01
                - batch_size (int): æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ 50
                - mu (int): è¿›åŒ–ç®—æ³•çš„ mu å‚æ•°ï¼Œé»˜è®¤ 100
                - lambda (int): è¿›åŒ–ç®—æ³•çš„ lambda å‚æ•°ï¼Œé»˜è®¤ 100
                - hof_size (int): åäººå ‚å¤§å°ï¼Œé»˜è®¤ 100

        Raises:
            ValueError: å¦‚æžœé…ç½®å‚æ•°æ— æ•ˆ
        """
        """
                åˆå§‹åŒ–é…ç½®ï¼Œå°†æ‰€æœ‰ config å–å€¼é›†ä¸­åœ¨æ­¤
                """
        # --- 1. åŸºç¡€ä¿¡æ¯é…ç½® ---
        self.config = config
        self.name = config.get("name", self.__class__.__name__)

        # --- 2. æ•°æ®ä¸Žæ—¥æœŸé…ç½® ---
        self.start_date = config.get("start_date", "20190101")
        self.end_date = config.get("end_date", "20241231")
        self.split_date = config.get("split_date", datetime(2022, 1, 1))
        self.opt_names = config.get("opt_names",("ic_mean", "ic_ir", "complexity"))  # å¤šç›®æ ‡ä¼˜åŒ–åç§°åŠæƒé‡
        self.opt_weights = config.get("opt_weights",(1.0, 1.0, -0.01))  # å¤šç›®æ ‡ä¼˜åŒ–åç§°åŠæƒé‡
        # æ•´ä½“ç§ç¾¤fitnesså‡½æ•°,è¾“å…¥å‚æ•°ä¸º:df,factors,split_date
        # å…¶å®ƒå‚æ•°é‡‡ç”¨é»˜è®¤å
        self.fitness_population_func = config.get("fitness_population_func", None)

        self.pool_func = config.get("pool_func", None)  # è‚¡ç¥¨æ± å‡½æ•°
        self.label_func = config.get("label_func", None)  # æ ‡ç­¾è®¡ç®—å‡½æ•°
        self.random_window_func = config.get("random_window_func", None)  # éšæœºçª—å£å‡½æ•°
        self.extra_terminal_func = config.get("extra_terminal_func", [])  # é¢å¤–ç»ˆç«¯å› å­è®¡ç®—å‡½æ•°

        self.terminals = config.get('terminals', [])  # ç»ˆç«¯å› å­åˆ—è¡¨


        # --- 3. æ ‡ç­¾è®¡ç®—é…ç½® ---
        self.label_window = config.get("label_window", 1) # è®¡ç®—æ ‡ç­¾çš„æœªæ¥çª—å£å¤§å°
        self.label_y = config.get("label_y", f"LABEL_OO_{self.label_window}")  # ç›®æ ‡æ ‡ç­¾åˆ—å,å½“å‰ä»…æ”¯æŒ OPEN-OPEN æ”¶ç›ŠçŽ‡

        # --- 4. è¿›åŒ–ç®—æ³•è¶…å‚æ•° ---
        self.mu = config.get("mu", 300) # ç§ç¾¤ä¿ç•™è§„æ¨¡
        self.lambda_ = config.get("lambda", 400)  # æ¯ä»£ç”ŸæˆåŽä»£è§„æ¨¡
        self.cxpb = config.get("cxpb", 0.6)  # äº¤å‰æ¦‚çŽ‡
        self.mutpb = config.get("mutpb", 0.2)  # å˜å¼‚æ¦‚çŽ‡
        self.hof_size = config.get("hof_size", 1000) # åäººå ‚å¤§å°
        self.batch_size = config.get("batch_size", 200) # æ‰¹å¤„ç†å¤§å°
        self.max_height = config.get("max_height", 6) # æœ€å¤§æ ‘é«˜é™åˆ¶


        # è·¯å¾„è®¾ç½®
        self.save_dir = Path(settings.GP_DEAP_DIR)/ self.name
        self.save_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"âœ“ GP ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ | æ ‡ç­¾: {self.label_y} | æ‰¹å¤§å°: {self.batch_size}")


    def _prepare_labeled_data(self) -> DataFrame:
        """
        ä»Ž DataProvider èŽ·å–æ•°æ®å¹¶è®¡ç®—æŒ–æŽ˜æ ‡ç­¾
        è®¡ç®—é€»è¾‘ï¼šæœªæ¥ N æ—¥çš„ Open-to-Open æ”¶ç›ŠçŽ‡
        """

        cache_file = self.save_dir / f"labeled_{self.label_y}.parquet"
        logger.info(f"ðŸ“¡ æ­£åœ¨è®¡ç®—æ ‡ç­¾ '{self.label_y}'...")

        data_provider = DataProvider()

        # 2. è½½å…¥åŽŸå§‹æ•°æ®
        # æŒ–æŽ˜å› å­é€šå¸¸éœ€è¦ OHLCVï¼Œè®¡ç®— OO æ”¶ç›ŠçŽ‡éœ€è¦ OPEN
        lf = data_provider.load_data(
            start_date= self.start_date,
            end_date=self.end_date,
            funcs=[self.pool_func,self.label_func,self.extra_terminal_func],
            select_cols=[F.POOL_MASK,self.label_y,*self.terminals],
            cache_path= cache_file
        )
        # æŽ’åº
        lf = lf.sort(['ASSET', 'DATE']).with_columns([
            pl.col("ASSET").set_sorted(),
            # å¼ºåˆ¶å°†æ‰€æœ‰æ•°å€¼åˆ—è½¬ä¸º Float64ï¼Œé¿å… GP è¿è¡Œæ—¶ SchemaError
            cs.numeric().cast(pl.Float64)
        ])
        logger.info("ðŸ’¾ æ ‡ç­¾æ•°æ®å·²å°±ç»ª")

        return lf


    def run_workflow(self, n_gen: int = 10) -> Tuple[List, Any, Any]:
        """
        å…¨æµç¨‹ä¸€é”®å¯åŠ¨ï¼šæ•°æ®å‡†å¤‡ -> è¿›åŒ–æŒ–æŽ˜

        Args:
            n_gen: è¿›åŒ–ä»£æ•°ï¼Œé»˜è®¤ 10
        Returns:
            Tuple[List, logbook, HallOfFame]: (æœ€ç»ˆç§ç¾¤, è¿›åŒ–æ—¥å¿—, åäººå ‚)
        """
        logger.info("=" * 60)
        logger.info("GP å› å­æŒ–æŽ˜å…¨æµç¨‹å¯åŠ¨")
        logger.info("=" * 60)

        # æ­¥éª¤ 1: æ•°æ®å‡†å¤‡
        logger.info(">>> æ­¥éª¤ 1/2: æ•°æ®å‡†å¤‡")
        input_df = self._prepare_labeled_data()

        # æ­¥éª¤ 2: å¯åŠ¨è¿›åŒ–
        logger.info(">>> æ­¥éª¤ 2/2: å¯åŠ¨é—ä¼ ç¼–ç¨‹è¿›åŒ–")
        result = self.run(input_df, n_gen=n_gen)

        logger.info("=" * 60)
        logger.info("âœ… å…¨æµç¨‹æ‰§è¡Œå®Œæˆ")
        logger.info("=" * 60)

        return result

    def _build_pset(self) -> gp.PrimitiveSetTyped:
        """
        ç²¾ç®€ç‰ˆç®—å­é›†ï¼šä¸“ä¸º LightGBM/ElasticNet ç‰¹å¾å·¥ç¨‹è®¾è®¡
        æ‰€æœ‰ç®—å­å‡è¿”å›žæ•°å€¼ç±»åž‹ï¼Œç§»é™¤äº†å¯¼è‡´ SchemaError çš„é€»è¾‘ç®—å­
        """
        # ç›´æŽ¥ä½¿ç”¨ Expr ä½œä¸ºæ ‡è¯†ï¼Œé»˜è®¤å³ä¸ºæµ®ç‚¹æ•°åºåˆ—
        pset = gp.PrimitiveSetTyped("MAIN", [], Expr)
        return pset

    def build_toolbox(self, input_data: pl.DataFrame) -> base.Toolbox:
        """
        æž„å»ºè¿›åŒ–å·¥å…·ç®±

        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œç”¨äºŽé€‚åº”åº¦è¯„ä¼°

        Returns:
            base.Toolbox: DEAP å·¥å…·ç®±å®žä¾‹
        """
        creator.create("FitnessMulti", base.Fitness, weights=self.opt_weights)
        creator.create("Individual", PrimitiveTree, fitness=creator.FitnessMulti)

        toolbox = base.Toolbox()

        # æ ‘ç”Ÿæˆç®—æ³•: åŠæ•°åŠèŒæ³• (Half and Half)
        toolbox.register("expr", gp.genHalfAndHalf, pset=self.pset, min_=2, max_=5)
        toolbox.register("individual", tools.initIterate, creator.Individual, toolbox.expr)
        toolbox.register("population", tools.initRepeat, list, toolbox.individual)

        # é—ä¼ ç®—å­: é”¦æ ‡èµ›é€‰æ‹©ã€äº¤å‰ã€å˜å¼‚
        toolbox.register("select", tools.selTournament, tournsize=3)
        toolbox.register("mate", gp.cxOnePoint)
        toolbox.register("expr_mut", gp.genFull, min_=0, max_=2)
        toolbox.register("mutate", gp.mutUniform, expr=toolbox.expr_mut, pset=self.pset)

        # é™åˆ¶æ ‘é«˜ï¼Œé˜²æ­¢è†¨èƒ€ (Bloat)
        toolbox.decorate("mate", gp.staticLimit(key=operator.attrgetter("height"), max_value=self.max_height))
        toolbox.decorate("mutate", gp.staticLimit(key=operator.attrgetter("height"), max_value=self.max_height))

        # æ ¸å¿ƒï¼šæ‰¹é‡è¯„ä¼°æ˜ å°„
        toolbox.register("evaluate", lambda x: (np.nan, np.nan))  # å®žé™…è¯„åˆ†åœ¨ map ä¸­å®Œæˆ
        toolbox.register(
            "map",
            self.map_exprs,
            gen=count(),
            label=self.label_y,
            split_date=self.split_date,
            input_data=input_data
        )

        logger.debug("âœ“ Toolbox æž„å»ºå®Œæˆ")
        return toolbox

    def map_exprs(
        self,
        evaluate_func: Any,
        individuals: List,
        gen,
        label: str,
        split_date: datetime,
        input_data: pl.DataFrame
    ) -> List[Tuple[float, float]]:
        """
        æ‰¹é‡è®¡ç®—ç§ç¾¤é€‚åº”åº¦çš„æ ¸å¿ƒæ–¹æ³•

        å¤„ç†æµç¨‹ï¼š
        1. å¤‡ä»½å½“å‰ä»£çš„è¡¨è¾¾å¼
        2. åŠ è½½åŽ†å²é€‚åº”åº¦ç¼“å­˜
        3. æå–å¹¶è¿‡æ»¤è¡¨è¾¾å¼
        4. æ‰¹é‡è®¡ç®—æ–°è¡¨è¾¾å¼çš„é€‚åº”åº¦
        5. æ›´æ–°ç¼“å­˜å¹¶è¿”å›žç»“æžœ

        Args:
            evaluate_func: è¯„ä¼°å‡½æ•°ï¼ˆæœªä½¿ç”¨ï¼Œç”± map è°ƒç”¨è¦æ±‚ï¼‰
            individuals: å½“å‰ä»£çš„ä¸ªä½“åˆ—è¡¨
            gen: ä»£æ•°è¿­ä»£å™¨
            label: æ ‡ç­¾åˆ—å
            split_date: è®­ç»ƒ/æµ‹è¯•åˆ†å‰²æ—¥æœŸ
            input_data: è¾“å…¥æ•°æ®

        Returns:
            List[Tuple[float, float]]: æ¯ä¸ªä¸ªä½“çš„é€‚åº”åº¦å…ƒç»„åˆ—è¡¨
        """
        g = next(gen)
        logger.info(f">>> ç¬¬ {g} ä»£ | ç§ç¾¤å¤§å°: {len(individuals)}")

        # 2. ç¼“å­˜ç®¡ç†
        cache_path = self.save_dir / 'fitness_cache.pkl'
        fitness_results: Dict = {} # è¡¨è¾¾å¼å­—ç¬¦ä¸² -> é€‚åº”åº¦å…ƒç»„
        if cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    fitness_results = pickle.load(f)
                logger.debug(f"âœ“ åŠ è½½åŽ†å²ç¼“å­˜ | å·²æœ‰ç»“æžœ: {len(fitness_results)}")
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")

        # 3. è¡¨è¾¾å¼æ¸…æ´—ä¸Žè¿‡æ»¤
        logger.debug("ðŸ”„ è½¬æ¢ DEAP æ ‘ -> Sympy è¡¨è¾¾å¼...")
        exprs_list = population_to_exprs(individuals, globals().copy())
        exprs_to_calc = filter_exprs(exprs_list, self.pset, RET_TYPE, fitness_results)

        logger.info(f"ðŸ“Š éœ€è®¡ç®—: {len(exprs_to_calc)} / {len(exprs_list)} ä¸ªè¡¨è¾¾å¼")

        # 4. æ‰¹é‡è®¡ç®—
        if len(exprs_to_calc) > 0:
            for batch_id, batch in enumerate(more_itertools.batched(exprs_to_calc, self.batch_size)):
                logger.debug(f"  æ‰¹æ¬¡ {batch_id + 1} | å¤§å°: {len(list(batch))}")
                new_scores = self.batched_exprs(batch_id, list(batch), g, label, split_date, input_data)
                fitness_results.update(new_scores)

            # æ›´æ–°å…¨å±€ç¼“å­˜
            try:
                with open(cache_path, 'wb') as f:
                    pickle.dump(fitness_results, f)
                logger.debug(f"âœ“ ç¼“å­˜å·²æ›´æ–° | æ€»ç»“æžœæ•°: {len(fitness_results)}")
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

        # 5. å›žå¡«é€‚åº”åº¦
        fitness_values = self.fill_fitness(exprs_list, fitness_results)
        logger.info(f"âœ“ ç¬¬ {g} ä»£è¯„ä¼°å®Œæˆ")
        return fitness_values

    def build_statistics(self) -> tools.Statistics:
        """
        å®šä¹‰è¿›åŒ–è¿‡ç¨‹ä¸­çš„ç»Ÿè®¡ç›‘æŽ§æŒ‡æ ‡

        Returns:
            tools.Statistics: DEAP ç»Ÿè®¡å¯¹è±¡
        """
        stats = tools.Statistics(lambda ind: ind.fitness.values)
        stats.register("avg", np.nanmean, axis=0)
        stats.register("max", np.nanmax, axis=0)
        stats.register("min", np.nanmin, axis=0)
        stats.register("std", np.nanstd, axis=0)
        return stats

    def run(
        self,
        input_data: pl.DataFrame,
        n_gen: int = 10,
        n_pop: int = 1000
    ) -> Tuple[List, Any, tools.HallOfFame]:
        """
        å¯åŠ¨è¿›åŒ–æµç¨‹

        Args:
            input_data: è¾“å…¥æ•°æ®ï¼Œå¿…é¡»åŒ…å«æ ‡ç­¾åˆ—
            n_gen: è¿›åŒ–ä»£æ•°ï¼Œé»˜è®¤ 10
            n_pop: åˆå§‹ç§ç¾¤å¤§å°ï¼Œé»˜è®¤ 1000

        Returns:
            Tuple: (æœ€ç»ˆç§ç¾¤, è¿›åŒ–æ—¥å¿—, åäººå ‚)

        Raises:
            ValueError: å¦‚æžœè¾“å…¥æ•°æ®æ— æ•ˆ
        """
        # éªŒè¯è¾“å…¥æ•°æ®
        if self.label_y not in input_data.collect_schema().names():
            raise ValueError(f"è¾“å…¥æ•°æ®ç¼ºå°‘æ ‡ç­¾åˆ—: {self.label_y}")

        logger.info(f"ðŸš€ å¯åŠ¨ GP è¿›åŒ– | ä»£æ•°: {n_gen} | ç§ç¾¤: {n_pop}")
        self.pset = self._build_pset()
        toolbox = self.build_toolbox(input_data)
        stats = self.build_statistics()
        hof = tools.HallOfFame(self.hof_size)

        # åˆå§‹åŒ–ç§ç¾¤
        pop = toolbox.population(n=n_pop)
        logger.info(f"âœ“ åˆå§‹ç§ç¾¤å·²ç”Ÿæˆ | å¤§å°: {len(pop)}")

        # æ‰§è¡Œè¿›åŒ–
        logger.info("â–¶ï¸ å¼€å§‹é—ä¼ ç¼–ç¨‹è¿›åŒ–...")
        pop, logbook = eaMuPlusLambda(
            pop, toolbox,
            mu=self.mu,
            lambda_=self.lambda_,
            cxpb= self.cxpb,  # äº¤å‰æ¦‚çŽ‡
            mutpb= self.mutpb,  # å˜å¼‚æ¦‚çŽ‡
            ngen=n_gen,
            stats=stats,
            halloffame=hof,
            verbose=True
        )

        # ä¿å­˜åäººå ‚
        hof_path = self.save_dir / 'best_hof.pkl'
        try:
            with open(hof_path, 'wb') as f:
                pickle.dump(hof, f)
            logger.info(f"ðŸ’¾ åäººå ‚å·²ä¿å­˜è‡³: {hof_path}")
        except Exception as e:
            logger.error(f"âŒ åäººå ‚ä¿å­˜å¤±è´¥: {e}")

        logger.info(f"âœ¨ GP è¿›åŒ–å®Œæˆ | æœ€ç»ˆç§ç¾¤: {len(pop)} | åäººå ‚: {len(hof)}")

        print('=' * 60)
        print(logbook)

        print('=' * 60)
        print_population(hof, globals().copy())
        self.export_hof_to_csv(hof, globals().copy())
        return pop, logbook, hof

    def export_hof_to_csv(self, hof, globals_, filename="best_factors.csv"):
        """
        å°†åäººå ‚å†…å®¹å¯¼å‡ºåˆ° CSV

        Args:
            hof: åäººå ‚å¯¹è±¡
            globals_: å…¨å±€å‘½åç©ºé—´ globals()
            filename: è¾“å‡ºæ–‡ä»¶å
        """
        import pandas as pd

        # 1. æ¨¡ä»¿ä½ çš„é€»è¾‘è§£æžè¡¨è¾¾å¼
        # exprs_list å¾—åˆ°çš„æ˜¯ (ç®€åŒ–å k, è¡¨è¾¾å¼æ–‡æœ¬ v, å¤æ‚åº¦ c)
        exprs_list = population_to_exprs(hof, globals_)

        data = []
        for (k, v, c), ind in zip(exprs_list, hof):
            # æå–é€‚åº”åº¦ï¼ˆå¤„ç†å¤šç›®æ ‡æƒ…å†µï¼‰
            fitness_values = ind.fitness.values
            train_ic = fitness_values[0] if len(fitness_values) > 0 else None
            test_ic = fitness_values[1] if len(fitness_values) > 1 else None

            data.append({
                "factor_name": k,  # å› å­ç®€åŒ–å
                "fitness_train": train_ic,  # è®­ç»ƒé›†é€‚åº”åº¦/IC
                "fitness_test": test_ic,  # æµ‹è¯•é›†é€‚åº”åº¦/IC
                "expression": v,  # ç®€åŒ–åŽçš„è¡¨è¾¾å¼æ–‡æœ¬ (v)
                "complexity": c,  # å¤æ‚åº¦ (c)
                "raw_tree": str(ind)  # åŽŸå§‹ DEAP æ ‘ç»“æž„
            })

        # 2. è½¬æ¢ä¸º DataFrame å¹¶ä¿å­˜
        df = pd.DataFrame(data)
        output_path = self.save_dir / filename
        df.to_csv(output_path, index=False, encoding='utf-8-sig')

        logger.info(f"âœ… åäººå ‚å› å­å·²å¯¼å‡ºè‡³ CSV: {output_path}")
        return df

    def fitness_individual(self,a: str, b: str) -> pl.Expr:
        """ä¸ªä½“fitnesså‡½æ•°"""
        return pl.corr(a, b, method='spearman', ddof=0, propagate_nans=False)

    def batched_exprs(self,batch_id, exprs_list, gen, label, split_date, df_input):
        return {}

    def fill_fitness(self,exprs_old, fitness_results):
        return []
