import polars as pl
import polars.selectors as cs
import numpy as np
import fastcluster
from loguru import logger
from typing import Union, List, Optional
from scipy.cluster.hierarchy import fcluster
from scipy.spatial.distance import squareform


def batch_clustering(
        df: Union[pl.DataFrame, pl.LazyFrame],
        factors: Union[str, List[str]] = r"^factor_.*",
        threshold: float = 0.8,
        method: str = "average",
        sample_n: Optional[int] = None
) -> dict:
    """
    å› å­èšç±»åˆ†ææ ¸å¿ƒå‡½æ•°ï¼šè¯†åˆ«å¹¶å½’ç±»é€»è¾‘å†—ä½™çš„å› å­ã€‚

    è¯¥å‡½æ•°é€šè¿‡è®¡ç®—å› å­é—´çš„æ–¯çš®å°”æ›¼ç§©ç›¸å…³æ€§ (Spearman Rank Correlation) æ„å»ºè·ç¦»çŸ©é˜µï¼Œ
    å¹¶åˆ©ç”¨å±‚æ¬¡èšç±»ç®—æ³•å°†è¡¨ç°ç›¸ä¼¼çš„å› å­åˆ’åˆ†ä¸ºåŒä¸€ä¸ªç°‡ã€‚

    Args:
        df (Union[pl.DataFrame, pl.LazyFrame]): åŒ…å«å› å­æ•°æ®çš„ Polars å¯¹è±¡ã€‚
        factors (Union[str, List[str]]): å› å­åˆ—é€‰æ‹©å™¨ï¼Œæ”¯æŒæ­£åˆ™æˆ–åç§°åˆ—è¡¨ã€‚é»˜è®¤ä¸º "factor_*"ã€‚
        threshold (float): ç›¸å…³æ€§èšç±»é˜ˆå€¼ (0~1)ã€‚
            - è¶Šé«˜(å¦‚0.95)è¡¨ç¤ºèšç±»è¶Šä¸¥æ ¼ï¼Œåªæœ‰æåº¦ç›¸ä¼¼çš„å› å­æ‰ä¼šè¢«åˆå¹¶ã€‚
            - è¶Šä½(å¦‚0.7)è¡¨ç¤ºèšç±»è¶Šå®½æ¾ã€‚
        method (str): å±‚æ¬¡èšç±»è”åŠ¨ç®—æ³•ã€‚
            - 'average': ç»„å¹³å‡æ³• (UPGMA)ï¼Œå¯¹é‡‘èæ•°æ®å™ªå£°è¾ƒç¨³å¥ã€‚
            - 'complete': å…¨è”åŠ¨ï¼Œå€¾å‘äºäº§ç”Ÿç´§å‡‘ä¸”ç›´å¾„å°çš„ç°‡ã€‚
        sample_n (Optional[int]): é‡‡æ ·è¡Œæ•°ã€‚
            - è‹¥ä¸º Noneï¼Œåˆ™ä½¿ç”¨å…¨é‡æ•°æ®è®¡ç®—ã€‚
            - è‹¥ä¸º intï¼Œåˆ™åœ¨ collect åéšæœºé‡‡æ · N è¡Œï¼Œä»¥åŠ é€Ÿç›¸å…³æ€§çŸ©é˜µè®¡ç®—ã€‚

    Returns:
        dict: å› å­åä¸èšç±»æ ‡ç­¾çš„æ˜ å°„å­—å…¸ {å› å­å: ç°‡ID}ã€‚ç°‡IDç›¸åŒçš„å› å­é€»è¾‘åŒè´¨åŒ–è¾ƒé«˜ã€‚
    """

    # 1. ç»Ÿä¸€è½¬æ¢ä¸º LazyFrame ä»¥å¤ç”¨æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–å™¨
    lf = df.lazy() if isinstance(df, pl.DataFrame) else df
    selector = cs.matches(factors) if isinstance(factors, str) else cs.by_name(factors)

    # 2. æå–å› å­åˆ—å¹¶æ‰§è¡Œé‡‡æ · (ä»…é’ˆå¯¹ LazyFrame ä¼˜åŒ–çš„å†…å­˜è·¯å¾„)
    try:
        if sample_n is not None:
            # ä»…æå–é€‰ä¸­çš„å› å­åˆ—ï¼Œé¿å… collect æ— å…³åˆ—å¯¼è‡´ OOM
            factor_df = lf.select(selector).collect()
            actual_n = min(sample_n, factor_df.height)
            factor_df = factor_df.sample(n=actual_n, seed=42)
            logger.info(f"ğŸ“Š å› å­èšç±»é‡‡æ ·ï¼šå·²ä» {lf.select(pl.len()).collect().item()} è¡Œä¸­æŠ½å– {actual_n} è¡Œ")
        else:
            factor_df = lf.select(selector).collect()
            logger.info(f"ğŸ“Š å› å­èšç±»å…¨é‡è®¡ç®—ï¼šå…±è®¡ {factor_df.height} è¡Œæ•°æ®")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®æå–å¤±è´¥: {e}")
        return {}

    # 3. è¾¹ç•Œæ¡ä»¶æ£€æŸ¥
    if factor_df.width < 1:
        logger.warning("âš ï¸ æœªåŒ¹é…åˆ°ä»»ä½•å› å­åˆ—ï¼Œè¯·æ£€æŸ¥ selector å‚æ•°")
        return {}
    if factor_df.width == 1:
        logger.info("â„¹ï¸ ä»…æœ‰ä¸€ä¸ªå› å­ï¼Œè·³è¿‡èšç±»")
        return {factor_df.columns[0]: 1}

    # 4. æ•°æ®é¢„å¤„ç†ï¼šå‰”é™¤æ–¹å·®è¿‡å°çš„å¸¸æ•°å› å­
    stats = factor_df.std()
    valid_cols = [col for col in factor_df.columns if (stats.get_column(col)[0] or 0) > 1e-9]
    invalid_count = factor_df.width - len(valid_cols)
    if invalid_count > 0:
        logger.warning(f"ğŸš« å·²å‰”é™¤ {invalid_count} ä¸ªå¸¸æ•°å› å­ (æ–¹å·® â‰ˆ 0)")

    if not valid_cols:
        return {col: 0 for col in factor_df.columns}

    # 5. æ¸…ç† Null å€¼ (Spearman Rank å¯¹ Null æ•æ„Ÿ)
    cleaned_df = factor_df.select(valid_cols).drop_nulls()
    if cleaned_df.height < 10:
        logger.error(f"âŒ æœ‰æ•ˆè¡Œæ•°ä¸è¶³ ({cleaned_df.height})ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")
        return {col: i + 1 for i, col in enumerate(factor_df.columns)}

    # 6. è®¡ç®—æ–¯çš®å°”æ›¼ç§©ç›¸å…³æ€§ (Spearman Rank Correlation)
    # ä½¿ç”¨ç§©å˜æ¢ (rank) åè®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°ï¼Œå³ä¸ºæ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°
    logger.debug(f"ğŸ” æ­£åœ¨è®¡ç®— {len(valid_cols)} ä¸ªå› å­çš„ç›¸å…³æ€§çŸ©é˜µ...")
    corr_matrix = cleaned_df.select([pl.col(c).rank() for c in valid_cols]).corr().to_numpy()
    corr_matrix = np.nan_to_num(corr_matrix, nan=0.0)

    # 7. æ„å»ºè·ç¦»çŸ©é˜µ (Distance Matrix)
    # è·ç¦»å®šä¹‰ï¼šd = 1 - |rho|ã€‚é«˜åº¦æ­£ç›¸å…³æˆ–è´Ÿç›¸å…³çš„å› å­è·ç¦»éƒ½æ¥è¿‘ 0
    dist_matrix = np.clip(1 - np.abs(corr_matrix), 0, 1)

    # å…³é”®æ­¥éª¤ï¼šå¼ºåˆ¶å¯¹ç§°åŒ–ä»¥æ¶ˆé™¤æµ®ç‚¹æ•°ç²¾åº¦å¼•èµ·çš„ Scipy æŠ¥é”™
    dist_matrix = (dist_matrix + dist_matrix.T) / 2
    np.fill_diagonal(dist_matrix, 0)

    # 8. æ‰§è¡Œå±‚æ¬¡èšç±»
    try:
        # å°†å¯¹ç§°æ–¹é˜µå‹ç¼©ä¸º 1D è·ç¦»å‘é‡ï¼ˆsquareform è¦æ±‚çš„è¾“å…¥æ ¼å¼ï¼‰
        dist_vec = squareform(dist_matrix)

        # ä½¿ç”¨ fastcluster è¿›è¡Œè®¡ç®—ï¼ˆæ¯” scipy åŸç”Ÿå¿«ï¼‰
        Z = fastcluster.linkage(dist_vec, method=method)

        # åˆ‡å‰²èšç±»æ ‘ï¼Œå¾—åˆ°æ ‡ç­¾ã€‚t = 1 - threshold ä¸ºåˆ‡å‰²é«˜åº¦
        labels = fcluster(Z, t=1 - threshold, criterion='distance')
    except Exception as e:
        logger.error(f"âŒ å±‚æ¬¡èšç±»ç®—æ³•å´©æºƒ: {e}")
        return {col: i + 1 for i, col in enumerate(factor_df.columns)}

    # 9. æ„é€ æœ€ç»ˆç»“æœæ˜ å°„
    result = {col: 0 for col in factor_df.columns}
    for col, label in zip(valid_cols, labels):
        result[col] = int(label)

    # 10. æ‰“å°èšç±»æ‘˜è¦æ—¥å¿—
    num_clusters = len(set(labels))
    logger.success(
        f"âœ… å› å­èšç±»å®Œæˆ | åŸå§‹å› å­: {len(factor_df.columns)} | æœ‰æ•ˆå‚ä¸: {len(valid_cols)} | é€»è¾‘ç°‡æ•°é‡: {num_clusters}")

    return result
