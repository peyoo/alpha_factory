"""
å¤§æ‰¹é‡å› å­è¯„ä¼°ç›¸å…³è®¡ç®—å‡½æ•°é›†

"""

import polars.selectors as cs
from typing import List

import polars as pl
from loguru import logger

from alpha.utils.schema import F


def batch_get_ic_summary(df: pl.DataFrame,
                         factor_pattern: str = r"^factor_.*",
                         ret_col: str = "LABEL_OO_1", # å»ºè®®è¿™é‡Œé»˜è®¤å€¼ä¸ä½ å¸¸ç”¨çš„ä¿æŒä¸€è‡´
                         # label_ic_col: str = "LABEL_IC",

                         split_date: str = None,
                         date_col: str = F.DATE,
                         pool_mask_col: str = F.POOL_MASK  # ğŸ†• æ–°å¢è‚¡ç¥¨æ± å‚æ•°
                         ) -> pl.DataFrame:
    lf = df.lazy() if isinstance(df, pl.DataFrame) else df

    # 1. è‡ªåŠ¨è·å–å› å­åˆ—
    # å¦‚æœ factor_pattern è¢«è¯¯ä¼ æˆäº†åˆ—è¡¨ï¼ˆä¾‹å¦‚å› å­ååˆ—è¡¨ï¼‰ï¼Œç›´æ¥ä½¿ç”¨è¯¥åˆ—è¡¨
    if isinstance(factor_pattern, (list, tuple)):
        factor_cols = [c for c in factor_pattern if c in lf.collect_schema().names()]
    else:
        # å¦åˆ™ä½¿ç”¨æ­£åˆ™åŒ¹é…
        factor_cols = lf.select(cs.matches(factor_pattern)).collect_schema().names()

    # A. é¦–å…ˆåº”ç”¨è‚¡ç¥¨æ± è¿‡æ»¤
    if pool_mask_col in lf.collect_schema().names():
        lf = lf.filter(pl.col(pool_mask_col))
        logger.debug(f"â„¹ï¸ å·²åº”ç”¨è‚¡ç¥¨æ± æ©ç : {pool_mask_col}")

    # 2. æ„é€ è®¡ç®—é“¾è·¯
    ic_summary = (
        lf.select([date_col, ret_col] + factor_cols)
        .drop_nulls()  # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿å‚ä¸è®¡ç®—çš„è¡Œæ²¡æœ‰ç©ºå€¼
        .group_by(date_col)
        .agg([
            # ä½¿ç”¨ Spearman (Rank) ç›¸å…³æ€§é€šå¸¸æ¯” Pearson æ›´ç¨³å¥
            pl.corr(pl.col(f), pl.col(ret_col), method="spearman").alias(f) for f in factor_cols
        ])
        .unpivot(index=date_col, on=factor_cols, variable_name="factor", value_name="ic")
        .filter(pl.col("ic").is_not_nan() & pl.col("ic").is_not_null())
        .group_by("factor")
        .agg([
            pl.col("ic").mean().alias("ic_mean"),
            pl.col("ic").std().alias("ic_std"),
            # å¢åŠ  fill_nan(0) é˜²æ­¢é™¤ä»¥ 0 çš„æƒ…å†µ
            (pl.col("ic").mean() / pl.col("ic").std().fill_nan(1e-9)).alias("ic_ir"),

            (pl.col("ic").mean() / pl.col("ic").std().fill_nan(1e-9) * pl.count().sqrt()).alias("t_stat"),
            (pl.col("ic").filter(pl.col("ic") > 0).count() / pl.count()).alias("win_rate")
        ]).with_columns(
            [# æ·»åŠ ä¸€ä¸ªic_mean_absåˆ—ï¼Œæ–¹ä¾¿åç»­ç­›é€‰
            pl.col("ic_mean").abs().alias("ic_mean_abs"),
            pl.col('ic_ir').abs().alias('ic_ir_abs')
            ]
        )
        .collect()
    )

    return ic_summary

def batch_calc_factor_decay_stats(
        df: pl.DataFrame,
        factor_pattern: List[str],
        ret_col: str,
        max_lag: int = 10,
        date_col: str = F.DATE,
        asset_col: str = F.ASSET,
) -> pl.DataFrame:
    """
    å¤§æ‰¹é‡è®¡ç®—å› å­ IC è¡°å‡å›¾è°±
    ç»“æœè¿”å›æ¯ä¸ªå› å­åœ¨ä¸åŒæ»åæœŸçš„ IC Mean å’Œ IR
    è®¡ç®—é€»è¾‘ï¼š
    1. æ„é€ æ»åæ”¶ç›Šç‡åˆ—å¹¶ Rank
    2. é•¿è¡¨åŒ–å¤„ç†æ‰€æœ‰å› å­ï¼Œæ–¹ä¾¿ç»Ÿä¸€è®¡ç®—
    3. æŒ‰æ—¥æœŸå’Œå› å­åˆ†ç»„ï¼Œè®¡ç®—å„æ»åæœŸçš„ IC åºåˆ—
    4. æœ€ç»ˆç»Ÿè®¡å„å› å­åœ¨ä¸åŒæ»åæœŸçš„ IC Mean å’Œ IR
    è¯´æ˜ï¼š
    è¿™ç§æ–¹æ³•é¿å…äº†å¤šæ¬¡å¾ªç¯è®¡ç®—ï¼Œæå¤§æå‡äº†æ•ˆç‡ã€‚
    é€‚ç”¨äºå¤§è§„æ¨¡å› å­è¯„ä¼°åœºæ™¯ã€‚

    :param df:
    :param factor_pattern:
    :param ret_col:
    :param date_col:
    :param asset_col:
    :param max_lag:
    :return:
    """

    # 1. ç»Ÿä¸€é€‰å–å› å­åˆ—å
    # å¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™è§†ä¸ºæ­£åˆ™åŒ¹é…ï¼›å¦‚æœå·²ç»æ˜¯ Listï¼Œåˆ™ç›´æ¥ä½¿ç”¨
    if isinstance(factor_pattern, str):
        factor_cols = df.select(pl.col(factor_pattern)).columns
    else:
        factor_cols = factor_pattern

    # 1. é¢„å¤„ç†ï¼šæ„é€ æ»åæ”¶ç›Šç‡å¹¶ Rank
    # è¿™ä¸€æ­¥ä¿æŒä¸å˜ï¼Œæ˜¯æœ€é«˜æ•ˆçš„å¯¹é½æ–¹å¼
    target_lags = [f"target_lag_{i}" for i in range(max_lag)]
    q = df.lazy().with_columns([
        pl.col(ret_col).shift(-i).over(asset_col).rank().over(date_col).alias(f"target_lag_{i}")
        for i in range(max_lag)
    ])

    # 2. é•¿è¡¨åŒ–å¤„ç†ï¼šå°†æ‰€æœ‰å› å­è½¬ä¸ºä¸€åˆ—ï¼Œæ–¹ä¾¿ç»Ÿä¸€è®¡ç®—
    # ç»“æœæ ¼å¼ï¼š[date, asset, target_lag_0...target_lag_n, factor_name, factor_value]
    q_long = q.unpivot(
        index=[date_col, asset_col] + target_lags,
        on=factor_cols,
        variable_name="factor",
        value_name="factor_value"
    ).with_columns(
        pl.col("factor_value").rank().over([date_col, "factor"])
    )

    # 3. æ ¸å¿ƒèšåˆï¼šæŒ‰æ—¥æœŸå’Œå› å­åˆ†ç»„ï¼Œä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ç›¸å…³æ€§
    # å¾—åˆ°çš„æ˜¯ IC çš„æ—¶é—´åºåˆ—
    ic_series = q_long.group_by([date_col, "factor"]).agg([
        pl.corr("factor_value", pl.col(f"target_lag_{i}"), method="pearson").alias(f"lag_{i}")
        for i in range(max_lag)
    ])

    # 4. æœ€ç»ˆç»Ÿè®¡ï¼šè®¡ç®— Mean å’Œ IR
    # è¿™é‡Œä¸éœ€è¦ collect ä¸¤æ¬¡ï¼Œç›´æ¥åœ¨é“¾å¼è°ƒç”¨ä¸­å®Œæˆ
    decay_stats = ic_series.group_by("factor").agg([
        *[pl.col(f"lag_{i}").mean().alias(f"IC_Mean_Lag_{i}") for i in range(max_lag)],
        *[(pl.col(f"lag_{i}").mean() / pl.col(f"lag_{i}").std()).alias(f"IR_Lag_{i}") for i in range(max_lag)]
    ]).collect()

    return decay_stats


def batch_calc_factor_full_metrics(
        df: pl.DataFrame,
        factor_pattern: List[str],
        ret_col: str,
        date_col: str = "DATE",
        asset_col: str = "ASSET",
        max_lag: int = 10
) -> pl.DataFrame:
    """
    ã€åˆå¹¶ç‰ˆã€‘ä¸€æ¬¡æ€§è®¡ç®—åŸºç¡€ IC æŒ‡æ ‡ + IC/IR è¡°å‡å›¾è°±
    å¤§æ‰¹é‡è®¡ç®—å› å­ IC è¡°å‡å›¾è°±åŠåŸºç¡€ç»Ÿè®¡æŒ‡æ ‡
    ç»“æœè¿”å›æ¯ä¸ªå› å­åœ¨ä¸åŒæ»åæœŸçš„ IC Meanã€IR ä»¥åŠåŸºç¡€æŒ‡æ ‡ï¼ˆt-statã€win rateï¼‰
    è®¡ç®—é€»è¾‘ï¼š
    1. æ„é€ æ»åæ”¶ç›Šç‡åˆ—å¹¶ Rank
    2. é•¿è¡¨åŒ–å¤„ç†æ‰€æœ‰å› å­ï¼Œæ–¹ä¾¿ç»Ÿä¸€è®¡ç®—
    3. æŒ‰æ—¥æœŸå’Œå› å­åˆ†ç»„ï¼Œè®¡ç®—å„æ»åæœŸçš„ IC åºåˆ—
    4. æœ€ç»ˆç»Ÿè®¡å„å› å­åœ¨ä¸åŒæ»åæœŸçš„ IC Meanã€IR ä»¥åŠåŸºç¡€æŒ‡æ ‡
    è¯´æ˜ï¼š
    è¿™ç§æ–¹æ³•é¿å…äº†å¤šæ¬¡å¾ªç¯è®¡ç®—ï¼Œæå¤§æå‡äº†æ•ˆç‡ã€‚
    é€‚ç”¨äºå¤§è§„æ¨¡å› å­è¯„ä¼°åœºæ™¯ã€‚
    """

    # 1. ç»Ÿä¸€é€‰å–å› å­åˆ—å
    # å¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™è§†ä¸ºæ­£åˆ™åŒ¹é…ï¼›å¦‚æœå·²ç»æ˜¯ Listï¼Œåˆ™ç›´æ¥ä½¿ç”¨
    if isinstance(factor_pattern, str):
        factor_cols = df.select(pl.col(factor_pattern)).columns
    else:
        factor_cols = factor_pattern

    # 2. æ„é€ æ»åæ”¶ç›Šç‡å¹¶ Rank
    target_lags = [f"target_lag_{i}" for i in range(max_lag)]
    q = df.lazy().with_columns([
        pl.col(ret_col).shift(-i).over(asset_col).rank().over(date_col).alias(f"target_lag_{i}")
        for i in range(max_lag)
    ])

    # 2. é•¿è¡¨åŒ–å¤„ç†
    q_long = q.unpivot(
        index=[date_col, asset_col] + target_lags,
        on=factor_cols,
        variable_name="factor",
        value_name="factor_value"
    ).with_columns(
        pl.col("factor_value").rank().over([date_col, "factor"])
    )

    # 3. æ ¸å¿ƒèšåˆï¼šè®¡ç®—å„ Lag çš„æ¯æ—¥ IC åºåˆ—
    ic_series = q_long.group_by([date_col, "factor"]).agg([
        pl.corr("factor_value", pl.col(f"target_lag_{i}"), method="pearson").alias(f"lag_{i}")
        for i in range(max_lag)
    ])

    # 4. ç»ˆæèšåˆï¼šåˆå¹¶è¡°å‡ä¸åŸºç¡€æŒ‡æ ‡
    # æˆ‘ä»¬ä»¥ lag_0 ä½œä¸ºåŸºç¡€ IC (å³ä¼ ç»Ÿçš„ IC ç»Ÿè®¡)
    full_stats = ic_series.group_by("factor").agg([
        # --- è¡°å‡éƒ¨åˆ† ---
        *[pl.col(f"lag_{i}").mean().alias(f"IC_Mean_Lag_{i}") for i in range(max_lag)],
        *[(pl.col(f"lag_{i}").mean() / pl.col(f"lag_{i}").std()).alias(f"IR_Lag_{i}") for i in range(max_lag)],

        # --- åŸºç¡€æŒ‡æ ‡è¡¥å…¨ (åŸºäº lag_0) ---
        (pl.col("lag_0").mean() / pl.col("lag_0").std() * pl.count().sqrt()).alias("t_stat"),
        (pl.col("lag_0").filter(pl.col("lag_0") > 0).count() / pl.count()).alias("win_rate")
    ]).collect()

    return full_stats


def batch_calc_factor_turnover(
        df: pl.DataFrame,
        factor_pattern: str = r"^factor_.*",
        date_col: str = F.DATE,
        asset_col: str = F.ASSET,
        lag: int = 1
) -> pl.DataFrame:
    """
    å¤§æ‰¹é‡è®¡ç®—å› å­çš„è‡ªç›¸å…³æ€§ (è¡¡é‡æ¢æ‰‹ç‡)
    lag: æ»åå¤©æ•°ï¼Œ1ä»£è¡¨æ—¥æ¢æ‰‹ï¼Œ5ä»£è¡¨å‘¨æ¢æ‰‹
    """
    # 1. è‡ªåŠ¨è·å–å› å­åˆ—
    factor_cols = df.select(pl.col(factor_pattern)).columns

    # 2. è®¡ç®—å„å› å­è‡ªèº«çš„æ»åç›¸å…³æ€§
    # ç»“æœåæ˜ äº†å› å­çš„ç¨³å®šæ€§ï¼Œå€¼è¶Šå¤§æ¢æ‰‹è¶Šä½
    turnover_stats = (
        df.lazy()
        .with_columns([
            pl.col(f).shift(lag).over(asset_col).alias(f"{f}_lag")
            for f in factor_cols
        ])
        .group_by(date_col)
        .agg([
            pl.corr(pl.col(f).rank(), pl.col(f"{f}_lag").rank(), method="pearson").alias(f)
            for f in factor_cols
        ])
        .unpivot(index=date_col, on=factor_cols, variable_name="factor", value_name="autocorr")
        .group_by("factor")
        .agg([
            pl.col("autocorr").mean().alias("avg_autocorr"),
            # æ¢æ‰‹ç‡ä¼°ç®—ï¼š1 - å¹³å‡è‡ªç›¸å…³ç³»æ•°
            (1 - pl.col("autocorr").mean()).alias("turnover_estimate")
        ])
        .collect()
    )
    return turnover_stats


def batch_calc_quantile_returns(
        df: pl.DataFrame,
        factor_pattern: str = r"^factor_.*",
        ret_col: str = "target_ret",
        date_col: str = F.DATE,
        asset_col: str = F.ASSET,
        n_bins: int = 5  # åˆ†ä¸º5å±‚
) -> pl.DataFrame:
    """
    å¤§æ‰¹é‡å¹¶è¡Œè®¡ç®—å› å­çš„åˆ†å±‚æ”¶ç›Š
    """
    factor_cols = df.select(pl.col(factor_pattern)).columns

    # 1. é•¿è¡¨åŒ–å¹¶è®¡ç®—åˆ†å±‚
    # ç»“æœï¼š[date, factor, factor_value, target_ret]
    q_long = df.lazy().unpivot(
        index=[date_col, asset_col, ret_col],
        on=factor_cols,
        variable_name="factor",
        value_name="value"
    ).filter(pl.col("value").is_not_null())

    # 2. æˆªé¢åˆ†å±‚ (å…³é”®æ­¥éª¤)
    # å¯¹æ¯ä¸€ä¸ªæ—¥æœŸã€æ¯ä¸€ä¸ªå› å­ï¼Œç‹¬ç«‹è¿›è¡Œåˆ†ä½åˆ‡å‰²
    q_returns = (
        q_long.with_columns(
            pl.col("value")
            .qcut(n_bins, labels=[f"Q{i + 1}" for i in range(n_bins)])
            .over([date_col, "factor"])
            .alias("quantile")
        )
        # 3. è®¡ç®—æ¯æ—¥æ¯å±‚å¹³å‡æ”¶ç›Š
        .group_by([date_col, "factor", "quantile"])
        .agg(pl.col(ret_col).mean().alias("daily_ret"))

        # 4. è®¡ç®—ç´¯è®¡æ”¶ç›Š (ç´¯åŠ æˆ–ç´¯ä¹˜)
        .sort(["factor", "quantile", date_col])
        .with_columns(
            (pl.col("daily_ret") + 1).product().over(["factor", "quantile"]).alias("cum_ret")
        )
        .collect()
    )
    return q_returns


def batch_factor_alpha_lens(
        df: pl.DataFrame,
        factor_pattern: str = r"^factor_.*",
        ret_col: str = "target_ret",
        date_col: str = "DATE",
        asset_col: str = "ASSET",
        n_bins: int = 5,
        max_lag: int = 5
) -> pl.DataFrame:
    """
    ã€ç»ˆæå…¨èƒ½ç‰ˆã€‘å¤§æ‰¹é‡å› å­ä½“æ£€å¼•æ“ï¼šIC/IR + è¡°å‡ + æ¢æ‰‹ + åˆ†å±‚æ”¶ç›Š
    """
    # lf = df.lazy() if isinstance(df, pl.DataFrame) else df

    factor_cols = df.select(pl.col(factor_pattern)).columns

    # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€æŒ‡æ ‡ä¸è¡°å‡ (IC/IR/t-stat/WinRate) ---
    full_metrics = batch_calc_factor_full_metrics(
        df, factor_cols, ret_col, date_col, asset_col, max_lag
    )

    # --- ç¬¬äºŒéƒ¨åˆ†ï¼šç¨³å®šæ€§æŒ‡æ ‡ (Turnover/Autocorr) ---
    turnover_stats = batch_calc_factor_turnover(
        df, factor_pattern, date_col, asset_col
    )

    # --- ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®æˆ˜æ”¶ç›ŠæŒ‡æ ‡ (Quantile Returns) ---
    # æˆ‘ä»¬ä»ä¸­æå–å¤šç©ºå¹´åŒ–æ”¶ç›Šå’Œå¤šç©ºå¤æ™®
    q_rets = batch_calc_quantile_returns(
        df, factor_pattern, ret_col, date_col, n_bins
    )

    ls_metrics = (
        q_rets.pivot(index=[date_col, "factor"], on="quantile", values="daily_ret")
        .with_columns((pl.col("Q1") - pl.col(f"Q{n_bins}")).alias("ls_ret"))
        .group_by("factor")
        .agg([
            (pl.col("ls_ret").mean() * 242).alias("annual_ls_ret"),
            (pl.col("ls_ret").mean() / pl.col("ls_ret").std() * (242 ** 0.5)).alias("ls_sharpe")
        ])
    )

    # --- æœ€ç»ˆåˆå¹¶æ‰€æœ‰ç»´åº¦ ---
    master_table = (
        full_metrics
        .join(turnover_stats, on="factor")
        .join(ls_metrics, on="factor")
    )

    return master_table
