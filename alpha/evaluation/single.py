"""
å•å› å­æ·±åº¦åˆ†æå·¥å…·é›†
åŒ…å« IC è®¡ç®—ã€åˆ†å±‚æ”¶ç›Šåˆ†æã€è¡°å‡ä¸æ¢æ‰‹ç‡ç­‰åŠŸèƒ½

"""
from typing import Literal, Union

import numpy as np
import polars as pl
from loguru import logger

from alpha.evaluation.batch import batch_get_ic_summary
from alpha.utils.schema import F


def single_calc_ic_analysis(
        df: pl.DataFrame,
        factor_col: str,
        ret_col: str,
        date_col: str = F.DATE,
        rolling_window: int = 20
) -> pl.DataFrame:
    """
    è®¡ç®—å•å› å­çš„æ¯æ—¥ IC åºåˆ—åŠæ»šåŠ¨ ICIR
    """
    # 1. è®¡ç®—æ¯æ—¥ Rank IC
    ic_series = (
        df.group_by(date_col)
        .agg(pl.corr(pl.col(factor_col).rank(), pl.col(ret_col).rank(), method="pearson").alias("ic"))
        .sort(date_col)
    )

    # 2. è®¡ç®—æ»šåŠ¨æŒ‡æ ‡ (è¯†åˆ«å› å­æœ€è¿‘æ˜¯å¦å¤±æ•ˆ)
    ic_analysis = ic_series.with_columns([
        pl.col("ic").rolling_mean(rolling_window).alias("rolling_ic_mean"),
        (pl.col("ic").rolling_mean(rolling_window) / pl.col("ic").rolling_std(rolling_window)).alias("rolling_ir"),
        pl.col("ic").cum_sum().alias("cum_ic")
    ])

    return ic_analysis


def _check_factor_smoothness(q_rets: pl.DataFrame, n_bins: int) -> dict:
    """
    åˆ¤æ–­åˆ†å±‚æ”¶ç›Šçš„å¹³æ»‘åº¦
    """
    # 1. è®¡ç®—å„åˆ†å±‚çš„å…¨å‘¨æœŸå¹³å‡æ”¶ç›Š
    mean_rets = (
        q_rets.group_by("quantile")
        .agg(pl.col("ret").mean())
        .sort("quantile")
    )

    # 2. è®¡ç®—å•è°ƒæ€§å¾—åˆ† (Spearman Rank Correlation)
    # ç†æƒ³å€¼æ˜¯ 1 (ä¸¥æ ¼å•è°ƒé€’å¢) æˆ– -1 (ä¸¥æ ¼å•è°ƒé€’å‡)
    quantile_idx = np.arange(1, n_bins + 1)
    return_values = mean_rets["ret"].to_numpy()

    # ä½¿ç”¨ç®€å•ç›¸å…³ç³»æ•°è¡¡é‡å•è°ƒæ€§
    monotonicity = np.corrcoef(quantile_idx, return_values)[0, 1]

    # 3. è®¡ç®—æ”¶ç›Šé—´è·çš„ç¨³å®šæ€§ (Gap Deviation)
    # å¦‚æœ Q1-Q2, Q2-Q3... çš„é—´è·å‡åŒ€ï¼Œè¯´æ˜å› å­å¯¹å„åˆ†æ®µçš„åŒºåˆ†åº¦éƒ½å¾ˆå¹³æ»‘
    gaps = np.diff(return_values)
    gap_cv = np.std(gaps) / (np.abs(np.mean(gaps)) + 1e-9)  # é—´è·å˜å¼‚ç³»æ•°ï¼Œè¶Šå°è¶Šå¹³æ»‘

    return {
        "monotonicity_score": monotonicity,
        "gap_stability": 1 / (1 + gap_cv)  # å½’ä¸€åŒ–ï¼Œè¶Šæ¥è¿‘ 1 è¶Šå¹³æ»‘
    }


def single_calc_quantile_metrics(
        df: Union[pl.DataFrame, pl.LazyFrame],  # ä¿®æ”¹æ”¯æŒ LazyFrame
        factor_col: str,
        ret_col: str,
        date_col: str = F.DATE,
        asset_col: str = F.ASSET,
        pool_mask_col: str = F.POOL_MASK,
        n_bins: int = 10,
        mode: Literal['long_only', 'long_short', 'active'] = 'active',
        period: int = 1,
        cost: float = 0.0,
        est_turnover: float = 0.2,
        annual_days: int = 251,
        direction: Literal[1, -1] = 1,  # ğŸ†• æ–°å¢æ–¹å‘å‚æ•°
) -> dict:
    # --- 0. ç»Ÿä¸€è½¬ä¸º LazyFrame ä»¥ä¾¿åˆ©ç”¨ä¸‹å‹ä¼˜åŒ– ---
    lf = df.lazy() if isinstance(df, pl.DataFrame) else df

    # --- 1. æ¨¡æ‹Ÿè°ƒä»“å‘¨æœŸé€»è¾‘ (è¿™é‡Œå¿…é¡» collectï¼Œå› ä¸º Python éœ€è¦æ—¥æœŸåˆ—è¡¨æ¥åšå¾ªç¯) ---
    all_dates = (
        lf.select(date_col)
        .unique()
        .sort(date_col)
        .collect()
        .get_column(date_col)
        .to_list()
    )
    rebalance_dates = [all_dates[i] for i in range(0, len(all_dates), period)]

    # --- 2. åŠ¨æ€è‚¡ç¥¨æ± è¿‡æ»¤ä¸åˆ†å±‚ ---
    working_lf = lf.filter(pl.col(pool_mask_col)) if pool_mask_col else lf

    # åˆ†ç»„åˆ†ä½æ•°è®¡ç®—
    df_with_q = (
        working_lf.with_columns(
            pl.when(pl.col(date_col).is_in(rebalance_dates))
            .then(
                pl.col(factor_col)
                .rank(method="random")
                .over(date_col)
                .qcut(n_bins, labels=[f"Q{i + 1}" for i in range(n_bins)])
            )
            .otherwise(None).alias("quantile")
        )
        .sort([asset_col, date_col])
        .with_columns(pl.col("quantile").forward_fill().over(asset_col))
        .filter(pl.col("quantile").is_not_null())
    )

    # --- 3. èšåˆæ”¶ç›Š ---
    q_rets_lf = (
        df_with_q.group_by([date_col, "quantile"])
        .agg([
            pl.col(ret_col).mean().alias("ret"),
            pl.len().alias("count")  # å¢åŠ  count ç”¨äºåç»­ç»Ÿè®¡
        ])
    )

    # Pivot æ“ä½œåœ¨ Polars Lazy ä¸­æ˜¯é˜»å¡çš„ï¼Œä¼šè‡ªåŠ¨è§¦å‘éƒ¨åˆ† collect
    res_series = q_rets_lf.collect().pivot(
        index=date_col, on="quantile", values="ret"
    ).sort(date_col)

    # --- 4. æ‰£é™¤æˆæœ¬ ---
    reb_cost = est_turnover * period * cost
    # --- æ ¹æ®æ–¹å‘ç¡®å®šå¤šå¤´å’Œç©ºå¤´æ¡¶ ---
    if direction == 1:
        long_col = f"Q{n_bins}"  # å› å­å€¼æœ€å¤§ä¸ºå¤šå¤´
        short_col = "Q1"
    else:
        long_col = "Q1"  # å› å­å€¼æœ€å°ä¸ºå¤šå¤´
        short_col = f"Q{n_bins}"
    all_q_cols = [f"Q{i + 1}" for i in range(n_bins)]

    if mode == "long_only":
        res_series = res_series.with_columns(pl.col(long_col).alias("raw_ret"))
    elif mode == "long_short":
        # æ­¤æ—¶å¦‚æœæ˜¯ direction=-1ï¼Œä¼šè‡ªåŠ¨å˜æˆ Q1 - Q10
        res_series = res_series.with_columns((pl.col(long_col) - pl.col(short_col)).alias("raw_ret"))
        reb_cost = reb_cost * 2
    elif mode == "active":
        # ä½¿ç”¨ long_col å‡å»æˆªé¢å¹³å‡
        res_series = res_series.with_columns(
            (pl.col(long_col) - pl.mean_horizontal(all_q_cols)).alias("raw_ret")
        )

    res_series = res_series.with_columns(
        pl.when(pl.col(date_col).is_in(rebalance_dates))
        .then(pl.col("raw_ret") - reb_cost)
        .otherwise(pl.col("raw_ret"))
        .alias("target_ret")
    ).with_columns(
        (pl.col("target_ret").fill_null(0) + 1).cum_prod().alias("nav")
    )

    # --- 5. è®¡ç®—è¯„ä»·æŒ‡æ ‡ ---
    total_days = len(all_dates)
    if total_days <= 1:
        return {"error": "Insufficient data"}

    # ä½¿ç”¨ get_column æ›¿ä»£ [col]
    nav_arr = res_series.get_column("nav").to_numpy()
    target_ret_arr = res_series.get_column("target_ret")

    total_ret = nav_arr[-1] - 1 if len(nav_arr) > 0 else 0.0
    annual_ret = (1 + total_ret) ** (annual_days / total_days) - 1
    annual_vol = target_ret_arr.std() * (annual_days ** 0.5)
    sharpe_ratio = annual_ret / (annual_vol + 1e-9)

    # æœ€å¤§å›æ’¤
    running_max = np.maximum.accumulate(nav_arr)
    max_drawdown = np.min((nav_arr - running_max) / (running_max + 1e-9))

    # ç¨³å®šæ€§åˆ†æ
    # æ³¨æ„ï¼šq_rets æ­¤æ—¶éœ€è¦ collect
    q_rets_df = q_rets_lf.collect()
    smoothness = _check_factor_smoothness(q_rets_df, n_bins)

    return {
        "quantile_daily_ret": q_rets_df,
        "series": res_series,
        "mode": mode,
        "metrics": {
            "total_return": total_ret,
            "annual_return": annual_ret,
            "annual_volatility": annual_vol,
            "sharpe_ratio": sharpe_ratio,
            "max_drawdown": max_drawdown,
            "win_rate": res_series.filter(pl.col("target_ret") > 0).height / total_days,
            "monotonicity": smoothness["monotonicity_score"],
            "smoothness_index": smoothness["gap_stability"],
            "avg_count_per_bin": q_rets_df.get_column("count").mean(),
            "total_obs": q_rets_df.get_column("count").sum(),
            "rebalance_period": period,
            "avg_daily_turnover": est_turnover
        }
    }

def single_calc_decay_turnover(
        df: Union[pl.DataFrame, pl.LazyFrame],
        factor_col: str,
        ret_col: str,
        date_col: str = F.DATE,
        asset_col: str = F.ASSET,
        pool_mask_col: str = F.POOL_MASK,
        max_lag: int = 10
) -> dict:
    lf = df.lazy() if isinstance(df, pl.DataFrame) else df

    # 1. åœ¨â€œå®Œæ•´æ—¶åºâ€ä¸Šè®¡ç®—ä½ç§»åˆ—ï¼ˆä¸è¦å…ˆ filterï¼ï¼‰
    # è¿™æ · shift(1).over(asset) æ‰èƒ½æ‰¾åˆ°ç‰©ç†ä¸Šçš„å‰ä¸€ä¸ªäº¤æ˜“æ—¥
    shift_exprs = [
        pl.col(ret_col).shift(-i).over(asset_col).alias(f"_ret_lag_{i}")
        for i in range(max_lag)
    ]
    shift_exprs.append(pl.col(factor_col).shift(1).over(asset_col).alias("_factor_pre"))

    # 2. é¢„è®¡ç®—ä½ç§»å¹¶åº”ç”¨è¿‡æ»¤
    # åœ¨è¿™é‡Œ filterï¼Œä¿è¯ corr è®¡ç®—æ—¶åªä½¿ç”¨ POOL_MASK=True ä¸”ä½ç§»æˆåŠŸçš„è¡Œ
    filtered_lf = (
        lf.with_columns(shift_exprs)
        .filter(pl.col(pool_mask_col)) # è®¡ç®—å®Œä½ç§»å†è¿‡æ»¤
        .select([date_col, factor_col, "_factor_pre"] + [f"_ret_lag_{i}" for i in range(max_lag)])
    )

    # 3. è®¡ç®—èšåˆæŒ‡æ ‡
    daily_res = (
        filtered_lf.group_by(date_col)
        .agg([
            pl.corr(factor_col, f"_ret_lag_{i}", method="spearman").alias(f"ic_{i}")
            for i in range(max_lag)
        ] + [
            pl.corr(factor_col, "_factor_pre", method="spearman").alias("ac")
        ])
        .collect()
    )

    # 4. æå–å‡å€¼å¹¶å¤„ç†ç©º
    # ä½¿ç”¨ drop_nans().mean() ä¿è¯ç¨³å¥æ€§
    lags = [daily_res.get_column(f"ic_{i}").drop_nans().mean() or 0.0 for i in range(max_lag)]
    autocorr_val = daily_res.get_column("ac").drop_nans().mean() or 0.0

    # 5. æ¢æ‰‹ç‡è®¡ç®—é€»è¾‘ä¿æŠ¤
    # å¦‚æœ autocorr è¿˜æ˜¯ nanï¼Œç»™å®šä¸€ä¸ªä¿å®ˆçš„æä½å€¼ 0.0 (ä»£è¡¨ 100% æ¢æ‰‹)
    safe_ac = autocorr_val if not np.isnan(autocorr_val) else 0.0
    est_daily_turnover = (1 - max(0, safe_ac)) * 0.85

    return {
        "ic_lags": lags,
        "autocorr": autocorr_val,
        "est_daily_turnover": est_daily_turnover
    }

def single_factor_alpha_analysis(
        df: Union[pl.DataFrame, pl.LazyFrame],
        factor_col: str,
        ret_col: str,
        date_col: str = F.DATE,
        asset_col: str = F.ASSET,
        pool_mask_col: str = F.POOL_MASK,
        mode: Literal['long_only', 'long_short', 'active'] = 'active',
        n_bins: int = 5,
        period: int = 1,
        cost: float = 0.0015  # é»˜è®¤å•è¾¹è´¹ç‡ï¼ˆå¦‚å°èŠ±ç¨+ä½£é‡‘ï¼‰
) -> dict:
    """
    ã€å·¥ä¸šçº§ã€‘å•å› å­å…¨èƒ½ä½“æ£€æŠ¥å‘Šï¼š
    é›†æˆä¿¡å·è¡°å‡ã€è‡ªç›¸å…³æ€§æ¢æ‰‹ä¼°ç®—ã€æ‰£è´¹åˆ†å±‚å›æµ‹ã€‚
    """

    # 1. ä¿¡å·è¡°å‡ä¸æ¢æ‰‹ç‡ä¼°ç®— (æ ¸å¿ƒï¼šå…ˆç®—ç¨³å®šæ€§)
    # è¿”å›åŒ…å« ic_lags, autocorr, est_daily_turnover çš„å­—å…¸
    logger.info("ğŸ” æ­£åœ¨è®¡ç®—å› å­ä¿¡å·è¡°å‡ä¸æ¢æ‰‹ç‡ä¼°ç®—...")
    decay_stats = single_calc_decay_turnover(
        df, factor_col, ret_col, date_col, asset_col
    )
    logger.info(f"    > ä¼°ç®—æ—¥å‡æ¢æ‰‹ç‡: {decay_stats['est_daily_turnover']:.2%} (è‡ªç›¸å…³: {decay_stats['autocorr']:.3f})")
    est_turnover = decay_stats['est_daily_turnover']


    logger.info("ğŸ” æ­£åœ¨è®¡ç®—å› å­é¢„æµ‹æ•ˆåŠ›æŒ‡æ ‡ (IC Summary)...")
    # 2. åŸºç¡€ IC ç»Ÿè®¡ (é¢„æµ‹æ•ˆåŠ›)
    ic_summary = batch_get_ic_summary(
        df,
        factor_pattern=f"^{factor_col}$",
        ret_col=ret_col,
        date_col=date_col
    )
    ic_mean = ic_summary['ic_mean'][0]
    logger.info(f"    > IC å‡å€¼: {ic_mean:.4f}, ICIR: {ic_summary['ic_ir'][0]:.4f}")

    # 3. åˆ†å±‚æ”¶ç›Šä¸å®ç›˜é£é™©æŒ‡æ ‡ (ä¼ å…¥ä¼°ç®—çš„ est_turnover è¿›è¡Œæ‰£è´¹)
    quantile_res = single_calc_quantile_metrics(
        df, factor_col, ret_col,
        date_col=date_col,
        asset_col=asset_col,
        pool_mask_col=pool_mask_col,
        mode=mode,
        n_bins=n_bins,
        period=period,
        cost=cost,
        est_turnover=est_turnover,  # è‡ªåŠ¨å…³è”æ¢æ‰‹
        direction= 1 if ic_mean > 0 else -1  # æ ¹æ®ä¿¡å·æ–¹å‘è°ƒæ•´å¤šç©ºé€»è¾‘
    )

    m = quantile_res['metrics']
    nav_series = quantile_res['series']

    # --- å¼€å§‹æ‰“å°å…¨é‡è§£é‡ŠæŠ¥å‘Š ---
    print(f"\n{'#' * 30} å› å­ä½“æ£€æŠ¥å‘Š: {factor_col} {'#' * 30}")

    # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šé¢„æµ‹æ•ˆåŠ› ---
    print("\nã€1. é¢„æµ‹æ•ˆåŠ› - è¡¡é‡å› å­æ•æ‰æ”¶ç›Šçš„ç›¸å…³æ€§ã€‘")
    ic_val = ic_summary['ic_mean'][0]
    icir_val = ic_summary['ic_ir'][0]
    print(f"  > IC å‡å€¼: {ic_val:.4f}")
    print("    [è§£é‡Š]: å› å­å€¼ä¸ä¸‹æœŸæ”¶ç›Šçš„ç›¸å…³ç³»æ•°ã€‚>0.02ä»£è¡¨æœ‰é¢„æµ‹åŠ›ï¼Œå€¼è¶Šå¤§æ–¹å‘è¶Šå‡†ã€‚")
    print(f"  > ICIR: {icir_val:.4f}")
    print("    [è§£é‡Š]: ICå‡å€¼/ICæ ‡å‡†å·®ã€‚è¡¡é‡ç¨³å®šæ€§ï¼Œ>0.5ä»£è¡¨ä¿¡å·ç¨³å¥ã€‚")

    # --- ç¬¬äºŒéƒ¨åˆ†ï¼šå®ç›˜è¡¨ç° ---
    print("\nã€2. å®ç›˜è¡¨ç° - æ¨¡æ‹ŸçœŸå®äº¤æ˜“æ‰£è´¹åçš„æ”¶ç›Šã€‘")
    print(f"  > å‡€å¹´åŒ–æ”¶ç›Š: {m['annual_return']:.2%}")
    print("    [è§£é‡Š]: è€ƒè™‘è°ƒä»“å‘¨æœŸå’ŒåŸºäºè‡ªç›¸å…³æ€§ä¼°ç®—çš„æ¢æ‰‹æ‰£è´¹åçš„å¹´åŒ–ã€‚")
    print(f"  > å‡€å¤æ™®æ¯”ç‡: {m['sharpe_ratio']:.2f}")
    print(f"  > æœ€å¤§å›æ’¤: {m['max_drawdown']:.2%}")

    # --- ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ‰§è¡Œæˆæœ¬ ---
    print("\nã€3. æ‰§è¡Œæˆæœ¬ - è¡¡é‡å› å­åœ¨å®ç›˜ä¸­è½åœ°çš„éš¾æ˜“åº¦ã€‘")
    print(f"  > ä¼°ç®—æ—¥å‡æ¢æ‰‹ç‡: {est_turnover:.2%}")
    print(f"    [è§£é‡Š]: åŸºäºå› å­ç§©è‡ªç›¸å…³æ€§(AC={decay_stats['autocorr']:.3f})æ¨å¯¼å‡ºçš„æ¯æ—¥å¤´å¯¸å˜åŠ¨ã€‚")
    print(f"  > è°ƒä»“å‘¨æœŸ: {period} äº¤æ˜“æ—¥")
    print(f"  > æ‘©æ“¦æˆæœ¬ç³»æ•°: {cost * 10000:.1f} bps (åŸºç‚¹)")

    # --- ç¬¬å››éƒ¨åˆ†ï¼šé€»è¾‘å¥å£®æ€§ ---
    print("\nã€4. é€»è¾‘å¥å£®æ€§ - æ£€éªŒå› å­èµšé’±çš„åº•å±‚é€»è¾‘ã€‘")
    print(f"  > æ”¶ç›Šå•è°ƒæ€§: {m['monotonicity']:.2f}")
    print(f"  > åˆ†å±‚å¹³æ»‘åº¦: {m['smoothness_index']:.2f}")

    # --- ç¬¬äº”éƒ¨åˆ†ï¼šä¿¡å·è¡°å‡ ---
    print("\nã€5. ä¿¡å·è¡°å‡ - è¡¡é‡å› å­çš„â€œä¿é²œæœŸâ€ã€‘")
    lags = decay_stats['ic_lags']
    # é¿å…é™¤ä»¥ 0ï¼Œä¸” lag0 é€šå¸¸æ˜¯å½“æœŸ IC
    lag1_val = lags[1] if len(lags) > 1 else 1e-9
    lag5_val = lags[5] if len(lags) > 5 else 0.0
    retention = (lag5_val / lag1_val) if lag1_val != 0 else 0.0
    print(f"  > ä¿¡å·ç•™å­˜ç‡ (Lag5/Lag1): {retention:.1%}")
    print("    [è§£é‡Š]: 5å¤©åé¢„æµ‹èƒ½åŠ›å‰©ä¸‹çš„æ¯”ä¾‹ã€‚è‹¥<20%ï¼Œè¯´æ˜è¯¥å› å­å¿…é¡»é«˜é¢‘è°ƒä»“ã€‚")

    # --- æ ·æœ¬ç»Ÿè®¡ ---
    print("\nã€6. æ ·æœ¬ç»Ÿè®¡ã€‘")
    print(f"  > æ¯å±‚å¹³å‡æ ·æœ¬æ•°: {m['avg_count_per_bin']:.1f}")

    print(f"\n{'#' * 78}\n")
    logger.info("âœ… å› å­ä½“æ£€æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ã€‚")

    return {
        "summary": ic_summary,
        "metrics": m,
        "decay": decay_stats,
        "nav": nav_series
    }
