import numpy as np
import polars as pl
from loguru import logger
from typing import Dict

from alpha_factory.data_provider import DataProvider
from alpha_factory.data_provider.pool import main_small_pool
from alpha_factory.data_provider.utils import extract_expressions_from_csv
from alpha_factory.evaluation.batch.cluster import batch_clustering
from alpha_factory.evaluation.batch.returns import batch_quantile_returns
from alpha_factory.gp.extra_terminal import add_extra_terminals
from alpha_factory.gp.label import label_OO_for_tradable
from alpha_factory.utils.config import settings
from alpha_factory.utils.schema import F


def main():
    # 1. è®¾ç½®è·¯å¾„ä¸æå–è¡¨è¾¾å¼
    path = settings.OUTPUT_DIR / "gp" / "SmallCSGenerator" / "best_factors.csv"
    exprs = extract_expressions_from_csv(path)

    # åˆ›å»ºå› å­åä¸è¡¨è¾¾å¼çš„æ˜ å°„å­—å…¸
    factor_expr_map: Dict[str, str] = {
        f"factor_{i + 1}": expr for i, expr in enumerate(exprs)
    }
    # ç”¨äºæ·»åŠ åˆ°æ•°æ®ç®¡é“
    exprs_with_names = [f"factor_{i + 1}={expr}" for i, expr in enumerate(exprs)]

    logger.info(f"ğŸš€ ä» CSV æå–å¹¶å‡†å¤‡è®¡ç®— {len(exprs)} æ¡å› å­è¡¨è¾¾å¼")

    # 2. åŠ è½½æ•°æ®å¹¶è®¡ç®—å› å­
    lf = DataProvider().load_data(
        start_date="20190101",
        end_date="20251231",
        funcs=[main_small_pool, add_extra_terminals, label_OO_for_tradable],
        column_exprs=[f"{F.LABEL_FOR_RET}=OPEN[-2] / OPEN[-1] - 1", *exprs_with_names],
        lookback_window=200,
    )

    # 3. æ‰¹é‡ç»©æ•ˆè¯„ä¼°
    logger.info("ğŸ“Š æ­£åœ¨è¿›è¡Œå› å­ç»©æ•ˆè¯„ä¼°...")
    df_result = batch_quantile_returns(lf)

    # åå¤„ç†ï¼šæ·»åŠ  expression åˆ—å¹¶æ’åºåˆ—
    df_result = df_result.with_columns(
        pl.col("factor")
        .map_elements(
            lambda f: factor_expr_map.get(f, "unknown"), return_dtype=pl.String
        )
        .alias("expression")
    ).select(
        [
            "factor",
            "expression",
            *[col for col in df_result.columns if col not in ["factor", "expression"]],
        ]
    )

    # 4. é€»è¾‘èšç±»
    logger.info("ğŸ” æ­£åœ¨è¿›è¡Œå› å­é€»è¾‘èšç±» (é‡‡æ · 50,000 è¡Œ)...")
    cluster_dict = batch_clustering(lf, sample_n=50000)

    # 5. Cluster åˆ†ç»„ç»Ÿè®¡ä¿¡æ¯
    logger.info("ğŸ“ˆ æ­£åœ¨ç”Ÿæˆ Cluster åˆ†ç»„ç»Ÿè®¡...")

    # è½¬æ¢èšç±»å­—å…¸ä¸º DataFrame
    df_clusters = pl.DataFrame(
        {"factor": list(cluster_dict.keys()), "cluster_id": list(cluster_dict.values())}
    )

    # åˆå¹¶ç»©æ•ˆä¸èšç±» ID
    df_merged = df_result.join(df_clusters, on="factor")

    # æŒ‰ Cluster åˆ†ç»„ï¼šç»Ÿè®¡å› å­æ•°ã€æœ€é«˜å¤æ™®ã€å¹¶é€‰å‡ºæœ€å¼ºå› å­çš„ ID
    df_cluster_stats = (
        df_merged.group_by("cluster_id")
        .agg(
            [
                pl.count("factor").alias("å› å­æ•°é‡"),
                pl.max("sharpe").alias("æœ€é«˜å¤æ™®"),
                # æ‰¾åˆ°å¤æ™®æœ€é«˜çš„é‚£ä¸ªå› å­çš„ ID
                pl.col("factor").sort_by("sharpe").last().alias("æœ€å¼ºå› å­ID"),
                # æ‰¾åˆ°å¤æ™®æœ€é«˜çš„é‚£ä¸ªå› å­çš„ è¡¨è¾¾å¼
                pl.col("expression").sort_by("sharpe").last().alias("æœ€å¼ºå› å­é€»è¾‘"),
            ]
        )
        .sort("æœ€é«˜å¤æ™®", descending=True)
    )

    # æ‰“å° Cluster ç»Ÿè®¡è¡¨
    print("\n" + "=" * 60 + " Cluster åˆ†ç»„ç»Ÿè®¡ä¿¡æ¯ " + "=" * 60)
    with pl.Config(fmt_str_lengths=100, tbl_rows=20):
        print(df_cluster_stats)
    print("=" * 140 + "\n")

    # 6. ç›¸å…³æ€§åˆ†æ
    logger.info("ğŸ§ª æ­£åœ¨è®¡ç®—å„ Cluster æœ€å¼ºä»£è¡¨å› å­ä¹‹é—´çš„ç›¸å…³æ€§...")

    # è·å–æ¯ä¸ªç°‡æœ€å¼ºå› å­çš„ ID åˆ—è¡¨
    best_factor_ids = df_cluster_stats["æœ€å¼ºå› å­ID"].to_list()

    # ä» LazyFrame ä¸­æå–è¿™äº›å› å­çš„æ•°æ®å¹¶è®¡ç®—ç›¸å…³æ€§
    # é‡‡æ · 20000 è¡Œè¶³ä»¥ä»£è¡¨æˆªé¢ç›¸å…³æ€§
    df_corr_data = (
        lf.select(best_factor_ids).collect().sample(n=min(20000, 50000)).to_pandas()
    )
    corr_matrix = df_corr_data.corr()

    # æ‰“å°ç›¸å…³æ€§çŸ©é˜µ
    print(
        "\n" + "=" * 50 + " å„ Cluster æ—é•¿ç›¸å…³æ€§çŸ©é˜µ (Cross-Correlation) " + "=" * 50
    )
    print(corr_matrix.round(2))

    # è®¡ç®—æ•´ä½“ç›¸å…³æ€§æŒ‡æ ‡
    n = len(corr_matrix)
    if n > 1:
        # ä½¿ç”¨ np.triu_indices æå–ä¸Šä¸‰è§’ï¼ˆä¸å«å¯¹è§’çº¿ï¼‰
        upper_indices = np.triu_indices(n, k=1)
        corr_values = corr_matrix.values[upper_indices]
        mean_corr = corr_values.mean()
        max_corr = corr_values.max()

        print("-" * 118)
        logger.info(f"ğŸ’¡ ç°‡é—´ä»£è¡¨å› å­å¹³å‡ç›¸å…³æ€§: {mean_corr:.4f}")
        logger.info(f"ğŸ”¥ æœ€å¤§ç°‡é—´ç›¸å…³æ€§: {max_corr:.4f}")

        if mean_corr < 0.3:
            logger.success("ğŸš€ ç»“è®ºï¼šå› å­æ± é€»è¾‘åˆ†æ•£åº¦æé«˜ï¼Œå…·å¤‡æå¼ºçš„ç»„åˆæ½œåŠ›ï¼")
        else:
            logger.warning("âš ï¸ ç»“è®ºï¼šéƒ¨åˆ† Cluster ä¹‹é—´ä»å­˜åœ¨ä¸€å®šç›¸å…³æ€§ã€‚")
    print("=" * 118 + "\n")


if __name__ == "__main__":
    main()
