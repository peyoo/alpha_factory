import polars.selectors as cs
from typing import List, Union
import polars as pl
from loguru import logger
import time
from alpha_factory.utils.schema import F


def batch_calc_factor_turnover(
        df: Union[pl.DataFrame, pl.LazyFrame],
        factors: Union[str, List[str]] = r"^factor_.*",
        date_col: str = F.DATE,
        asset_col: str = F.ASSET,
        label_col=F.LABEL_FOR_RET,
        n_bins: int = 10,
        lag: int = 1,
        descending = False
) -> pl.DataFrame:
    """
    åŸºäº Rank å˜åŒ–æ¯”ä¾‹æ³•æ‰¹é‡è®¡ç®—å› å­æ¢æ‰‹ç‡ï¼ˆTop æ¡¶ï¼‰ã€‚

    âš ï¸ **é‡è¦è¯´æ˜**ï¼šæœ¬å‡½æ•°ä»…è®¡ç®— **Top æ¡¶**ï¼ˆå› å­å€¼æœ€å¤§çš„å‰ 1/n_binsï¼‰çš„æ¢æ‰‹ç‡ï¼Œ
    ä¸åŒºåˆ†å› å­æ–¹å‘ï¼ˆIC æ­£/è´Ÿï¼‰ã€‚è‹¥éœ€æ ¹æ® IC è‡ªåŠ¨é€‰æ‹© Top/Btm æ¡¶ï¼Œè¯·ä½¿ç”¨
    `batch_calc_factor_turnover_with_direction()` å‡½æ•°ã€‚

    è®¡ç®—é€»è¾‘ï¼š
    1. **Rank è®¡ç®—**ï¼šæ¯æ—¥æˆªé¢å¯¹å› å­å€¼è¿›è¡Œé™åºæ’åï¼Œå€¼å¤§æ’åé å‰ã€‚
    2. **Top æ¡¶æ ‡è®°**ï¼šæ ‡è®°æ’ååœ¨å‰ 1/n_bins çš„èµ„äº§ä¸º Top æŒä»“ã€‚
    3. **ä¿¡å·æ»å**ï¼šå°† Top æ ‡è®°å‘åä½ç§» lag æœŸï¼Œè·å–å‰æœŸæŒä»“çŠ¶æ€ã€‚
    4. **æ¢æ‰‹è®¡ç®—**ï¼šæ¢æ‰‹ç‡ = æ–°è¿›å…¥ Top çš„æ•°é‡ / Top æ€»æ•°é‡ã€‚

    Args:
        df: è¾“å…¥æ•°æ®ï¼ŒåŒ…å«å› å­åˆ—
        factors: å› å­åˆ—åæ­£åˆ™è¡¨è¾¾å¼æˆ–åˆ—è¡¨
        date_col: æ—¥æœŸåˆ—å
        asset_col: èµ„äº§åˆ—å
        n_bins: åˆ†æ¡¶æ•°é‡ï¼ŒTop æ¡¶ä¸ºå‰ 1/n_bins
        lag: æ»åæœŸæ•°ï¼Œé»˜è®¤ 1
        descending: æ’åæ˜¯å¦é™åºï¼Œé»˜è®¤ Falseï¼ˆå€¼å¤§æ’åé å‰ï¼‰

    Returns:
        pl.DataFrame: å› å­æ¢æ‰‹ç‡ç»Ÿè®¡è¡¨ã€‚
        | åˆ—å | ç±»å‹ | è¯´æ˜ |
        | :--- | :--- | :--- |
        | factor | String | å› å­åç§° |
        | avg_turnover | Float64 | å¹³å‡æ¢æ‰‹ç‡ (æ–°è¿›å…¥ Top / Top æ€»æ•°) |
        | turnover_std | Float64 | æ¢æ‰‹ç‡æ ‡å‡†å·® |

    Example:
        >>> df = pl.DataFrame({
        ...     "_DATE_": [20240101, 20240101, 20240102],
        ...     "_ASSET_": ["A", "B", "A"],
        ...     "factor_1": [0.5, 0.7, 0.6]
        ... })
        >>> batch_calc_factor_turnover(df, "factor_1", n_bins=2)
    """
    start_time = time.perf_counter()
    lf = df.lazy() if isinstance(df, pl.DataFrame) else df

    # --- 1. è‡ªåŠ¨è·å–å› å­åˆ— ---
    f_selector = cs.matches(factors) if isinstance(factors, str) else cs.by_name(factors)
    try:
        factor_cols = lf.select(f_selector).collect_schema().names()
    except Exception as e:
        logger.error(f"âŒ å› å­é€‰æ‹©å™¨åŒ¹é…å¤±è´¥: {e}")
        return pl.DataFrame()

    if not factor_cols:
        logger.warning(f"âš ï¸ æ— æ³•åŒ¹é…åˆ°ä»»ä½•å› å­ (æ¨¡å¼: {factors})ï¼Œè¿”å›ç©ºç»“æœã€‚")
        return pl.DataFrame()

    logger.info(f"ğŸ”„ å¼€å§‹è®¡ç®— {len(factor_cols)} ä¸ªå› å­çš„æ¢æ‰‹ç‡ (Rankå˜åŒ–æ³•, n_bins={n_bins}, lag={lag})")

    try:
        # --- 2. è®¡ç®— Rank å¹¶æ ‡è®° Top æ¡¶ ---
        lf_ranked = (
            lf.select([date_col, asset_col] + factor_cols)
            .sort([asset_col, date_col])  # ç¡®ä¿ over() åˆ†ç»„å†…æœ‰åº
            .with_columns(
                pl.count().over(date_col).alias("_daily_count_")
            )
            .with_columns([
                # é™åºæ’åï¼šå€¼å¤§ = æ’åé å‰ = Top
                (pl.col(f).rank(descending=descending).over(date_col)
                 <= (pl.col("_daily_count_") / n_bins)).alias(f"{f}_is_top")
                for f in factor_cols
            ]))

        # --- 3. è®¡ç®—æ»åä¿¡å· ---
        lf_with_lag = lf_ranked.with_columns([
            pl.col(f"{f}_is_top").shift(lag).over(asset_col).fill_null(False).alias(f"{f}_was_top")
            for f in factor_cols
        ])

        # --- 4. æŒ‰æ—¥èšåˆè®¡ç®—æ¢æ‰‹ ---
        daily_turnover = (
            lf_with_lag
            .group_by(date_col)
            .agg([
                # æ–°è¿›å…¥ = å½“å‰æ˜¯ Top ä¸”ä¹‹å‰ä¸æ˜¯
                (pl.col(f"{f}_is_top") & ~pl.col(f"{f}_was_top")).sum().alias(f"{f}_new_in")
                for f in factor_cols
            ] + [
                pl.col(f"{f}_is_top").sum().alias(f"{f}_top_count")
                for f in factor_cols
            ])
        )

        # --- 5. è®¡ç®—æ¢æ‰‹ç‡å¹¶è½¬ä¸ºé•¿è¡¨ ---
        # å…ˆè®¡ç®—æ¯æ—¥æ¢æ‰‹ç‡
        daily_turnover = daily_turnover.with_columns([
            (pl.col(f"{f}_new_in") / pl.col(f"{f}_top_count")).fill_null(0.0).alias(f"{f}_turnover")
            for f in factor_cols
        ])

        # è½¬ä¸ºé•¿è¡¨å¹¶èšåˆ
        turnover_cols = [f"{f}_turnover" for f in factor_cols]
        turnover_stats = (
            daily_turnover
            .select([date_col] + turnover_cols)
            .unpivot(
                index=date_col,
                on=turnover_cols,
                variable_name="factor_raw",
                value_name="turnover"
            )
            # è¿˜åŸå› å­åï¼ˆå»æ‰ _turnover åç¼€ï¼‰
            .with_columns(
                pl.col("factor_raw").str.replace("_turnover$", "").alias("factor")
            )
            .group_by("factor")
            .agg([
                pl.col("turnover").filter(pl.col("turnover").is_finite()).mean().alias("avg_turnover"),
                pl.col("turnover").filter(pl.col("turnover").is_finite()).std().alias("turnover_std"),
            ])
            .sort("avg_turnover")
            .collect()
        )

        duration = time.perf_counter() - start_time
        logger.success(f"âœ… å› å­æ¢æ‰‹ç‡ä¼°ç®—å®Œæˆ | è€—æ—¶: {duration:.2f}s | å› å­æ•°: {len(factor_cols)}")
        return turnover_stats

    except Exception as e:
        logger.exception(f"âŒ è®¡ç®—å› å­æ¢æ‰‹ç‡æ—¶å´©æºƒ: {e}")
        return pl.DataFrame()


def batch_calc_factor_turnover_with_direction(
        df: Union[pl.DataFrame, pl.LazyFrame],
        factors: Union[str, List[str]] = r"^factor_.*",
        label_col: str = F.LABEL_FOR_RET,
        date_col: str = F.DATE,
        asset_col: str = F.ASSET,
        n_bins: int = 10,
        lag: int = 1
) -> pl.DataFrame:
    """
    åŸºäº Rank å˜åŒ–æ¯”ä¾‹æ³•è®¡ç®—å› å­æ¢æ‰‹ç‡ï¼Œæ ¹æ® IC è‡ªåŠ¨é€‰æ‹© Top/Btm æ¡¶ã€‚

    **æ ¸å¿ƒå·®å¼‚**ï¼šæœ¬å‡½æ•°æ ¹æ®å› å­ä¸æ ‡ç­¾çš„ç›¸å…³æ€§ï¼ˆICï¼‰åˆ¤æ–­å› å­æ–¹å‘ï¼š
    - IC â‰¥ 0ï¼šå› å­å€¼å¤§ = å¥½ â†’ è®¡ç®— **Top æ¡¶**çš„æ¢æ‰‹ç‡
    - IC < 0ï¼šå› å­å€¼å° = å¥½ â†’ è®¡ç®— **Btm æ¡¶**çš„æ¢æ‰‹ç‡

    è¿™ç¡®ä¿è®¡ç®—çš„æ¢æ‰‹ç‡ä¸å®é™…æŒä»“ç­–ç•¥ç›¸å¯¹åº”ã€‚

    Args:
        df: è¾“å…¥æ•°æ®ï¼ŒåŒ…å«å› å­åˆ—å’Œæ ‡ç­¾åˆ—
        factors: å› å­åˆ—åæ­£åˆ™è¡¨è¾¾å¼æˆ–åˆ—è¡¨
        label_col: ç”¨äºè®¡ç®— IC çš„æ ‡ç­¾åˆ—ï¼ˆå¦‚æ—¥æ”¶ç›Šç‡ï¼‰
        date_col: æ—¥æœŸåˆ—å
        asset_col: èµ„äº§åˆ—å
        n_bins: åˆ†æ¡¶æ•°é‡ï¼ŒTop/Btm æ¡¶å„å  1/n_bins
        lag: ä¿¡å·æ»åæœŸæ•°ï¼Œé»˜è®¤ 1

    Returns:
        pl.DataFrame: å› å­æ¢æ‰‹ç‡ç»Ÿè®¡è¡¨ï¼Œé¢å¤–åŒ…å«æ–¹å‘æŒ‡ç¤ºã€‚
        | åˆ—å | ç±»å‹ | è¯´æ˜ |
        | :--- | :--- | :--- |
        | factor | String | å› å­åç§° |
        | ic_mean | Float64 | å¹³å‡ IC |
        | direction | Int32 | æ–¹å‘ï¼ˆ1=çœ‹æ¶¨ï¼Œ-1=çœ‹è·Œï¼‰ |
        | side | String | å®é™…æŒä»“æ¡¶ï¼ˆtop æˆ– btmï¼‰ |
        | avg_turnover | Float64 | å¹³å‡æ¢æ‰‹ç‡ |
        | turnover_std | Float64 | æ¢æ‰‹ç‡æ ‡å‡†å·® |
    """
    start_time = time.perf_counter()
    lf = df.lazy() if isinstance(df, pl.DataFrame) else df

    # --- 1. è‡ªåŠ¨è·å–å› å­åˆ— ---
    f_selector = cs.matches(factors) if isinstance(factors, str) else cs.by_name(factors)
    try:
        factor_cols = lf.select(f_selector).collect_schema().names()
    except Exception as e:
        logger.error(f"âŒ å› å­é€‰æ‹©å™¨åŒ¹é…å¤±è´¥: {e}")
        return pl.DataFrame()

    if not factor_cols:
        logger.warning(f"âš ï¸ æ— æ³•åŒ¹é…åˆ°ä»»ä½•å› å­ (æ¨¡å¼: {factors})ï¼Œè¿”å›ç©ºç»“æœã€‚")
        return pl.DataFrame()

    logger.info(f"ğŸ”„ å¼€å§‹è®¡ç®— {len(factor_cols)} ä¸ªå› å­çš„æ¢æ‰‹ç‡ï¼ˆå« IC æ–¹å‘åˆ¤æ–­ï¼‰")

    try:
        # --- 2. è®¡ç®— IC åˆ¤æ–­å› å­æ–¹å‘ ---
        # ä¿®å¤ï¼šå…ˆé€‰æ‹©æœ‰æ•ˆçš„æ•°æ®ï¼ˆæ’é™¤ null å’Œæ— ç©·å€¼ï¼‰
        lf_valid = (
            lf.select([date_col] + factor_cols + [label_col])
            .with_columns([
                pl.col(f).fill_null(strategy="forward").alias(f)
                for f in factor_cols + [label_col]
            ])
        )

        ic_daily = (
            lf_valid
            .group_by(date_col)
            .agg([
                pl.corr(f, label_col, method="spearman").alias(f"{f}_ic")
                for f in factor_cols
            ])
            .collect()
        )

        logger.debug("IC è®¡ç®—å®Œæˆï¼Œç»“æœæ‘˜è¦:")
        for f in factor_cols:
            ic_col = ic_daily.get_column(f"{f}_ic")
            logger.debug(f"  {f}: null={ic_col.is_null().sum()}, nan={(~ic_col.is_finite()).sum()}, mean={ic_col.mean()}")

        # æå–å„å› å­å¹³å‡ IC å¹¶åˆ¤æ–­æ–¹å‘
        factor_directions = {}
        factor_ics = {}
        for f in factor_cols:
            ic_series = ic_daily.get_column(f"{f}_ic").drop_nulls()

            # è¿‡æ»¤æœ‰é™å€¼ï¼ˆæ’é™¤ NaN, Infï¼‰
            ic_finite = ic_series.filter(ic_series.is_finite())

            if ic_finite.len() > 0:
                avg_ic = ic_finite.mean()
            else:
                logger.warning(f"âš ï¸ {f} çš„ IC å…¨éƒ¨ä¸º NaN/Infï¼Œä½¿ç”¨é»˜è®¤æ–¹å‘ 'top'")
                avg_ic = 0.0

            factor_directions[f] = "top" if avg_ic >= 0 else "btm"
            factor_ics[f] = avg_ic

        logger.debug("IC æ–¹å‘åˆ¤æ–­å®Œæˆ")
        for f in factor_cols:
            logger.debug(f"  {f}: IC={factor_ics[f]:.6f} â†’ {factor_directions[f]}")

        # --- 3. è®¡ç®— Rank å¹¶æ ‡è®° Top/Btm æ¡¶ ---
        lf_ranked = (
            lf.select([date_col, asset_col] + factor_cols)
            .sort([asset_col, date_col])  # ç¡®ä¿ over() åˆ†ç»„å†…æœ‰åº
            .with_columns(
                pl.len().over(date_col).alias("_daily_count_")
            )
            .with_columns([
                (pl.col(f).rank(descending=True).over(date_col)
                 <= (pl.col("_daily_count_") / n_bins)).alias(f"{f}_is_top")
                for f in factor_cols
            ] + [
                (pl.col(f).rank(descending=True).over(date_col)
                 > (pl.col("_daily_count_") * (n_bins - 1) / n_bins)).alias(f"{f}_is_btm")
                for f in factor_cols
            ])
        )

        # --- 4. è®¡ç®—æ»åä¿¡å· ---
        lf_with_lag = lf_ranked.with_columns([
            pl.col(f"{f}_is_top").shift(lag).over(asset_col).fill_null(False).alias(f"{f}_was_top")
            for f in factor_cols
        ] + [
            pl.col(f"{f}_is_btm").shift(lag).over(asset_col).fill_null(False).alias(f"{f}_was_btm")
            for f in factor_cols
        ])

        # --- 5. æŒ‰æ—¥èšåˆè®¡ç®—æ¢æ‰‹ ---
        daily_turnover = (
            lf_with_lag
            .group_by(date_col)
            .agg([
                (pl.col(f"{f}_is_top") & ~pl.col(f"{f}_was_top")).sum().alias(f"{f}_new_in_top")
                for f in factor_cols
            ] + [
                (pl.col(f"{f}_is_btm") & ~pl.col(f"{f}_was_btm")).sum().alias(f"{f}_new_in_btm")
                for f in factor_cols
            ] + [
                pl.col(f"{f}_is_top").sum().alias(f"{f}_top_count")
                for f in factor_cols
            ] + [
                pl.col(f"{f}_is_btm").sum().alias(f"{f}_btm_count")
                for f in factor_cols
            ])
            .with_columns([
                (pl.col(f"{f}_new_in_top") / pl.col(f"{f}_top_count")).fill_null(0.0).alias(f"{f}_turnover_top")
                for f in factor_cols
            ] + [
                (pl.col(f"{f}_new_in_btm") / pl.col(f"{f}_btm_count")).fill_null(0.0).alias(f"{f}_turnover_btm")
                for f in factor_cols
            ])
            .collect()
        )

        # --- 6. æ ¹æ® IC æ–¹å‘é€‰æ‹©å¯¹åº”æ¡¶çš„æ¢æ‰‹ç‡ ---
        results = []
        for f in factor_cols:
            side = factor_directions[f]
            turnover_col = f"{f}_turnover_{side}"
            turnover_series = daily_turnover.get_column(turnover_col).filter(
                daily_turnover.get_column(turnover_col).is_finite()
            )
            results.append({
                "factor": f,
                "ic_mean": factor_ics[f],
                "direction": 1 if side == "top" else -1,
                "side": side,
                "avg_turnover": turnover_series.mean() or 0.0,
                "turnover_std": turnover_series.std() or 0.0,
            })

        result_df = pl.DataFrame(results).sort("avg_turnover")

        duration = time.perf_counter() - start_time
        logger.success(f"âœ… å› å­æ¢æ‰‹ç‡ï¼ˆå«æ–¹å‘åˆ¤æ–­ï¼‰è®¡ç®—å®Œæˆ | è€—æ—¶: {duration:.2f}s | å› å­æ•°: {len(factor_cols)}")
        return result_df

    except Exception as e:
        logger.exception(f"âŒ è®¡ç®—å› å­æ¢æ‰‹ç‡æ—¶å´©æºƒ: {e}")
        return pl.DataFrame()


def batch_calc_factor_turnover_single_agg(
        df: Union[pl.DataFrame, pl.LazyFrame],
        factors: Union[str, List[str]] = r"^factor_.*",
        date_col: str = F.DATE,
        asset_col: str = F.ASSET,
        label_col=F.LABEL_FOR_RET,
        n_bins: int = 10,
        lag: int = 1
) -> pl.DataFrame:
    """
    å•è¡Œèšåˆæ–¹å¼è®¡ç®—å› å­æ¢æ‰‹ç‡ï¼ˆé«˜æ•ˆç‰ˆï¼‰ã€‚

    **ä¼˜åŠ¿**ï¼šåœ¨ä¸€ä¸ªèšåˆæ­¥éª¤ä¸­è®¡ç®—æ‰€æœ‰ç»Ÿè®¡é‡ï¼Œå‡å°‘ä¸­é—´æ­¥éª¤ï¼Œæ€§èƒ½æ›´ä¼˜ã€‚

    è®¡ç®—é€»è¾‘åŒ batch_calc_factor_turnover()ï¼Œä½†ä½¿ç”¨å•è¡Œ agg() å®ç°ï¼š
    1. è®¡ç®— Rank å¹¶æ ‡è®° Top æ¡¶
    2. è®¡ç®—æ»åä¿¡å·
    3. æŒ‰æ—¥æœŸèšåˆå¹¶è®¡ç®—æ¢æ‰‹ç‡ç»Ÿè®¡

    Args:
        df: è¾“å…¥æ•°æ®ï¼ŒåŒ…å«å› å­åˆ—
        factors: å› å­åˆ—åæ­£åˆ™è¡¨è¾¾å¼æˆ–åˆ—è¡¨
        date_col: æ—¥æœŸåˆ—å
        asset_col: èµ„äº§åˆ—å
        n_bins: åˆ†æ¡¶æ•°é‡ï¼ŒTop æ¡¶ä¸ºå‰ 1/n_bins
        lag: æ»åæœŸæ•°ï¼Œé»˜è®¤ 1

    Returns:
        pl.DataFrame: å› å­æ¢æ‰‹ç‡ç»Ÿè®¡è¡¨ï¼ˆä¸ batch_calc_factor_turnover() ç›¸åŒæ ¼å¼ï¼‰
        | åˆ—å | ç±»å‹ | è¯´æ˜ |
        | :--- | :--- | :--- |
        | factor | String | å› å­åç§° |
        | avg_turnover | Float64 | å¹³å‡æ¢æ‰‹ç‡ |
        | turnover_std | Float64 | æ¢æ‰‹ç‡æ ‡å‡†å·® |

    Example:
        >>> df = pl.DataFrame({
        ...     "DATE": [20240101, 20240101, 20240102],
        ...     "ASSET": ["A", "B", "A"],
        ...     "factor_1": [0.5, 0.7, 0.6],
        ...     "factor_2": [0.3, 0.8, 0.4]
        ... })
        >>> batch_calc_factor_turnover_single_agg(df, n_bins=2)
    """
    start_time = time.perf_counter()
    lf = df.lazy() if isinstance(df, pl.DataFrame) else df

    # --- 1. è‡ªåŠ¨è·å–å› å­åˆ— ---
    f_selector = cs.matches(factors) if isinstance(factors, str) else cs.by_name(factors)
    try:
        factor_cols = lf.select(f_selector).collect_schema().names()
    except Exception as e:
        logger.error(f"âŒ å› å­é€‰æ‹©å™¨åŒ¹é…å¤±è´¥: {e}")
        return pl.DataFrame()

    if not factor_cols:
        logger.warning(f"âš ï¸ æ— æ³•åŒ¹é…åˆ°ä»»ä½•å› å­ (æ¨¡å¼: {factors})ï¼Œè¿”å›ç©ºç»“æœã€‚")
        return pl.DataFrame()

    logger.info(f"ğŸ”„ å¼€å§‹è®¡ç®— {len(factor_cols)} ä¸ªå› å­çš„æ¢æ‰‹ç‡ (å•è¡Œèšåˆ, n_bins={n_bins}, lag={lag})")

    try:
        # --- 2. æ•°æ®é¢„å¤„ç†ï¼šè®¡ç®— Rank + æ»åä¿¡å· ---
        lf_prep = (
            lf.select([date_col, asset_col] + factor_cols)
            .sort([asset_col, date_col])
            .with_columns(
                pl.len().over(date_col).alias("_daily_count_")
            )
            .with_columns([
                # å½“å‰æ˜¯å¦åœ¨ Top æ¡¶
                (pl.col(f).rank(descending=True).over(date_col)
                 <= (pl.col("_daily_count_") / n_bins)).alias(f"{f}_is_top")
                for f in factor_cols
            ])
            .with_columns([
                # æ˜¨æ—¥æ˜¯å¦åœ¨ Top æ¡¶ï¼ˆæ»å lag æœŸï¼‰
                pl.col(f"{f}_is_top").shift(lag).over(asset_col)
                .fill_null(False).alias(f"{f}_was_top")
                for f in factor_cols
            ])
        )

        # --- 3. å•è¡Œèšåˆï¼šä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰æ¢æ‰‹ç‡ç»Ÿè®¡ ---
        daily_stats = (
            lf_prep
            .group_by(date_col)
            .agg([
                # å¯¹æ¯ä¸ªå› å­ï¼Œè®¡ç®— Top æ¡¶å¤§å°ã€æ–°è¿›å…¥æ•°é‡å’Œæ¢æ‰‹ç‡
                *[
                    (
                        (pl.col(f"{f}_is_top") & ~pl.col(f"{f}_was_top")).sum()
                        / pl.col(f"{f}_is_top").sum()
                    ).fill_null(0.0).alias(f"{f}_turnover")
                    for f in factor_cols
                ]
            ])
            .collect()
        )

        # --- 4. è½¬ä¸ºé•¿è¡¨å¹¶æœ€ç»ˆèšåˆ ---
        turnover_cols = [f"{f}_turnover" for f in factor_cols]
        result_df = (
            daily_stats
            .select([date_col] + turnover_cols)
            .unpivot(
                index=date_col,
                on=turnover_cols,
                variable_name="factor_raw",
                value_name="turnover"
            )
            .with_columns(
                pl.col("factor_raw").str.replace("_turnover$", "").alias("factor")
            )
            .group_by("factor")
            .agg([
                pl.col("turnover").filter(pl.col("turnover").is_finite()).mean().alias("avg_turnover"),
                pl.col("turnover").filter(pl.col("turnover").is_finite()).std().alias("turnover_std"),
            ])
            .sort("avg_turnover")
        )

        duration = time.perf_counter() - start_time
        logger.success(f"âœ… å› å­æ¢æ‰‹ç‡ï¼ˆå•è¡Œèšåˆï¼‰è®¡ç®—å®Œæˆ | è€—æ—¶: {duration:.2f}s | å› å­æ•°: {len(factor_cols)}")
        return result_df

    except Exception as e:
        logger.exception(f"âŒ è®¡ç®—å› å­æ¢æ‰‹ç‡æ—¶å´©æºƒ: {e}")
        return pl.DataFrame()


def batch_calc_factor_turnover_by_autocorr(
        df: Union[pl.DataFrame, pl.LazyFrame],
        factors: Union[str, List[str]] = r"^factor_.*",
        date_col: str = F.DATE,
        asset_col: str = F.ASSET,
        lag: int = 1,
        method: str = "spearman"
) -> pl.DataFrame:
    """
    åŸºäºæˆªé¢è‡ªç›¸å…³æ³•è®¡ç®—å› å­æ¢æ‰‹ç‡ï¼ˆè½»é‡çº§ç‰ˆæœ¬ï¼‰ã€‚

    **åŸç†**ï¼šå› å­çš„æˆªé¢è‡ªç›¸å…³æ€§åæ˜ å…¶æ’åºçš„ç¨³å®šæ€§ã€‚
    - è‡ªç›¸å…³æ€§è¶Šé«˜ â†’ æ’åºè¶Šç¨³å®š â†’ æ¢æ‰‹ç‡è¶Šä½
    - è‡ªç›¸å…³æ€§è¶Šä½ â†’ æ’åºå˜åŒ–è¶Šå¤§ â†’ æ¢æ‰‹ç‡è¶Šé«˜

    æ¢æ‰‹ç‡ä¼°ç®—å…¬å¼ï¼š**estimated_turnover â‰ˆ 1 - autocorr(Factor_T, Factor_{T-lag})**

    **ä¼˜åŠ¿**ï¼š
    1. æ— éœ€åˆ†æ¡¶ï¼Œè®¡ç®—ç®€å•å¿«é€Ÿ
    2. æ— éœ€æŒ‡å®š n_bins å‚æ•°
    3. å¯¹å› å­çš„æ’åºç¨³å®šæ€§æœ‰ç›´è§‚ç†è§£
    4. æ€§èƒ½æœ€ä¼˜ï¼ˆä»…éœ€æ—¥åº¦ç›¸å…³ç³»æ•°è®¡ç®—ï¼‰

    **å±€é™æ€§**ï¼š
    1. è¿™æ˜¯æ¢æ‰‹ç‡çš„ä»£ç†æŒ‡æ ‡ï¼Œä¸æ˜¯ç²¾ç¡®å€¼
    2. éœ€è¦è¶³å¤Ÿçš„äº¤å‰æˆªé¢æ ·æœ¬ï¼ˆè‡³å°‘ 20+ åªèµ„äº§ï¼‰
    3. å‡è®¾çº¿æ€§å…³ç³»

    Args:
        df: è¾“å…¥æ•°æ®ï¼ŒåŒ…å«å› å­åˆ—
        factors: å› å­åˆ—åæ­£åˆ™è¡¨è¾¾å¼æˆ–åˆ—è¡¨
        date_col: æ—¥æœŸåˆ—å
        asset_col: èµ„äº§åˆ—å
        lag: æ»åæœŸæ•°ï¼ˆè®¡ç®—ç›¸å…³æ€§æ—¶çš„é—´éš”ï¼‰ï¼Œé»˜è®¤ 1
        method: ç›¸å…³æ€§æ–¹æ³•ï¼Œ'spearman' æˆ– 'pearson'ï¼Œé»˜è®¤ 'spearman'

    Returns:
        pl.DataFrame: å› å­è‡ªç›¸å…³æ€§ç»Ÿè®¡è¡¨
        | åˆ—å | ç±»å‹ | è¯´æ˜ |
        | :--- | :--- | :--- |
        | factor | String | å› å­åç§° |
        | avg_autocorr | Float64 | å¹³å‡è‡ªç›¸å…³æ€§ (-1 ~ 1) |
        | autocorr_std | Float64 | è‡ªç›¸å…³æ€§æ ‡å‡†å·® |
        | estimated_turnover | Float64 | ä¼°è®¡æ¢æ‰‹ç‡ (1 - avg_autocorr) |

    Example:
        >>> df = pl.DataFrame({
        ...     "DATE": [20240101, 20240102, 20240103],
        ...     "ASSET": ["A", "B", "A"],
        ...     "factor_1": [0.5, 0.7, 0.6]
        ... })
        >>> batch_calc_factor_turnover_by_autocorr(df)
        # è¾“å‡º:
        # factor   | avg_autocorr | autocorr_std | estimated_turnover
        # factor_1 | 0.85         | 0.12         | 0.15
    """
    start_time = time.perf_counter()
    lf = df.lazy() if isinstance(df, pl.DataFrame) else df

    # --- 1. è‡ªåŠ¨è·å–å› å­åˆ— ---
    f_selector = cs.matches(factors) if isinstance(factors, str) else cs.by_name(factors)
    try:
        factor_cols = lf.select(f_selector).collect_schema().names()
    except Exception as e:
        logger.error(f"âŒ å› å­é€‰æ‹©å™¨åŒ¹é…å¤±è´¥: {e}")
        return pl.DataFrame()

    if not factor_cols:
        logger.warning(f"âš ï¸ æ— æ³•åŒ¹é…åˆ°ä»»ä½•å› å­ (æ¨¡å¼: {factors})ï¼Œè¿”å›ç©ºç»“æœã€‚")
        return pl.DataFrame()

    logger.info(f"ğŸ”„ å¼€å§‹è®¡ç®— {len(factor_cols)} ä¸ªå› å­çš„è‡ªç›¸å…³æ€§ (æˆªé¢æ³•, lag={lag}, method={method})")

    try:
        # --- 2. è®¡ç®—æˆªé¢è‡ªç›¸å…³æ€§ ---
        # æ ¸å¿ƒæ€æƒ³ï¼šå¯¹æ¯ä¸ªå› å­ï¼Œè®¡ç®— T æœŸä¸ T-lag æœŸçš„æˆªé¢æ’åºç›¸å…³æ€§
        lf_autocorr = (
            lf.select([date_col, asset_col] + factor_cols)
            .sort([asset_col, date_col])
            .with_columns([
                # è®¡ç®—æ»åå€¼ï¼ˆåŒä¸€èµ„äº§çš„ lag æœŸå‰çš„å€¼ï¼‰
                pl.col(f).shift(lag).over(asset_col).alias(f"{f}_lag")
                for f in factor_cols
            ])
            .group_by(date_col)
            .agg([
                # è®¡ç®—æˆªé¢ç›¸å…³æ€§ï¼ˆåŒä¸€æ—¥æœŸå†…ï¼Œä¸åŒèµ„äº§ä¹‹é—´çš„ç›¸å…³æ€§ï¼‰
                pl.corr(f, f"{f}_lag", method=method).alias(f"{f}_autocorr")
                for f in factor_cols
            ])
            .collect()
        )

        logger.debug("è‡ªç›¸å…³æ€§è®¡ç®—å®Œæˆï¼Œç»“æœæ‘˜è¦:")
        for f in factor_cols:
            col = lf_autocorr.get_column(f"{f}_autocorr")
            valid_count = col.is_not_null().sum()
            avg_val = col.drop_nulls().mean() if valid_count > 0
